{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f9bpdR8y6rAb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --upgrade pip\n",
        "!pip install gymnasium\n",
        "!pip install pfrl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pfrl\n",
        "import torch\n",
        "from torch import nn\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random"
      ],
      "metadata": {
        "id": "5rwbHAM88NKJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "assert x_train.shape == (50000, 32, 32, 3)\n",
        "assert x_test.shape == (10000, 32, 32, 3)\n",
        "assert y_train.shape == (50000, 1)\n",
        "assert y_test.shape == (10000, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujFBCglr8OJy",
        "outputId": "efb48d79-e5c2-43bd-f002-4a5a4bc5a53d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CifarEnv(gym.Env):\n",
        "\n",
        "    def __init__(self,):\n",
        "\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(32, 32, 3), dtype=np.uint8)\n",
        "\n",
        "        self.action_space = spaces.Discrete(10)\n",
        "        self.expected_action = 0\n",
        "\n",
        "        # assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
        "        self.render_mode = None\n",
        "        self.x, self.y = (x_train, y_train)\n",
        "        self.random = True\n",
        "        self.images_per_episode = 1\n",
        "        self.dataset_idx = 0\n",
        "\n",
        "    def _get_info(self):\n",
        "      return 0\n",
        "\n",
        "    def step(self, action):\n",
        "        done = False\n",
        "        reward = int(action == self.expected_action)\n",
        "\n",
        "        obs = self._next_obs()\n",
        "\n",
        "        self.step_count += 1\n",
        "        if self.step_count >= self.images_per_episode:\n",
        "            done = True\n",
        "\n",
        "        return obs, reward, done, {}\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        self.step_count = 0\n",
        "        obs = self._next_obs()\n",
        "        return obs\n",
        "\n",
        "    def _next_obs(self):\n",
        "        if self.random:\n",
        "            next_obs_idx = random.randint(0, len(self.x) - 1)\n",
        "            self.expected_action = int(self.y[next_obs_idx])\n",
        "            obs = self.x[next_obs_idx]\n",
        "\n",
        "        else:\n",
        "            obs = self.x[self.dataset_idx]\n",
        "            self.expected_action = int(self.y[self.dataset_idx])\n",
        "\n",
        "            self.dataset_idx += 1\n",
        "            if self.dataset_idx >= len(self.x):\n",
        "                raise StopIteration()\n",
        "\n",
        "        return obs\n",
        "\n",
        "class CifarEnvTest(gym.Env):\n",
        "\n",
        "    def __init__(self,):\n",
        "\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(32, 32, 3), dtype=np.uint8)\n",
        "\n",
        "        self.action_space = spaces.Discrete(10)\n",
        "        self.expected_action = 0\n",
        "\n",
        "        # assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
        "        self.render_mode = None\n",
        "        self.x, self.y = (x_test, y_test)\n",
        "        self.random = True\n",
        "        self.images_per_episode = 10000\n",
        "        self.dataset_idx = 0\n",
        "\n",
        "    def _get_info(self):\n",
        "      return 0\n",
        "\n",
        "    def step(self, action):\n",
        "        done = False\n",
        "        reward = int(action == self.expected_action)\n",
        "\n",
        "        obs = self._next_obs()\n",
        "\n",
        "        self.step_count += 1\n",
        "        if self.step_count >= self.images_per_episode:\n",
        "            done = True\n",
        "\n",
        "        return obs, reward, done, {}\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        self.step_count = 0\n",
        "        obs = self._next_obs()\n",
        "        return obs\n",
        "\n",
        "    def _next_obs(self):\n",
        "        if self.random:\n",
        "            next_obs_idx = random.randint(0, len(self.x) - 1)\n",
        "            self.expected_action = int(self.y[next_obs_idx])\n",
        "            obs = self.x[next_obs_idx]\n",
        "\n",
        "        else:\n",
        "            obs = self.x[self.dataset_idx]\n",
        "            self.expected_action = int(self.y[self.dataset_idx])\n",
        "\n",
        "            self.dataset_idx += 1\n",
        "            if self.dataset_idx >= len(self.x):\n",
        "                raise StopIteration()\n",
        "\n",
        "        return obs"
      ],
      "metadata": {
        "id": "Vwk0DhTb8R_A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = CifarEnv()\n",
        "test_env = CifarEnvTest()"
      ],
      "metadata": {
        "id": "-fSUr0R28TdS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from pfrl import action_value\n",
        "from pfrl.nn.mlp import MLP\n",
        "from pfrl.q_function import StateQFunction\n",
        "\n",
        "def constant_bias_initializer(bias=0.0):\n",
        "    @torch.no_grad()\n",
        "    def init_bias(m):\n",
        "        if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
        "            m.bias.fill_(bias)\n",
        "\n",
        "    return init_bias\n",
        "\n",
        "class QFunction(torch.nn.Module):\n",
        "\n",
        "    def __init__(self,):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
        "            nn.BatchNorm2d(64),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
        "            nn.BatchNorm2d(128),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
        "            nn.BatchNorm2d(256),\n",
        "\n",
        "            # nn.Flatten(),\n",
        "            # nn.Linear(256*4*4, 1024),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Linear(1024, 512),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Linear(512, 10),\n",
        "            pfrl.q_functions.DuelingDQN(n_actions=10, n_input_channels=256)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.network(x)\n",
        "        return h\n",
        "\n",
        "class DuelingDQN(nn.Module, StateQFunction):\n",
        "    \"\"\"Dueling Q-Network\n",
        "\n",
        "    See: http://arxiv.org/abs/1511.06581\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_actions, n_input_channels=3, activation=F.relu, bias=0.1):\n",
        "        self.n_actions = n_actions\n",
        "        self.n_input_channels = n_input_channels\n",
        "        self.activation = activation\n",
        "\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
        "            nn.BatchNorm2d(64),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
        "            nn.BatchNorm2d(128),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
        "            nn.BatchNorm2d(256),\n",
        "            )\n",
        "        # self.conv_layers = nn.ModuleList(\n",
        "        #     [\n",
        "        #         nn.Conv2d(n_input_channels, 32, 8, stride=4),\n",
        "        #         nn.Conv2d(32, 64, 4, stride=2),\n",
        "        #         nn.Conv2d(64, 64, 3, stride=1),\n",
        "        #     ]\n",
        "        # )\n",
        "\n",
        "        self.a_stream = MLP(4096, n_actions, [512])\n",
        "        self.v_stream = MLP(4096, 1, [512])\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.network(x)\n",
        "\n",
        "        # Advantage\n",
        "        batch_size = x.shape[0]\n",
        "        h = h.reshape(batch_size, -1)\n",
        "        ya = self.a_stream(h)\n",
        "        mean = torch.reshape(torch.sum(ya, dim=1) / self.n_actions, (batch_size, 1))\n",
        "        ya, mean = torch.broadcast_tensors(ya, mean)\n",
        "        ya -= mean\n",
        "\n",
        "        # State value\n",
        "        ys = self.v_stream(h)\n",
        "\n",
        "        ya, ys = torch.broadcast_tensors(ya, ys)\n",
        "        q = ya + ys\n",
        "        return action_value.DiscreteActionValue(q)\n",
        "\n",
        "obs_size = env.observation_space.low.size\n",
        "n_actions = env.action_space.n\n",
        "q_func = DuelingDQN(10)"
      ],
      "metadata": {
        "id": "ECBIvNbo8VB9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Adam to optimize q_func. eps=1e-2 is for stability.\n",
        "optimizer = torch.optim.Adam(q_func.parameters(), eps=1e-2)\n",
        "# Set the discount factor that discounts future rewards.\n",
        "gamma = 1\n",
        "\n",
        "# Use epsilon-greedy for exploration\n",
        "explorer = pfrl.explorers.ConstantEpsilonGreedy(\n",
        "    epsilon=0.3, random_action_func=env.action_space.sample)\n",
        "explorer_2 = pfrl.explorers.LinearDecayEpsilonGreedy(start_epsilon=0.9, end_epsilon=0.01, decay_steps=50000, random_action_func = env.action_space.sample)\n",
        "# DQN uses Experience Replay.\n",
        "# Specify a replay buffer and its capacity.\n",
        "replay_buffer = pfrl.replay_buffers.PrioritizedReplayBuffer(capacity=10 ** 5)\n",
        "\n",
        "# Since observations from CartPole-v0 is numpy.float64 while\n",
        "# As PyTorch only accepts numpy.float32 by default, specify\n",
        "# a converter as a feature extractor function phi.\n",
        "phi = lambda x:np.resize(x,(3,32,32)).astype(np.float32, copy=False)\n",
        "\n",
        "# Set the device id to use GPU. To use CPU only, set it to -1.\n",
        "gpu = 0\n",
        "\n",
        "# Now create an agent that will interact with the environment.\n",
        "agent = pfrl.agents.DoubleDQN(\n",
        "    q_func,\n",
        "    optimizer,\n",
        "    replay_buffer,\n",
        "    gamma,\n",
        "    explorer_2,\n",
        "    replay_start_size=10000,\n",
        "    update_interval=1,\n",
        "    target_update_interval=10000,\n",
        "    phi = phi,\n",
        "    gpu=gpu,\n",
        ")"
      ],
      "metadata": {
        "id": "LL9yCS6_8dyV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the logger to print info messages for understandability.\n",
        "import logging\n",
        "import sys\n",
        "import time\n",
        "start_time = time.time()\n",
        "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='')\n",
        "eval_ep = 10000\n",
        "pfrl.experiments.train_agent_with_evaluation(\n",
        "    agent,\n",
        "    env,\n",
        "    steps=100000,           # Train the agent for 2000 steps\n",
        "    eval_n_steps=None,       # We evaluate for episodes, not time\n",
        "    eval_n_episodes=1,       # 10 episodes are sampled for each evaluation\n",
        "    train_max_episode_len=1,  # Maximum length of each episode\n",
        "    eval_max_episode_len=10000,\n",
        "    eval_interval=20000,   # Evaluate the agent after every 1000 steps\n",
        "    outdir='result',      # Save everything to 'result' directory\n",
        "    eval_env = test_env,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvDWO29mC0eW",
        "outputId": "da60c2d3-e3f3-4799-e0b6-100a6af05a68"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<pfrl.agents.double_dqn.DoubleDQN at 0x78eb75c65090>,\n",
              " [{'average_q': 0.19232978,\n",
              "   'average_loss': 0.005065429147798568,\n",
              "   'cumulative_steps': 20000,\n",
              "   'n_updates': 10001,\n",
              "   'rlen': 20000,\n",
              "   'eval_score': 3320.0},\n",
              "  {'average_q': 0.3095606,\n",
              "   'average_loss': 0.005119726323755458,\n",
              "   'cumulative_steps': 40000,\n",
              "   'n_updates': 30001,\n",
              "   'rlen': 40000,\n",
              "   'eval_score': 3914.0},\n",
              "  {'average_q': 0.40963265,\n",
              "   'average_loss': 0.0052067339979112144,\n",
              "   'cumulative_steps': 60000,\n",
              "   'n_updates': 50001,\n",
              "   'rlen': 60000,\n",
              "   'eval_score': 4309.0},\n",
              "  {'average_q': 0.43191582,\n",
              "   'average_loss': 0.004073946775170043,\n",
              "   'cumulative_steps': 80000,\n",
              "   'n_updates': 70001,\n",
              "   'rlen': 80000,\n",
              "   'eval_score': 4279.0},\n",
              "  {'average_q': 0.49659368,\n",
              "   'average_loss': 0.003933122666785494,\n",
              "   'cumulative_steps': 100000,\n",
              "   'n_updates': 90001,\n",
              "   'rlen': 100000,\n",
              "   'eval_score': 4497.0}])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DQN Training Time:\", time.time() - start_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaSrG8HvC2EX",
        "outputId": "145c9b8a-b26a-49e2-da8f-b35538ee788c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DQN Training Time: 1815.3063447475433\n"
          ]
        }
      ]
    }
  ]
}