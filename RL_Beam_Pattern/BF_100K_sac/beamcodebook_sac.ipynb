{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-07T18:12:55.616821Z",
     "iopub.status.busy": "2023-11-07T18:12:55.616381Z",
     "iopub.status.idle": "2023-11-07T18:12:57.715541Z",
     "shell.execute_reply": "2023-11-07T18:12:57.714318Z",
     "shell.execute_reply.started": "2023-11-07T18:12:55.616750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Codebook_Learning_RL' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/YuZhang-GitHub/Codebook_Learning_RL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:12:57.717998Z",
     "iopub.status.busy": "2023-11-07T18:12:57.717711Z",
     "iopub.status.idle": "2023-11-07T18:13:13.960801Z",
     "shell.execute_reply": "2023-11-07T18:13:13.959613Z",
     "shell.execute_reply.started": "2023-11-07T18:12:57.717970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pfrl in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pfrl) (2.1.0a0+32f93b1)\n",
      "Requirement already satisfied: gym>=0.9.7 in /usr/local/lib/python3.10/dist-packages (from pfrl) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from pfrl) (1.22.2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pfrl) (9.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from pfrl) (3.12.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.9.7->pfrl) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.9.7->pfrl) (0.0.8)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (4.7.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (2023.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->pfrl) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->pfrl) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.22.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pfrl\n",
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:13:13.962474Z",
     "iopub.status.busy": "2023-11-07T18:13:13.962149Z",
     "iopub.status.idle": "2023-11-07T18:13:13.967460Z",
     "shell.execute_reply": "2023-11-07T18:13:13.966388Z",
     "shell.execute_reply.started": "2023-11-07T18:13:13.962444Z"
    }
   },
   "outputs": [],
   "source": [
    "# from Codebook_Learning_RL import DDPG_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:13:13.970037Z",
     "iopub.status.busy": "2023-11-07T18:13:13.969775Z",
     "iopub.status.idle": "2023-11-07T18:13:13.980920Z",
     "shell.execute_reply": "2023-11-07T18:13:13.979942Z",
     "shell.execute_reply.started": "2023-11-07T18:13:13.970015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Codebook_Learning_RL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/workspace/Codebook_Learning_RL')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:13:13.982235Z",
     "iopub.status.busy": "2023-11-07T18:13:13.981998Z",
     "iopub.status.idle": "2023-11-07T18:13:20.276087Z",
     "shell.execute_reply": "2023-11-07T18:13:20.275229Z",
     "shell.execute_reply.started": "2023-11-07T18:13:13.982214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1699694698.052032] [36ccdc8c3f8d:6905 :f]        vfs_fuse.c:281  UCX  ERROR inotify_add_watch(/tmp) failed: No space left on device\n"
     ]
    }
   ],
   "source": [
    "import pfrl\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from DataPrep import dataPrep\n",
    "from env_ddpg import envCB\n",
    "from clustering import KMeans_only\n",
    "from function_lib import bf_gain_cal, corr_mining\n",
    "from DDPG_classes import OUNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1321,\n",
       " 27148,\n",
       " 26104,\n",
       " 30786,\n",
       " 31404,\n",
       " 31885,\n",
       " 31885,\n",
       " 15171,\n",
       " 31573,\n",
       " 31573,\n",
       " 31883,\n",
       " 31883,\n",
       " 31883,\n",
       " 14846,\n",
       " 1,\n",
       " 2035]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "\n",
    "def get_gpu_memory():\n",
    "    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    return memory_free_values\n",
    "\n",
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:13:20.278225Z",
     "iopub.status.busy": "2023-11-07T18:13:20.277693Z",
     "iopub.status.idle": "2023-11-07T18:13:20.283686Z",
     "shell.execute_reply": "2023-11-07T18:13:20.282666Z",
     "shell.execute_reply.started": "2023-11-07T18:13:20.278188Z"
    }
   },
   "outputs": [],
   "source": [
    "class envCB_(envCB):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.gain_history = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:13:20.285453Z",
     "iopub.status.busy": "2023-11-07T18:13:20.285104Z",
     "iopub.status.idle": "2023-11-07T18:13:38.312574Z",
     "shell.execute_reply": "2023-11-07T18:13:38.311420Z",
     "shell.execute_reply.started": "2023-11-07T18:13:20.285421Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "options = {\n",
    "        'gpu_idx': 12,\n",
    "        'num_ant': 32,\n",
    "        'num_bits': 4,\n",
    "        'num_NNs': 4,  # codebook size\n",
    "        'ch_sample_ratio': 0.5,\n",
    "        'num_loop': 1000,  # outer loop\n",
    "        'target_update': 3,\n",
    "        'path': './grid1101-1400.mat',\n",
    "        'clustering_mode': 'random',\n",
    "    }\n",
    "\n",
    "train_opt = {\n",
    "        'state': 0,\n",
    "        'best_state': 0,\n",
    "        'num_iter': 100,  # inner loop\n",
    "        'tau': 1e-2,\n",
    "        'overall_iter': 1,\n",
    "        'replay_memory': [],\n",
    "        'replay_memory_size': 8192,\n",
    "        'minibatch_size': 1024,\n",
    "        'gamma': 0,\n",
    "        'gpu':12\n",
    "    }\n",
    "if not os.path.exists('beams/'):\n",
    "    os.mkdir('beams/')\n",
    "\n",
    "ch = dataPrep(options['path'])\n",
    "ch = np.concatenate((ch[:, :options['num_ant']],\n",
    "                     ch[:, int(ch.shape[1] / 2):int(ch.shape[1] / 2) + options['num_ant']]), axis=1)\n",
    "with torch.cuda.device(options['gpu_idx']):\n",
    "    u_classifier, sensing_beam = KMeans_only(ch, options['num_NNs'], n_bit=options['num_bits'], n_rand_beam=30)\n",
    "    np.save('sensing_beam.npy', sensing_beam)\n",
    "    sensing_beam = torch.from_numpy(sensing_beam).float().cuda()\n",
    "\n",
    "    filename = 'kmeans_model.sav'\n",
    "    pickle.dump(u_classifier, open(filename, 'wb'))\n",
    "\n",
    "    # Quantization settings\n",
    "    options['num_ph'] = 2 ** options['num_bits']\n",
    "    options['multi_step'] = torch.from_numpy(\n",
    "        np.linspace(int(-(options['num_ph'] - 2) / 2),\n",
    "                    int(options['num_ph'] / 2),\n",
    "                    num=options['num_ph'],\n",
    "                    endpoint=True)).type(dtype=torch.float32).reshape(1, -1).cuda()\n",
    "    options['pi'] = torch.tensor(np.pi).cuda()\n",
    "    options['ph_table'] = (2 * options['pi']) / options['num_ph'] * options['multi_step']\n",
    "    options['ph_table'].cuda()\n",
    "    options['ph_table_rep'] = options['ph_table'].repeat(options['num_ant'], 1)\n",
    "    actor_net_list = []\n",
    "    critic_net_list = []\n",
    "    actor_net_t_list = []\n",
    "    critic_net_t_list = []\n",
    "    ounoise_list = []\n",
    "    env_list = []\n",
    "    train_opt_list = []\n",
    "    agent_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:13:38.314670Z",
     "iopub.status.busy": "2023-11-07T18:13:38.314142Z",
     "iopub.status.idle": "2023-11-07T18:13:38.329119Z",
     "shell.execute_reply": "2023-11-07T18:13:38.328135Z",
     "shell.execute_reply.started": "2023-11-07T18:13:38.314631Z"
    }
   },
   "outputs": [],
   "source": [
    "from pfrl.utils.batch_states import batch_states\n",
    "def batch_experiences_(experiences, device, phi, gamma, batch_states=batch_states):\n",
    "    \"\"\"Takes a batch of k experiences each of which contains j\n",
    "\n",
    "    consecutive transitions and vectorizes them, where j is between 1 and n.\n",
    "\n",
    "    Args:\n",
    "        experiences: list of experiences. Each experience is a list\n",
    "            containing between 1 and n dicts containing\n",
    "              - state (object): State\n",
    "              - action (object): Action\n",
    "              - reward (float): Reward\n",
    "              - is_state_terminal (bool): True iff next state is terminal\n",
    "              - next_state (object): Next state\n",
    "        device : GPU or CPU the tensor should be placed on\n",
    "        phi : Preprocessing function\n",
    "        gamma: discount factor\n",
    "        batch_states: function that converts a list to a batch\n",
    "    Returns:\n",
    "        dict of batched transitions\n",
    "    \"\"\"\n",
    "\n",
    "    batch_exp = {\n",
    "        \"state\": batch_states([elem[0][\"state\"] for elem in experiences], device, phi),\n",
    "        \"action\": torch.as_tensor(\n",
    "            np.array([elem[0][\"action\"] for elem in experiences]), device=device\n",
    "        ),\n",
    "        \"reward\": torch.as_tensor(\n",
    "            [\n",
    "                sum((gamma**i) * exp[i][\"reward\"] for i in range(len(exp)))\n",
    "                for exp in experiences\n",
    "            ],\n",
    "            dtype=torch.float32,\n",
    "            device=device,\n",
    "        ),\n",
    "        \"next_state\": batch_states(\n",
    "            [elem[-1][\"next_state\"] for elem in experiences], device, phi\n",
    "        ),\n",
    "        \"is_state_terminal\": torch.as_tensor(\n",
    "            [\n",
    "                any(transition[\"is_state_terminal\"] for transition in exp)\n",
    "                for exp in experiences\n",
    "            ],\n",
    "            dtype=torch.float32,\n",
    "            device=device,\n",
    "        ),\n",
    "        \"discount\": torch.as_tensor(\n",
    "            [(gamma ** len(elem)) for elem in experiences],\n",
    "            dtype=torch.float32,\n",
    "            device=device,\n",
    "        ),\n",
    "    }\n",
    "    if all(elem[-1][\"next_action\"] is not None for elem in experiences):\n",
    "        batch_exp[\"next_action\"] = torch.as_tensor(\n",
    "            [elem[-1][\"next_action\"] for elem in experiences], device=device\n",
    "        )\n",
    "    return batch_exp\n",
    "\n",
    "from pfrl.agents.ddpg import DDPG\n",
    "class DDPG_(DDPG):\n",
    "    def update(self, experiences, errors_out=None):\n",
    "        \"\"\"Update the model from experiences\"\"\"\n",
    "\n",
    "        batch = batch_experiences_(experiences, self.device, self.phi, self.gamma)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        self.compute_critic_loss(batch).backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.compute_actor_loss(batch).backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.n_updates += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:13:38.330988Z",
     "iopub.status.busy": "2023-11-07T18:13:38.330457Z",
     "iopub.status.idle": "2023-11-07T18:13:38.344990Z",
     "shell.execute_reply": "2023-11-07T18:13:38.344059Z",
     "shell.execute_reply.started": "2023-11-07T18:13:38.330961Z"
    }
   },
   "outputs": [],
   "source": [
    "import pfrl\n",
    "from pfrl import experiments, explorers, replay_buffers, utils\n",
    "from pfrl.agents.ddpg import DDPG\n",
    "from pfrl.nn import BoundByTanh, ConcatObsAndAction\n",
    "from pfrl.policies import DeterministicHead\n",
    "from pfrl.explorers import Greedy\n",
    "\n",
    "def create_agent(state_size,action_size,train_options,gpu=12):\n",
    "    \n",
    "    #critic_network\n",
    "    q_func = nn.Sequential(\n",
    "        ConcatObsAndAction(),\n",
    "        nn.Linear(state_size + action_size, 16*state_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16*state_size, 16*action_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16*action_size, 1),\n",
    "    )\n",
    "    #actor_network\n",
    "    policy = nn.Sequential(\n",
    "        nn.Linear(state_size, 16*state_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16*state_size, 16*action_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16*action_size, action_size),\n",
    "        BoundByTanh(low= -torch.tensor(np.pi).float(), high= torch.tensor(np.pi).float()),\n",
    "        DeterministicHead(),\n",
    "    )\n",
    "    \n",
    "    opt_a = torch.optim.Adam(policy.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "    opt_c = torch.optim.Adam(q_func.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "    rbuf = replay_buffers.ReplayBuffer(train_options['replay_memory_size'])\n",
    "    \n",
    "    explorer =  Greedy()\n",
    "    \n",
    "    agent = DDPG_(\n",
    "        policy,\n",
    "        q_func,\n",
    "        opt_a,\n",
    "        opt_c,\n",
    "        rbuf,\n",
    "        gamma=train_options['gamma'],\n",
    "        explorer = explorer,\n",
    "        replay_start_size=train_options['minibatch_size'],\n",
    "        target_update_method=\"soft\",\n",
    "        target_update_interval=options['target_update'],\n",
    "        update_interval=1,\n",
    "        soft_update_tau= train_opt['tau'],\n",
    "        n_times_update=1,\n",
    "        gpu=gpu,\n",
    "        minibatch_size=train_options['minibatch_size'],\n",
    "    )\n",
    "    \n",
    "    return agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:13:38.348114Z",
     "iopub.status.busy": "2023-11-07T18:13:38.347862Z",
     "iopub.status.idle": "2023-11-07T18:13:38.362335Z",
     "shell.execute_reply": "2023-11-07T18:13:38.361570Z",
     "shell.execute_reply.started": "2023-11-07T18:13:38.348093Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_agent_sac(state_size,action_size,train_options,gpu=12):\n",
    "    def squashed_diagonal_gaussian_head(x):\n",
    "        assert x.shape[-1] == action_size * 2\n",
    "        mean, log_scale = torch.chunk(x, 2, dim=2)\n",
    "        log_scale = torch.clamp(log_scale, -20.0, 2.0)\n",
    "        var = torch.exp(log_scale * 2)\n",
    "        base_distribution = distributions.Independent(\n",
    "            distributions.Normal(loc=mean, scale=torch.sqrt(var)), 1\n",
    "        )\n",
    "        # cache_size=1 is required for numerical stability\n",
    "        return distributions.transformed_distribution.TransformedDistribution(\n",
    "            base_distribution, [distributions.transforms.TanhTransform(cache_size=1),\n",
    "                                distributions.transforms.AffineTransform(0, torch.pi, event_dim=0, cache_size=1)]\n",
    "        )\n",
    "\n",
    "    #critic_networks\n",
    "    q_func_1 = nn.Sequential(\n",
    "        ConcatObsAndAction(),\n",
    "        nn.Linear(state_size + action_size, 16*state_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16*state_size, 16*action_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16*action_size, 1),\n",
    "    )\n",
    "    q_func_2 = nn.Sequential(\n",
    "        ConcatObsAndAction(),\n",
    "        nn.Linear(state_size + action_size, 16*state_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16*state_size, 16*action_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16*action_size, 1),\n",
    "    )\n",
    "    \n",
    "    #actor_network\n",
    "    policy = nn.Sequential(\n",
    "        nn.Linear(state_size, 16*state_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16*state_size, 16*action_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16*action_size, action_size* 2),\n",
    "        pfrl.nn.lmbda.Lambda(squashed_diagonal_gaussian_head),\n",
    "    )\n",
    "\n",
    "\n",
    "    opt_a = torch.optim.Adam(policy.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "    opt_c_1 = torch.optim.Adam(q_func_1.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "    opt_c_2 = torch.optim.Adam(q_func_2.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "    \n",
    "    rbuf = replay_buffers.ReplayBuffer(train_options['replay_memory_size'])\n",
    "    \n",
    "    agent = pfrl.agents.SoftActorCritic(\n",
    "        policy,\n",
    "        q_func_1,\n",
    "        q_func_2,\n",
    "        opt_a,\n",
    "        opt_c_1,\n",
    "        opt_c_2,\n",
    "        rbuf,\n",
    "        gamma=train_options['gamma'],\n",
    "        update_interval=1,\n",
    "        replay_start_size=train_options['minibatch_size'],\n",
    "        gpu=gpu,\n",
    "        soft_update_tau= train_opt['tau'],\n",
    "        minibatch_size = train_options['minibatch_size'],\n",
    "        entropy_target=-action_size,\n",
    "        temperature_optimizer_lr=1e-3,\n",
    "    )\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:13:38.363891Z",
     "iopub.status.busy": "2023-11-07T18:13:38.363588Z",
     "iopub.status.idle": "2023-11-07T18:13:38.380440Z",
     "shell.execute_reply": "2023-11-07T18:13:38.379643Z",
     "shell.execute_reply.started": "2023-11-07T18:13:38.363865Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "          env,\n",
    "          options,\n",
    "          train_options,\n",
    "          agent,\n",
    "          ounoise,\n",
    "          beam_id):\n",
    "    \n",
    "#     action_pred_noisy = ounoise.get_action(action_pred,\n",
    "#                                                t=train_options['overall_iter'])\n",
    "    \n",
    "    CB_Env = env   \n",
    "    if train_options['overall_iter'] == 1:\n",
    "        state = torch.zeros((1, options['num_ant'])).float().cuda()\n",
    "        print('Initial State Activated.')\n",
    "    else:\n",
    "        state = train_options['state']\n",
    "    \n",
    "    \n",
    "    #training_loop\n",
    "    iteration = 0\n",
    "    num_of_iter = train_options['num_iter']  \n",
    "    while iteration < num_of_iter:\n",
    "        \n",
    "        device_index = train_options['gpu']\n",
    "        device = f'cuda:{device_index}'\n",
    "        \n",
    "        action = agent.act(state)\n",
    "#         print(type(agent.batch_last_action),agent.batch_last_action)\n",
    "        reward_pred, bf_gain_pred, action_quant_pred, state_1_pred = CB_Env.get_reward(torch.Tensor(action).to(device))\n",
    "        reward_pred = torch.from_numpy(reward_pred).float().to(device)\n",
    "        \n",
    "        action_pred_noisy = torch.Tensor(action).to(device)\n",
    "        mat_dist = torch.abs(action_pred_noisy.reshape(options['num_ant'], 1) - options['ph_table_rep'])\n",
    "        action_quant = options['ph_table_rep'][range(options['num_ant']), torch.argmin(mat_dist, dim=1)].reshape(1, -1)\n",
    "   \n",
    "        state_1, reward, bf_gain, terminal = CB_Env.step(action_quant)\n",
    "        reward = torch.from_numpy(reward).float().to(device)\n",
    "        action = action_quant.reshape((1, -1)).float().to(device)\n",
    "        \n",
    "        \n",
    "#         print(type(action_quant_pred.cpu().numpy()),action_quant_pred.cpu().numpy().shape)\n",
    "#         print(type(action.cpu().numpy()),action.cpu().numpy().shape)\n",
    "        agent.replay_buffer.append(\n",
    "                    state=state,\n",
    "                    action=action_quant_pred.cpu().numpy(),\n",
    "                    reward=reward_pred,\n",
    "                    next_state=state_1_pred,\n",
    "                    next_action=None,\n",
    "                    is_state_terminal=terminal,\n",
    "                    env_id=0,\n",
    "                )\n",
    "#         print(agent.batch_last_action,agent.batch_last_action[0].shape)\n",
    "        agent.batch_last_action = [action.cpu().numpy()] #because the critic is expecting quantizazed actions\n",
    "        agent.observe(state_1, reward, terminal, False)\n",
    "\n",
    "        \n",
    "        iteration += 1\n",
    "        train_options['overall_iter'] += 1\n",
    "        state = state_1\n",
    "        \n",
    "        new_gain = torch.Tensor.cpu(CB_Env.achievement).detach().numpy().reshape((1, 1))\n",
    "        max_previous_gain = max(CB_Env.gain_history)\n",
    "        if new_gain > max_previous_gain:\n",
    "            CB_Env.gain_history.append(float(new_gain))                   \n",
    "        else:\n",
    "            CB_Env.gain_history.append(float(max_previous_gain))\n",
    "            \n",
    "    train_options['state'] = state  # used for the next loop\n",
    "    train_options['best_state'] = CB_Env.best_bf_vec  # used for clustering and assignment\n",
    "    if (train_options['overall_iter']-1)%500==0:\n",
    "#         print(\n",
    "#             \"Beam: %d, Iter: %d, Reward: %d, BF Gain: %.2f\" % \\\n",
    "#             (beam_id, \n",
    "#              train_options['overall_iter'],\n",
    "#              int(torch.Tensor.cpu(reward).numpy().squeeze()),\n",
    "#              torch.Tensor.cpu(bf_gain.detach()).numpy().squeeze(),))\n",
    "        print(\n",
    "            \"Beam: %d, Iter: %d, Reward pred: %d, Reward: %d, BF Gain pred: %.2f, BF Gain: %.2f\" % \\\n",
    "            (beam_id, train_options['overall_iter'],\n",
    "             int(torch.Tensor.cpu(reward_pred).numpy().squeeze()),\n",
    "             int(torch.Tensor.cpu(reward).numpy().squeeze()),\n",
    "             torch.Tensor.cpu(bf_gain_pred.detach()).numpy().squeeze(),\n",
    "             torch.Tensor.cpu(bf_gain.detach()).numpy().squeeze(),))      \n",
    "    \n",
    "    return train_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:13:38.382023Z",
     "iopub.status.busy": "2023-11-07T18:13:38.381733Z",
     "iopub.status.idle": "2023-11-07T18:13:38.511939Z",
     "shell.execute_reply": "2023-11-07T18:13:38.510646Z",
     "shell.execute_reply.started": "2023-11-07T18:13:38.381999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EGC bf gain:  231.17094\n",
      "EGC bf gain:  231.17094\n",
      "EGC bf gain:  231.17094\n",
      "EGC bf gain:  231.17094\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.device(options['gpu_idx']):\n",
    "    for beam_id in range(options['num_NNs']):\n",
    "        ounoise_list.append(OUNoise((1, options['num_ant'])))\n",
    "        env_list.append(envCB_(ch, options['num_ant'], options['num_bits'], beam_id, options))\n",
    "        train_opt_list.append(copy.deepcopy(train_opt))\n",
    "        agent = create_agent_sac(options['num_ant'],options['num_ant'],train_opt_list[beam_id],gpu=train_opt['gpu'])\n",
    "        agent_list.append(agent)\n",
    "        ounoise_list.append(OUNoise((1, options['num_ant'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:13:38.513735Z",
     "iopub.status.busy": "2023-11-07T18:13:38.513389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State Activated.\n",
      "Initial State Activated.\n",
      "Initial State Activated.\n",
      "Initial State Activated.\n",
      "Beam: 0, Iter: 501, Reward pred: 1, Reward: 1, BF Gain pred: 7.68, BF Gain: 7.68\n",
      "Beam: 1, Iter: 501, Reward pred: -1, Reward: -1, BF Gain pred: 2.44, BF Gain: 2.44\n",
      "Beam: 2, Iter: 501, Reward pred: 1, Reward: 1, BF Gain pred: 29.38, BF Gain: 29.38\n",
      "Beam: 3, Iter: 501, Reward pred: 1, Reward: 1, BF Gain pred: 17.47, BF Gain: 17.47\n",
      "Training for 500 iteration for each Beam uses 11.189823150634766 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pfrl/replay_buffer.py:180: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  \"action\": torch.as_tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam: 0, Iter: 1001, Reward pred: 1, Reward: 1, BF Gain pred: 18.11, BF Gain: 18.11\n",
      "Beam: 1, Iter: 1001, Reward pred: 1, Reward: 1, BF Gain pred: 4.22, BF Gain: 4.22\n",
      "Beam: 2, Iter: 1001, Reward pred: 1, Reward: 1, BF Gain pred: 8.92, BF Gain: 8.92\n",
      "Beam: 3, Iter: 1001, Reward pred: -1, Reward: -1, BF Gain pred: 2.68, BF Gain: 2.68\n",
      "Training for 500 iteration for each Beam uses 141.98333764076233 seconds.\n",
      "Beam: 0, Iter: 1501, Reward pred: 1, Reward: 1, BF Gain pred: 8.02, BF Gain: 8.02\n",
      "Beam: 1, Iter: 1501, Reward pred: -1, Reward: -1, BF Gain pred: 0.98, BF Gain: 0.98\n",
      "Beam: 2, Iter: 1501, Reward pred: 1, Reward: 1, BF Gain pred: 4.38, BF Gain: 4.38\n",
      "Beam: 3, Iter: 1501, Reward pred: -1, Reward: -1, BF Gain pred: 2.68, BF Gain: 2.68\n",
      "Training for 500 iteration for each Beam uses 142.54650497436523 seconds.\n",
      "Beam: 0, Iter: 2001, Reward pred: 1, Reward: 1, BF Gain pred: 7.37, BF Gain: 7.37\n",
      "Beam: 1, Iter: 2001, Reward pred: 1, Reward: 1, BF Gain pred: 7.60, BF Gain: 7.60\n",
      "Beam: 2, Iter: 2001, Reward pred: -1, Reward: -1, BF Gain pred: 6.81, BF Gain: 6.81\n",
      "Beam: 3, Iter: 2001, Reward pred: -1, Reward: -1, BF Gain pred: 10.92, BF Gain: 10.92\n",
      "Training for 500 iteration for each Beam uses 143.267187833786 seconds.\n",
      "Beam: 0, Iter: 2501, Reward pred: 1, Reward: 1, BF Gain pred: 8.05, BF Gain: 8.05\n",
      "Beam: 1, Iter: 2501, Reward pred: -1, Reward: -1, BF Gain pred: 1.44, BF Gain: 1.44\n",
      "Beam: 2, Iter: 2501, Reward pred: -1, Reward: -1, BF Gain pred: 2.18, BF Gain: 2.18\n",
      "Beam: 3, Iter: 2501, Reward pred: 1, Reward: 1, BF Gain pred: 9.91, BF Gain: 9.91\n",
      "Training for 500 iteration for each Beam uses 144.03344750404358 seconds.\n",
      "Beam: 0, Iter: 3001, Reward pred: 1, Reward: 1, BF Gain pred: 24.45, BF Gain: 24.45\n",
      "Beam: 1, Iter: 3001, Reward pred: -1, Reward: -1, BF Gain pred: 2.36, BF Gain: 2.36\n",
      "Beam: 2, Iter: 3001, Reward pred: -1, Reward: -1, BF Gain pred: 2.47, BF Gain: 2.47\n",
      "Beam: 3, Iter: 3001, Reward pred: -1, Reward: -1, BF Gain pred: 4.26, BF Gain: 4.26\n",
      "Training for 500 iteration for each Beam uses 143.839519739151 seconds.\n",
      "Beam: 0, Iter: 3501, Reward pred: 1, Reward: 1, BF Gain pred: 6.32, BF Gain: 6.32\n",
      "Beam: 1, Iter: 3501, Reward pred: -1, Reward: -1, BF Gain pred: 2.16, BF Gain: 2.16\n",
      "Beam: 2, Iter: 3501, Reward pred: 1, Reward: 1, BF Gain pred: 12.33, BF Gain: 12.33\n",
      "Beam: 3, Iter: 3501, Reward pred: -1, Reward: -1, BF Gain pred: 8.13, BF Gain: 8.13\n",
      "Training for 500 iteration for each Beam uses 143.92670559883118 seconds.\n",
      "Beam: 0, Iter: 4001, Reward pred: -1, Reward: -1, BF Gain pred: 3.49, BF Gain: 3.49\n",
      "Beam: 1, Iter: 4001, Reward pred: 1, Reward: 1, BF Gain pred: 2.21, BF Gain: 2.21\n",
      "Beam: 2, Iter: 4001, Reward pred: -1, Reward: -1, BF Gain pred: 7.11, BF Gain: 7.11\n",
      "Beam: 3, Iter: 4001, Reward pred: -1, Reward: -1, BF Gain pred: 2.09, BF Gain: 2.09\n",
      "Training for 500 iteration for each Beam uses 145.2826211452484 seconds.\n",
      "Beam: 0, Iter: 4501, Reward pred: 1, Reward: 1, BF Gain pred: 3.19, BF Gain: 3.19\n",
      "Beam: 1, Iter: 4501, Reward pred: -1, Reward: -1, BF Gain pred: 8.23, BF Gain: 8.23\n",
      "Beam: 2, Iter: 4501, Reward pred: -1, Reward: -1, BF Gain pred: 3.85, BF Gain: 3.85\n",
      "Beam: 3, Iter: 4501, Reward pred: -1, Reward: -1, BF Gain pred: 10.38, BF Gain: 10.38\n",
      "Training for 500 iteration for each Beam uses 143.42882776260376 seconds.\n",
      "Beam: 0, Iter: 5001, Reward pred: 1, Reward: 1, BF Gain pred: 7.03, BF Gain: 7.03\n",
      "Beam: 1, Iter: 5001, Reward pred: 1, Reward: 1, BF Gain pred: 1.77, BF Gain: 1.77\n",
      "Beam: 2, Iter: 5001, Reward pred: 1, Reward: 1, BF Gain pred: 3.63, BF Gain: 3.63\n",
      "Beam: 3, Iter: 5001, Reward pred: -1, Reward: -1, BF Gain pred: 3.25, BF Gain: 3.25\n",
      "Training for 500 iteration for each Beam uses 144.72527384757996 seconds.\n",
      "Beam: 0, Iter: 5501, Reward pred: 1, Reward: 1, BF Gain pred: 33.81, BF Gain: 33.81\n",
      "Beam: 1, Iter: 5501, Reward pred: 1, Reward: 1, BF Gain pred: 26.16, BF Gain: 26.16\n",
      "Beam: 2, Iter: 5501, Reward pred: 1, Reward: 1, BF Gain pred: 2.96, BF Gain: 2.96\n",
      "Beam: 3, Iter: 5501, Reward pred: 1, Reward: 1, BF Gain pred: 30.71, BF Gain: 30.71\n",
      "Training for 500 iteration for each Beam uses 145.37555527687073 seconds.\n",
      "Beam: 0, Iter: 6001, Reward pred: -1, Reward: -1, BF Gain pred: 28.14, BF Gain: 28.14\n",
      "Beam: 1, Iter: 6001, Reward pred: -1, Reward: -1, BF Gain pred: 18.84, BF Gain: 18.84\n",
      "Beam: 2, Iter: 6001, Reward pred: 1, Reward: 1, BF Gain pred: 33.31, BF Gain: 33.31\n",
      "Beam: 3, Iter: 6001, Reward pred: 1, Reward: 1, BF Gain pred: 24.75, BF Gain: 24.75\n",
      "Training for 500 iteration for each Beam uses 147.36072874069214 seconds.\n",
      "Beam: 0, Iter: 6501, Reward pred: -1, Reward: -1, BF Gain pred: 58.71, BF Gain: 58.71\n",
      "Beam: 1, Iter: 6501, Reward pred: 1, Reward: 1, BF Gain pred: 41.20, BF Gain: 41.20\n",
      "Beam: 2, Iter: 6501, Reward pred: -1, Reward: -1, BF Gain pred: 16.02, BF Gain: 16.02\n",
      "Beam: 3, Iter: 6501, Reward pred: 1, Reward: 1, BF Gain pred: 10.79, BF Gain: 10.79\n",
      "Training for 500 iteration for each Beam uses 146.06727004051208 seconds.\n",
      "Beam: 0, Iter: 7001, Reward pred: 1, Reward: 1, BF Gain pred: 84.54, BF Gain: 84.54\n",
      "Beam: 1, Iter: 7001, Reward pred: 1, Reward: 1, BF Gain pred: 66.50, BF Gain: 66.50\n",
      "Beam: 2, Iter: 7001, Reward pred: 1, Reward: 1, BF Gain pred: 114.08, BF Gain: 114.08\n",
      "Beam: 3, Iter: 7001, Reward pred: -1, Reward: -1, BF Gain pred: 27.32, BF Gain: 27.32\n",
      "Training for 500 iteration for each Beam uses 148.0568549633026 seconds.\n",
      "Beam: 0, Iter: 7501, Reward pred: -1, Reward: -1, BF Gain pred: 110.88, BF Gain: 110.88\n",
      "Beam: 1, Iter: 7501, Reward pred: -1, Reward: -1, BF Gain pred: 63.80, BF Gain: 63.80\n",
      "Beam: 2, Iter: 7501, Reward pred: 1, Reward: 1, BF Gain pred: 155.40, BF Gain: 155.40\n",
      "Beam: 3, Iter: 7501, Reward pred: 1, Reward: 1, BF Gain pred: 74.52, BF Gain: 74.52\n",
      "Training for 500 iteration for each Beam uses 147.90179467201233 seconds.\n",
      "Beam: 0, Iter: 8001, Reward pred: 1, Reward: 1, BF Gain pred: 125.23, BF Gain: 125.23\n",
      "Beam: 1, Iter: 8001, Reward pred: -1, Reward: -1, BF Gain pred: 84.73, BF Gain: 84.73\n",
      "Beam: 2, Iter: 8001, Reward pred: 1, Reward: 1, BF Gain pred: 175.58, BF Gain: 175.58\n",
      "Beam: 3, Iter: 8001, Reward pred: -1, Reward: -1, BF Gain pred: 123.33, BF Gain: 123.33\n",
      "Training for 500 iteration for each Beam uses 148.3162305355072 seconds.\n",
      "Beam: 0, Iter: 8501, Reward pred: 1, Reward: 1, BF Gain pred: 136.65, BF Gain: 136.65\n",
      "Beam: 1, Iter: 8501, Reward pred: 1, Reward: 1, BF Gain pred: 95.75, BF Gain: 95.75\n",
      "Beam: 2, Iter: 8501, Reward pred: 1, Reward: 1, BF Gain pred: 209.32, BF Gain: 209.32\n",
      "Beam: 3, Iter: 8501, Reward pred: -1, Reward: -1, BF Gain pred: 141.27, BF Gain: 141.27\n",
      "Training for 500 iteration for each Beam uses 148.28445291519165 seconds.\n",
      "Beam: 0, Iter: 9001, Reward pred: -1, Reward: -1, BF Gain pred: 155.82, BF Gain: 155.82\n",
      "Beam: 1, Iter: 9001, Reward pred: 1, Reward: 1, BF Gain pred: 97.51, BF Gain: 97.51\n",
      "Beam: 2, Iter: 9001, Reward pred: 1, Reward: 1, BF Gain pred: 216.49, BF Gain: 216.49\n",
      "Beam: 3, Iter: 9001, Reward pred: -1, Reward: -1, BF Gain pred: 176.06, BF Gain: 176.06\n",
      "Training for 500 iteration for each Beam uses 147.49195742607117 seconds.\n",
      "Beam: 0, Iter: 9501, Reward pred: -1, Reward: -1, BF Gain pred: 155.99, BF Gain: 155.99\n",
      "Beam: 1, Iter: 9501, Reward pred: -1, Reward: -1, BF Gain pred: 100.52, BF Gain: 100.52\n",
      "Beam: 2, Iter: 9501, Reward pred: 1, Reward: 1, BF Gain pred: 225.57, BF Gain: 225.57\n",
      "Beam: 3, Iter: 9501, Reward pred: 1, Reward: 1, BF Gain pred: 189.79, BF Gain: 189.79\n",
      "Training for 500 iteration for each Beam uses 146.7683596611023 seconds.\n",
      "Beam: 0, Iter: 10001, Reward pred: 1, Reward: 1, BF Gain pred: 159.88, BF Gain: 159.88\n",
      "Beam: 1, Iter: 10001, Reward pred: 1, Reward: 1, BF Gain pred: 105.11, BF Gain: 105.11\n",
      "Beam: 2, Iter: 10001, Reward pred: -1, Reward: -1, BF Gain pred: 219.96, BF Gain: 219.96\n",
      "Beam: 3, Iter: 10001, Reward pred: -1, Reward: -1, BF Gain pred: 197.23, BF Gain: 197.23\n",
      "Training for 500 iteration for each Beam uses 147.63400197029114 seconds.\n",
      "Beam: 0, Iter: 10501, Reward pred: -1, Reward: -1, BF Gain pred: 149.27, BF Gain: 149.27\n",
      "Beam: 1, Iter: 10501, Reward pred: -1, Reward: -1, BF Gain pred: 103.76, BF Gain: 103.76\n",
      "Beam: 2, Iter: 10501, Reward pred: 1, Reward: 1, BF Gain pred: 237.65, BF Gain: 237.65\n",
      "Beam: 3, Iter: 10501, Reward pred: -1, Reward: -1, BF Gain pred: 210.32, BF Gain: 210.32\n",
      "Training for 500 iteration for each Beam uses 149.69923615455627 seconds.\n",
      "Beam: 0, Iter: 11001, Reward pred: -1, Reward: -1, BF Gain pred: 158.82, BF Gain: 158.82\n",
      "Beam: 1, Iter: 11001, Reward pred: -1, Reward: -1, BF Gain pred: 104.74, BF Gain: 104.74\n",
      "Beam: 2, Iter: 11001, Reward pred: 1, Reward: 1, BF Gain pred: 236.81, BF Gain: 236.81\n",
      "Beam: 3, Iter: 11001, Reward pred: -1, Reward: -1, BF Gain pred: 206.83, BF Gain: 206.83\n",
      "Training for 500 iteration for each Beam uses 148.68209528923035 seconds.\n",
      "Beam: 0, Iter: 11501, Reward pred: -1, Reward: -1, BF Gain pred: 158.78, BF Gain: 158.78\n",
      "Beam: 1, Iter: 11501, Reward pred: 1, Reward: 1, BF Gain pred: 106.13, BF Gain: 106.13\n",
      "Beam: 2, Iter: 11501, Reward pred: 1, Reward: 1, BF Gain pred: 235.18, BF Gain: 235.18\n",
      "Beam: 3, Iter: 11501, Reward pred: 1, Reward: 1, BF Gain pred: 211.66, BF Gain: 211.66\n",
      "Training for 500 iteration for each Beam uses 148.2496519088745 seconds.\n",
      "Beam: 0, Iter: 12001, Reward pred: 1, Reward: 1, BF Gain pred: 162.47, BF Gain: 162.47\n",
      "Beam: 1, Iter: 12001, Reward pred: 1, Reward: 1, BF Gain pred: 100.07, BF Gain: 100.07\n",
      "Beam: 2, Iter: 12001, Reward pred: -1, Reward: -1, BF Gain pred: 228.67, BF Gain: 228.67\n",
      "Beam: 3, Iter: 12001, Reward pred: 1, Reward: 1, BF Gain pred: 206.81, BF Gain: 206.81\n",
      "Training for 500 iteration for each Beam uses 149.18733954429626 seconds.\n",
      "Beam: 0, Iter: 12501, Reward pred: -1, Reward: -1, BF Gain pred: 159.61, BF Gain: 159.61\n",
      "Beam: 1, Iter: 12501, Reward pred: -1, Reward: -1, BF Gain pred: 104.32, BF Gain: 104.32\n",
      "Beam: 2, Iter: 12501, Reward pred: 1, Reward: 1, BF Gain pred: 232.53, BF Gain: 232.53\n",
      "Beam: 3, Iter: 12501, Reward pred: 1, Reward: 1, BF Gain pred: 211.43, BF Gain: 211.43\n",
      "Training for 500 iteration for each Beam uses 147.8442885875702 seconds.\n",
      "Beam: 0, Iter: 13001, Reward pred: 1, Reward: 1, BF Gain pred: 162.70, BF Gain: 162.70\n",
      "Beam: 1, Iter: 13001, Reward pred: 1, Reward: 1, BF Gain pred: 105.59, BF Gain: 105.59\n",
      "Beam: 2, Iter: 13001, Reward pred: -1, Reward: -1, BF Gain pred: 233.64, BF Gain: 233.64\n",
      "Beam: 3, Iter: 13001, Reward pred: -1, Reward: -1, BF Gain pred: 211.62, BF Gain: 211.62\n",
      "Training for 500 iteration for each Beam uses 147.70789980888367 seconds.\n",
      "Beam: 0, Iter: 13501, Reward pred: -1, Reward: -1, BF Gain pred: 157.42, BF Gain: 157.42\n",
      "Beam: 1, Iter: 13501, Reward pred: 1, Reward: 1, BF Gain pred: 106.63, BF Gain: 106.63\n",
      "Beam: 2, Iter: 13501, Reward pred: 1, Reward: 1, BF Gain pred: 233.44, BF Gain: 233.44\n",
      "Beam: 3, Iter: 13501, Reward pred: -1, Reward: -1, BF Gain pred: 208.36, BF Gain: 208.36\n",
      "Training for 500 iteration for each Beam uses 147.58457851409912 seconds.\n",
      "Beam: 0, Iter: 14001, Reward pred: 1, Reward: 1, BF Gain pred: 162.78, BF Gain: 162.78\n",
      "Beam: 1, Iter: 14001, Reward pred: -1, Reward: -1, BF Gain pred: 102.35, BF Gain: 102.35\n",
      "Beam: 2, Iter: 14001, Reward pred: -1, Reward: -1, BF Gain pred: 230.10, BF Gain: 230.10\n",
      "Beam: 3, Iter: 14001, Reward pred: 1, Reward: 1, BF Gain pred: 212.51, BF Gain: 212.51\n",
      "Training for 500 iteration for each Beam uses 147.40463709831238 seconds.\n",
      "Beam: 0, Iter: 14501, Reward pred: 1, Reward: 1, BF Gain pred: 155.44, BF Gain: 155.44\n",
      "Beam: 1, Iter: 14501, Reward pred: 1, Reward: 1, BF Gain pred: 104.89, BF Gain: 104.89\n",
      "Beam: 2, Iter: 14501, Reward pred: 1, Reward: 1, BF Gain pred: 234.49, BF Gain: 234.49\n",
      "Beam: 3, Iter: 14501, Reward pred: -1, Reward: -1, BF Gain pred: 209.65, BF Gain: 209.65\n",
      "Training for 500 iteration for each Beam uses 148.83098220825195 seconds.\n",
      "Beam: 0, Iter: 15001, Reward pred: 1, Reward: 1, BF Gain pred: 164.64, BF Gain: 164.64\n",
      "Beam: 1, Iter: 15001, Reward pred: 1, Reward: 1, BF Gain pred: 107.84, BF Gain: 107.84\n",
      "Beam: 2, Iter: 15001, Reward pred: -1, Reward: -1, BF Gain pred: 232.20, BF Gain: 232.20\n",
      "Beam: 3, Iter: 15001, Reward pred: 1, Reward: 1, BF Gain pred: 211.05, BF Gain: 211.05\n",
      "Training for 500 iteration for each Beam uses 148.40866446495056 seconds.\n",
      "Beam: 0, Iter: 15501, Reward pred: -1, Reward: -1, BF Gain pred: 163.72, BF Gain: 163.72\n",
      "Beam: 1, Iter: 15501, Reward pred: -1, Reward: -1, BF Gain pred: 107.10, BF Gain: 107.10\n",
      "Beam: 2, Iter: 15501, Reward pred: -1, Reward: -1, BF Gain pred: 232.41, BF Gain: 232.41\n",
      "Beam: 3, Iter: 15501, Reward pred: 1, Reward: 1, BF Gain pred: 215.19, BF Gain: 215.19\n",
      "Training for 500 iteration for each Beam uses 148.20219445228577 seconds.\n",
      "Beam: 0, Iter: 16001, Reward pred: 1, Reward: 1, BF Gain pred: 162.68, BF Gain: 162.68\n",
      "Beam: 1, Iter: 16001, Reward pred: 1, Reward: 1, BF Gain pred: 104.87, BF Gain: 104.87\n",
      "Beam: 2, Iter: 16001, Reward pred: -1, Reward: -1, BF Gain pred: 233.62, BF Gain: 233.62\n",
      "Beam: 3, Iter: 16001, Reward pred: 1, Reward: 1, BF Gain pred: 209.17, BF Gain: 209.17\n",
      "Training for 500 iteration for each Beam uses 149.4749631881714 seconds.\n",
      "Beam: 0, Iter: 16501, Reward pred: -1, Reward: -1, BF Gain pred: 161.26, BF Gain: 161.26\n",
      "Beam: 1, Iter: 16501, Reward pred: -1, Reward: -1, BF Gain pred: 103.87, BF Gain: 103.87\n",
      "Beam: 2, Iter: 16501, Reward pred: -1, Reward: -1, BF Gain pred: 235.49, BF Gain: 235.49\n",
      "Beam: 3, Iter: 16501, Reward pred: -1, Reward: -1, BF Gain pred: 214.67, BF Gain: 214.67\n",
      "Training for 500 iteration for each Beam uses 147.02561712265015 seconds.\n",
      "Beam: 0, Iter: 17001, Reward pred: 1, Reward: 1, BF Gain pred: 157.17, BF Gain: 157.17\n",
      "Beam: 1, Iter: 17001, Reward pred: -1, Reward: -1, BF Gain pred: 106.02, BF Gain: 106.02\n",
      "Beam: 2, Iter: 17001, Reward pred: -1, Reward: -1, BF Gain pred: 234.68, BF Gain: 234.68\n",
      "Beam: 3, Iter: 17001, Reward pred: 1, Reward: 1, BF Gain pred: 208.57, BF Gain: 208.57\n",
      "Training for 500 iteration for each Beam uses 146.43828535079956 seconds.\n",
      "Beam: 0, Iter: 17501, Reward pred: 1, Reward: 1, BF Gain pred: 159.18, BF Gain: 159.18\n",
      "Beam: 1, Iter: 17501, Reward pred: 1, Reward: 1, BF Gain pred: 106.45, BF Gain: 106.45\n",
      "Beam: 2, Iter: 17501, Reward pred: -1, Reward: -1, BF Gain pred: 231.02, BF Gain: 231.02\n",
      "Beam: 3, Iter: 17501, Reward pred: -1, Reward: -1, BF Gain pred: 208.71, BF Gain: 208.71\n",
      "Training for 500 iteration for each Beam uses 145.85820174217224 seconds.\n",
      "Beam: 0, Iter: 18001, Reward pred: 1, Reward: 1, BF Gain pred: 162.32, BF Gain: 162.32\n",
      "Beam: 1, Iter: 18001, Reward pred: 1, Reward: 1, BF Gain pred: 106.32, BF Gain: 106.32\n",
      "Beam: 2, Iter: 18001, Reward pred: -1, Reward: -1, BF Gain pred: 233.89, BF Gain: 233.89\n",
      "Beam: 3, Iter: 18001, Reward pred: 1, Reward: 1, BF Gain pred: 214.42, BF Gain: 214.42\n",
      "Training for 500 iteration for each Beam uses 146.44449281692505 seconds.\n",
      "Beam: 0, Iter: 18501, Reward pred: 1, Reward: 1, BF Gain pred: 162.42, BF Gain: 162.42\n",
      "Beam: 1, Iter: 18501, Reward pred: -1, Reward: -1, BF Gain pred: 104.11, BF Gain: 104.11\n",
      "Beam: 2, Iter: 18501, Reward pred: -1, Reward: -1, BF Gain pred: 229.80, BF Gain: 229.80\n",
      "Beam: 3, Iter: 18501, Reward pred: 1, Reward: 1, BF Gain pred: 206.30, BF Gain: 206.30\n",
      "Training for 500 iteration for each Beam uses 146.1472671031952 seconds.\n",
      "Beam: 0, Iter: 19001, Reward pred: -1, Reward: -1, BF Gain pred: 161.62, BF Gain: 161.62\n",
      "Beam: 1, Iter: 19001, Reward pred: 1, Reward: 1, BF Gain pred: 106.80, BF Gain: 106.80\n",
      "Beam: 2, Iter: 19001, Reward pred: -1, Reward: -1, BF Gain pred: 231.30, BF Gain: 231.30\n",
      "Beam: 3, Iter: 19001, Reward pred: -1, Reward: -1, BF Gain pred: 213.40, BF Gain: 213.40\n",
      "Training for 500 iteration for each Beam uses 147.59286236763 seconds.\n",
      "Beam: 0, Iter: 19501, Reward pred: -1, Reward: -1, BF Gain pred: 160.89, BF Gain: 160.89\n",
      "Beam: 1, Iter: 19501, Reward pred: -1, Reward: -1, BF Gain pred: 106.03, BF Gain: 106.03\n",
      "Beam: 2, Iter: 19501, Reward pred: -1, Reward: -1, BF Gain pred: 229.54, BF Gain: 229.54\n",
      "Beam: 3, Iter: 19501, Reward pred: -1, Reward: -1, BF Gain pred: 211.35, BF Gain: 211.35\n",
      "Training for 500 iteration for each Beam uses 148.81513929367065 seconds.\n",
      "Beam: 0, Iter: 20001, Reward pred: -1, Reward: -1, BF Gain pred: 159.04, BF Gain: 159.04\n",
      "Beam: 1, Iter: 20001, Reward pred: 1, Reward: 1, BF Gain pred: 106.51, BF Gain: 106.51\n",
      "Beam: 2, Iter: 20001, Reward pred: -1, Reward: -1, BF Gain pred: 226.73, BF Gain: 226.73\n",
      "Beam: 3, Iter: 20001, Reward pred: 1, Reward: 1, BF Gain pred: 210.44, BF Gain: 210.44\n",
      "Training for 500 iteration for each Beam uses 148.12895250320435 seconds.\n",
      "Beam: 0, Iter: 20501, Reward pred: 1, Reward: 1, BF Gain pred: 159.20, BF Gain: 159.20\n",
      "Beam: 1, Iter: 20501, Reward pred: -1, Reward: -1, BF Gain pred: 104.79, BF Gain: 104.79\n",
      "Beam: 2, Iter: 20501, Reward pred: -1, Reward: -1, BF Gain pred: 228.07, BF Gain: 228.07\n",
      "Beam: 3, Iter: 20501, Reward pred: -1, Reward: -1, BF Gain pred: 206.24, BF Gain: 206.24\n",
      "Training for 500 iteration for each Beam uses 148.72474598884583 seconds.\n",
      "Beam: 0, Iter: 21001, Reward pred: -1, Reward: -1, BF Gain pred: 162.49, BF Gain: 162.49\n",
      "Beam: 1, Iter: 21001, Reward pred: -1, Reward: -1, BF Gain pred: 105.02, BF Gain: 105.02\n",
      "Beam: 2, Iter: 21001, Reward pred: 1, Reward: 1, BF Gain pred: 235.31, BF Gain: 235.31\n",
      "Beam: 3, Iter: 21001, Reward pred: -1, Reward: -1, BF Gain pred: 208.28, BF Gain: 208.28\n",
      "Training for 500 iteration for each Beam uses 147.01727724075317 seconds.\n",
      "Beam: 0, Iter: 21501, Reward pred: -1, Reward: -1, BF Gain pred: 159.86, BF Gain: 159.86\n",
      "Beam: 1, Iter: 21501, Reward pred: 1, Reward: 1, BF Gain pred: 106.58, BF Gain: 106.58\n",
      "Beam: 2, Iter: 21501, Reward pred: -1, Reward: -1, BF Gain pred: 233.85, BF Gain: 233.85\n",
      "Beam: 3, Iter: 21501, Reward pred: 1, Reward: 1, BF Gain pred: 214.86, BF Gain: 214.86\n",
      "Training for 500 iteration for each Beam uses 145.8472728729248 seconds.\n",
      "Beam: 0, Iter: 22001, Reward pred: 1, Reward: 1, BF Gain pred: 163.09, BF Gain: 163.09\n",
      "Beam: 1, Iter: 22001, Reward pred: -1, Reward: -1, BF Gain pred: 103.93, BF Gain: 103.93\n",
      "Beam: 2, Iter: 22001, Reward pred: -1, Reward: -1, BF Gain pred: 233.65, BF Gain: 233.65\n",
      "Beam: 3, Iter: 22001, Reward pred: 1, Reward: 1, BF Gain pred: 208.71, BF Gain: 208.71\n",
      "Training for 500 iteration for each Beam uses 147.57639861106873 seconds.\n",
      "Beam: 0, Iter: 22501, Reward pred: -1, Reward: -1, BF Gain pred: 161.54, BF Gain: 161.54\n",
      "Beam: 1, Iter: 22501, Reward pred: -1, Reward: -1, BF Gain pred: 107.99, BF Gain: 107.99\n",
      "Beam: 2, Iter: 22501, Reward pred: -1, Reward: -1, BF Gain pred: 232.23, BF Gain: 232.23\n",
      "Beam: 3, Iter: 22501, Reward pred: -1, Reward: -1, BF Gain pred: 211.56, BF Gain: 211.56\n",
      "Training for 500 iteration for each Beam uses 148.33603310585022 seconds.\n",
      "Beam: 0, Iter: 23001, Reward pred: -1, Reward: -1, BF Gain pred: 162.83, BF Gain: 162.83\n",
      "Beam: 1, Iter: 23001, Reward pred: 1, Reward: 1, BF Gain pred: 107.19, BF Gain: 107.19\n",
      "Beam: 2, Iter: 23001, Reward pred: -1, Reward: -1, BF Gain pred: 235.82, BF Gain: 235.82\n",
      "Beam: 3, Iter: 23001, Reward pred: 1, Reward: 1, BF Gain pred: 214.55, BF Gain: 214.55\n",
      "Training for 500 iteration for each Beam uses 147.14235258102417 seconds.\n",
      "Beam: 0, Iter: 23501, Reward pred: -1, Reward: -1, BF Gain pred: 159.21, BF Gain: 159.21\n",
      "Beam: 1, Iter: 23501, Reward pred: 1, Reward: 1, BF Gain pred: 107.46, BF Gain: 107.46\n",
      "Beam: 2, Iter: 23501, Reward pred: 1, Reward: 1, BF Gain pred: 228.09, BF Gain: 228.09\n",
      "Beam: 3, Iter: 23501, Reward pred: 1, Reward: 1, BF Gain pred: 210.96, BF Gain: 210.96\n",
      "Training for 500 iteration for each Beam uses 146.9589409828186 seconds.\n",
      "Beam: 0, Iter: 24001, Reward pred: 1, Reward: 1, BF Gain pred: 162.07, BF Gain: 162.07\n",
      "Beam: 1, Iter: 24001, Reward pred: 1, Reward: 1, BF Gain pred: 106.05, BF Gain: 106.05\n",
      "Beam: 2, Iter: 24001, Reward pred: -1, Reward: -1, BF Gain pred: 233.59, BF Gain: 233.59\n",
      "Beam: 3, Iter: 24001, Reward pred: -1, Reward: -1, BF Gain pred: 206.55, BF Gain: 206.55\n",
      "Training for 500 iteration for each Beam uses 148.91753673553467 seconds.\n",
      "Beam: 0, Iter: 24501, Reward pred: -1, Reward: -1, BF Gain pred: 159.07, BF Gain: 159.07\n",
      "Beam: 1, Iter: 24501, Reward pred: 1, Reward: 1, BF Gain pred: 105.51, BF Gain: 105.51\n",
      "Beam: 2, Iter: 24501, Reward pred: -1, Reward: -1, BF Gain pred: 232.08, BF Gain: 232.08\n",
      "Beam: 3, Iter: 24501, Reward pred: 1, Reward: 1, BF Gain pred: 216.25, BF Gain: 216.25\n",
      "Training for 500 iteration for each Beam uses 150.8485894203186 seconds.\n",
      "Beam: 0, Iter: 25001, Reward pred: 1, Reward: 1, BF Gain pred: 160.89, BF Gain: 160.89\n",
      "Beam: 1, Iter: 25001, Reward pred: -1, Reward: -1, BF Gain pred: 104.61, BF Gain: 104.61\n",
      "Beam: 2, Iter: 25001, Reward pred: 1, Reward: 1, BF Gain pred: 232.90, BF Gain: 232.90\n",
      "Beam: 3, Iter: 25001, Reward pred: 1, Reward: 1, BF Gain pred: 207.59, BF Gain: 207.59\n",
      "Training for 500 iteration for each Beam uses 146.88009357452393 seconds.\n",
      "Beam: 0, Iter: 25501, Reward pred: 1, Reward: 1, BF Gain pred: 161.08, BF Gain: 161.08\n",
      "Beam: 1, Iter: 25501, Reward pred: -1, Reward: -1, BF Gain pred: 107.81, BF Gain: 107.81\n",
      "Beam: 2, Iter: 25501, Reward pred: 1, Reward: 1, BF Gain pred: 238.95, BF Gain: 238.95\n",
      "Beam: 3, Iter: 25501, Reward pred: -1, Reward: -1, BF Gain pred: 204.71, BF Gain: 204.71\n",
      "Training for 500 iteration for each Beam uses 148.24034428596497 seconds.\n",
      "Beam: 0, Iter: 26001, Reward pred: 1, Reward: 1, BF Gain pred: 161.30, BF Gain: 161.30\n",
      "Beam: 1, Iter: 26001, Reward pred: -1, Reward: -1, BF Gain pred: 107.08, BF Gain: 107.08\n",
      "Beam: 2, Iter: 26001, Reward pred: -1, Reward: -1, BF Gain pred: 228.54, BF Gain: 228.54\n",
      "Beam: 3, Iter: 26001, Reward pred: 1, Reward: 1, BF Gain pred: 207.91, BF Gain: 207.91\n",
      "Training for 500 iteration for each Beam uses 147.6861925125122 seconds.\n",
      "Beam: 0, Iter: 26501, Reward pred: 1, Reward: 1, BF Gain pred: 159.87, BF Gain: 159.87\n",
      "Beam: 1, Iter: 26501, Reward pred: 1, Reward: 1, BF Gain pred: 108.32, BF Gain: 108.32\n",
      "Beam: 2, Iter: 26501, Reward pred: 1, Reward: 1, BF Gain pred: 231.95, BF Gain: 231.95\n",
      "Beam: 3, Iter: 26501, Reward pred: -1, Reward: -1, BF Gain pred: 207.23, BF Gain: 207.23\n",
      "Training for 500 iteration for each Beam uses 146.60046243667603 seconds.\n",
      "Beam: 0, Iter: 27001, Reward pred: -1, Reward: -1, BF Gain pred: 158.74, BF Gain: 158.74\n",
      "Beam: 1, Iter: 27001, Reward pred: -1, Reward: -1, BF Gain pred: 107.71, BF Gain: 107.71\n",
      "Beam: 2, Iter: 27001, Reward pred: 1, Reward: 1, BF Gain pred: 237.03, BF Gain: 237.03\n",
      "Beam: 3, Iter: 27001, Reward pred: -1, Reward: -1, BF Gain pred: 210.74, BF Gain: 210.74\n",
      "Training for 500 iteration for each Beam uses 148.27003288269043 seconds.\n",
      "Beam: 0, Iter: 27501, Reward pred: -1, Reward: -1, BF Gain pred: 160.44, BF Gain: 160.44\n",
      "Beam: 1, Iter: 27501, Reward pred: -1, Reward: -1, BF Gain pred: 104.07, BF Gain: 104.07\n",
      "Beam: 2, Iter: 27501, Reward pred: -1, Reward: -1, BF Gain pred: 230.84, BF Gain: 230.84\n",
      "Beam: 3, Iter: 27501, Reward pred: 1, Reward: 1, BF Gain pred: 210.36, BF Gain: 210.36\n",
      "Training for 500 iteration for each Beam uses 147.7181751728058 seconds.\n",
      "Beam: 0, Iter: 28001, Reward pred: 1, Reward: 1, BF Gain pred: 157.78, BF Gain: 157.78\n",
      "Beam: 1, Iter: 28001, Reward pred: 1, Reward: 1, BF Gain pred: 104.98, BF Gain: 104.98\n",
      "Beam: 2, Iter: 28001, Reward pred: 1, Reward: 1, BF Gain pred: 229.36, BF Gain: 229.36\n",
      "Beam: 3, Iter: 28001, Reward pred: 1, Reward: 1, BF Gain pred: 205.04, BF Gain: 205.04\n",
      "Training for 500 iteration for each Beam uses 147.46585488319397 seconds.\n",
      "Beam: 0, Iter: 28501, Reward pred: -1, Reward: -1, BF Gain pred: 158.79, BF Gain: 158.79\n",
      "Beam: 1, Iter: 28501, Reward pred: -1, Reward: -1, BF Gain pred: 102.81, BF Gain: 102.81\n",
      "Beam: 2, Iter: 28501, Reward pred: -1, Reward: -1, BF Gain pred: 233.58, BF Gain: 233.58\n",
      "Beam: 3, Iter: 28501, Reward pred: 1, Reward: 1, BF Gain pred: 212.81, BF Gain: 212.81\n",
      "Training for 500 iteration for each Beam uses 150.1713161468506 seconds.\n",
      "Beam: 0, Iter: 29001, Reward pred: 1, Reward: 1, BF Gain pred: 162.27, BF Gain: 162.27\n",
      "Beam: 1, Iter: 29001, Reward pred: -1, Reward: -1, BF Gain pred: 108.17, BF Gain: 108.17\n",
      "Beam: 2, Iter: 29001, Reward pred: -1, Reward: -1, BF Gain pred: 235.37, BF Gain: 235.37\n",
      "Beam: 3, Iter: 29001, Reward pred: 1, Reward: 1, BF Gain pred: 207.81, BF Gain: 207.81\n",
      "Training for 500 iteration for each Beam uses 148.50906085968018 seconds.\n",
      "Beam: 0, Iter: 29501, Reward pred: -1, Reward: -1, BF Gain pred: 163.69, BF Gain: 163.69\n",
      "Beam: 1, Iter: 29501, Reward pred: 1, Reward: 1, BF Gain pred: 107.05, BF Gain: 107.05\n",
      "Beam: 2, Iter: 29501, Reward pred: -1, Reward: -1, BF Gain pred: 233.86, BF Gain: 233.86\n",
      "Beam: 3, Iter: 29501, Reward pred: -1, Reward: -1, BF Gain pred: 212.35, BF Gain: 212.35\n",
      "Training for 500 iteration for each Beam uses 147.76261615753174 seconds.\n",
      "Beam: 0, Iter: 30001, Reward pred: 1, Reward: 1, BF Gain pred: 161.91, BF Gain: 161.91\n",
      "Beam: 1, Iter: 30001, Reward pred: -1, Reward: -1, BF Gain pred: 108.03, BF Gain: 108.03\n",
      "Beam: 2, Iter: 30001, Reward pred: 1, Reward: 1, BF Gain pred: 234.77, BF Gain: 234.77\n",
      "Beam: 3, Iter: 30001, Reward pred: -1, Reward: -1, BF Gain pred: 210.59, BF Gain: 210.59\n",
      "Training for 500 iteration for each Beam uses 150.58261704444885 seconds.\n",
      "Beam: 0, Iter: 30501, Reward pred: 1, Reward: 1, BF Gain pred: 161.17, BF Gain: 161.17\n",
      "Beam: 1, Iter: 30501, Reward pred: -1, Reward: -1, BF Gain pred: 105.23, BF Gain: 105.23\n",
      "Beam: 2, Iter: 30501, Reward pred: 1, Reward: 1, BF Gain pred: 232.92, BF Gain: 232.92\n",
      "Beam: 3, Iter: 30501, Reward pred: 1, Reward: 1, BF Gain pred: 218.10, BF Gain: 218.10\n",
      "Training for 500 iteration for each Beam uses 147.71191811561584 seconds.\n",
      "Beam: 0, Iter: 31001, Reward pred: 1, Reward: 1, BF Gain pred: 162.56, BF Gain: 162.56\n",
      "Beam: 1, Iter: 31001, Reward pred: 1, Reward: 1, BF Gain pred: 106.52, BF Gain: 106.52\n",
      "Beam: 2, Iter: 31001, Reward pred: 1, Reward: 1, BF Gain pred: 231.48, BF Gain: 231.48\n",
      "Beam: 3, Iter: 31001, Reward pred: 1, Reward: 1, BF Gain pred: 215.20, BF Gain: 215.20\n",
      "Training for 500 iteration for each Beam uses 148.02810263633728 seconds.\n",
      "Beam: 0, Iter: 31501, Reward pred: -1, Reward: -1, BF Gain pred: 161.90, BF Gain: 161.90\n",
      "Beam: 1, Iter: 31501, Reward pred: -1, Reward: -1, BF Gain pred: 103.42, BF Gain: 103.42\n",
      "Beam: 2, Iter: 31501, Reward pred: -1, Reward: -1, BF Gain pred: 234.72, BF Gain: 234.72\n",
      "Beam: 3, Iter: 31501, Reward pred: 1, Reward: 1, BF Gain pred: 217.17, BF Gain: 217.17\n",
      "Training for 500 iteration for each Beam uses 147.80867838859558 seconds.\n",
      "Beam: 0, Iter: 32001, Reward pred: 1, Reward: 1, BF Gain pred: 163.97, BF Gain: 163.97\n",
      "Beam: 1, Iter: 32001, Reward pred: -1, Reward: -1, BF Gain pred: 105.17, BF Gain: 105.17\n",
      "Beam: 2, Iter: 32001, Reward pred: 1, Reward: 1, BF Gain pred: 227.73, BF Gain: 227.73\n",
      "Beam: 3, Iter: 32001, Reward pred: -1, Reward: -1, BF Gain pred: 210.04, BF Gain: 210.04\n",
      "Training for 500 iteration for each Beam uses 148.58706521987915 seconds.\n",
      "Beam: 0, Iter: 32501, Reward pred: -1, Reward: -1, BF Gain pred: 162.77, BF Gain: 162.77\n",
      "Beam: 1, Iter: 32501, Reward pred: -1, Reward: -1, BF Gain pred: 105.31, BF Gain: 105.31\n",
      "Beam: 2, Iter: 32501, Reward pred: -1, Reward: -1, BF Gain pred: 229.01, BF Gain: 229.01\n",
      "Beam: 3, Iter: 32501, Reward pred: -1, Reward: -1, BF Gain pred: 209.57, BF Gain: 209.57\n",
      "Training for 500 iteration for each Beam uses 149.07627749443054 seconds.\n",
      "Beam: 0, Iter: 33001, Reward pred: 1, Reward: 1, BF Gain pred: 159.49, BF Gain: 159.49\n",
      "Beam: 1, Iter: 33001, Reward pred: -1, Reward: -1, BF Gain pred: 104.40, BF Gain: 104.40\n",
      "Beam: 2, Iter: 33001, Reward pred: 1, Reward: 1, BF Gain pred: 232.47, BF Gain: 232.47\n",
      "Beam: 3, Iter: 33001, Reward pred: -1, Reward: -1, BF Gain pred: 210.05, BF Gain: 210.05\n",
      "Training for 500 iteration for each Beam uses 148.72984552383423 seconds.\n",
      "Beam: 0, Iter: 33501, Reward pred: -1, Reward: -1, BF Gain pred: 159.39, BF Gain: 159.39\n",
      "Beam: 1, Iter: 33501, Reward pred: -1, Reward: -1, BF Gain pred: 106.80, BF Gain: 106.80\n",
      "Beam: 2, Iter: 33501, Reward pred: -1, Reward: -1, BF Gain pred: 232.41, BF Gain: 232.41\n",
      "Beam: 3, Iter: 33501, Reward pred: -1, Reward: -1, BF Gain pred: 212.46, BF Gain: 212.46\n",
      "Training for 500 iteration for each Beam uses 148.70367169380188 seconds.\n",
      "Beam: 0, Iter: 34001, Reward pred: -1, Reward: -1, BF Gain pred: 159.03, BF Gain: 159.03\n",
      "Beam: 1, Iter: 34001, Reward pred: -1, Reward: -1, BF Gain pred: 105.48, BF Gain: 105.48\n",
      "Beam: 2, Iter: 34001, Reward pred: -1, Reward: -1, BF Gain pred: 234.81, BF Gain: 234.81\n",
      "Beam: 3, Iter: 34001, Reward pred: -1, Reward: -1, BF Gain pred: 216.17, BF Gain: 216.17\n",
      "Training for 500 iteration for each Beam uses 148.16266226768494 seconds.\n",
      "Beam: 0, Iter: 34501, Reward pred: -1, Reward: -1, BF Gain pred: 163.90, BF Gain: 163.90\n",
      "Beam: 1, Iter: 34501, Reward pred: 1, Reward: 1, BF Gain pred: 105.60, BF Gain: 105.60\n",
      "Beam: 2, Iter: 34501, Reward pred: -1, Reward: -1, BF Gain pred: 231.30, BF Gain: 231.30\n",
      "Beam: 3, Iter: 34501, Reward pred: 1, Reward: 1, BF Gain pred: 211.50, BF Gain: 211.50\n",
      "Training for 500 iteration for each Beam uses 150.810884475708 seconds.\n",
      "Beam: 0, Iter: 35001, Reward pred: 1, Reward: 1, BF Gain pred: 161.87, BF Gain: 161.87\n",
      "Beam: 1, Iter: 35001, Reward pred: -1, Reward: -1, BF Gain pred: 107.35, BF Gain: 107.35\n",
      "Beam: 2, Iter: 35001, Reward pred: -1, Reward: -1, BF Gain pred: 233.23, BF Gain: 233.23\n",
      "Beam: 3, Iter: 35001, Reward pred: 1, Reward: 1, BF Gain pred: 217.87, BF Gain: 217.87\n",
      "Training for 500 iteration for each Beam uses 148.84333324432373 seconds.\n",
      "Beam: 0, Iter: 35501, Reward pred: -1, Reward: -1, BF Gain pred: 162.78, BF Gain: 162.78\n",
      "Beam: 1, Iter: 35501, Reward pred: -1, Reward: -1, BF Gain pred: 106.97, BF Gain: 106.97\n",
      "Beam: 2, Iter: 35501, Reward pred: -1, Reward: -1, BF Gain pred: 233.79, BF Gain: 233.79\n",
      "Beam: 3, Iter: 35501, Reward pred: 1, Reward: 1, BF Gain pred: 219.86, BF Gain: 219.86\n",
      "Training for 500 iteration for each Beam uses 148.85380578041077 seconds.\n",
      "Beam: 0, Iter: 36001, Reward pred: -1, Reward: -1, BF Gain pred: 157.44, BF Gain: 157.44\n",
      "Beam: 1, Iter: 36001, Reward pred: 1, Reward: 1, BF Gain pred: 104.10, BF Gain: 104.10\n",
      "Beam: 2, Iter: 36001, Reward pred: 1, Reward: 1, BF Gain pred: 233.54, BF Gain: 233.54\n",
      "Beam: 3, Iter: 36001, Reward pred: 1, Reward: 1, BF Gain pred: 216.80, BF Gain: 216.80\n",
      "Training for 500 iteration for each Beam uses 149.70869851112366 seconds.\n",
      "Beam: 0, Iter: 36501, Reward pred: -1, Reward: -1, BF Gain pred: 163.76, BF Gain: 163.76\n",
      "Beam: 1, Iter: 36501, Reward pred: 1, Reward: 1, BF Gain pred: 107.13, BF Gain: 107.13\n",
      "Beam: 2, Iter: 36501, Reward pred: -1, Reward: -1, BF Gain pred: 231.08, BF Gain: 231.08\n",
      "Beam: 3, Iter: 36501, Reward pred: -1, Reward: -1, BF Gain pred: 205.67, BF Gain: 205.67\n",
      "Training for 500 iteration for each Beam uses 151.8769772052765 seconds.\n",
      "Beam: 0, Iter: 37001, Reward pred: -1, Reward: -1, BF Gain pred: 159.77, BF Gain: 159.77\n",
      "Beam: 1, Iter: 37001, Reward pred: -1, Reward: -1, BF Gain pred: 104.44, BF Gain: 104.44\n",
      "Beam: 2, Iter: 37001, Reward pred: 1, Reward: 1, BF Gain pred: 238.04, BF Gain: 238.04\n",
      "Beam: 3, Iter: 37001, Reward pred: -1, Reward: -1, BF Gain pred: 211.81, BF Gain: 211.81\n",
      "Training for 500 iteration for each Beam uses 157.25538635253906 seconds.\n",
      "Beam: 0, Iter: 37501, Reward pred: 1, Reward: 1, BF Gain pred: 162.06, BF Gain: 162.06\n",
      "Beam: 1, Iter: 37501, Reward pred: 1, Reward: 1, BF Gain pred: 107.85, BF Gain: 107.85\n",
      "Beam: 2, Iter: 37501, Reward pred: 1, Reward: 1, BF Gain pred: 232.74, BF Gain: 232.74\n",
      "Beam: 3, Iter: 37501, Reward pred: -1, Reward: -1, BF Gain pred: 218.00, BF Gain: 218.00\n",
      "Training for 500 iteration for each Beam uses 155.18815302848816 seconds.\n",
      "Beam: 0, Iter: 38001, Reward pred: -1, Reward: -1, BF Gain pred: 160.47, BF Gain: 160.47\n",
      "Beam: 1, Iter: 38001, Reward pred: -1, Reward: -1, BF Gain pred: 104.75, BF Gain: 104.75\n",
      "Beam: 2, Iter: 38001, Reward pred: 1, Reward: 1, BF Gain pred: 232.10, BF Gain: 232.10\n",
      "Beam: 3, Iter: 38001, Reward pred: 1, Reward: 1, BF Gain pred: 208.98, BF Gain: 208.98\n",
      "Training for 500 iteration for each Beam uses 153.83530044555664 seconds.\n",
      "Beam: 0, Iter: 38501, Reward pred: 1, Reward: 1, BF Gain pred: 162.79, BF Gain: 162.79\n",
      "Beam: 1, Iter: 38501, Reward pred: 1, Reward: 1, BF Gain pred: 109.27, BF Gain: 109.27\n",
      "Beam: 2, Iter: 38501, Reward pred: 1, Reward: 1, BF Gain pred: 233.84, BF Gain: 233.84\n",
      "Beam: 3, Iter: 38501, Reward pred: 1, Reward: 1, BF Gain pred: 213.00, BF Gain: 213.00\n",
      "Training for 500 iteration for each Beam uses 150.1965353488922 seconds.\n",
      "Beam: 0, Iter: 39001, Reward pred: 1, Reward: 1, BF Gain pred: 161.74, BF Gain: 161.74\n",
      "Beam: 1, Iter: 39001, Reward pred: -1, Reward: -1, BF Gain pred: 105.59, BF Gain: 105.59\n",
      "Beam: 2, Iter: 39001, Reward pred: 1, Reward: 1, BF Gain pred: 228.92, BF Gain: 228.92\n",
      "Beam: 3, Iter: 39001, Reward pred: 1, Reward: 1, BF Gain pred: 206.58, BF Gain: 206.58\n",
      "Training for 500 iteration for each Beam uses 151.4036943912506 seconds.\n",
      "Beam: 0, Iter: 39501, Reward pred: -1, Reward: -1, BF Gain pred: 160.59, BF Gain: 160.59\n",
      "Beam: 1, Iter: 39501, Reward pred: -1, Reward: -1, BF Gain pred: 104.53, BF Gain: 104.53\n",
      "Beam: 2, Iter: 39501, Reward pred: 1, Reward: 1, BF Gain pred: 235.83, BF Gain: 235.83\n",
      "Beam: 3, Iter: 39501, Reward pred: -1, Reward: -1, BF Gain pred: 209.69, BF Gain: 209.69\n",
      "Training for 500 iteration for each Beam uses 152.0638828277588 seconds.\n",
      "Beam: 0, Iter: 40001, Reward pred: 1, Reward: 1, BF Gain pred: 160.83, BF Gain: 160.83\n",
      "Beam: 1, Iter: 40001, Reward pred: -1, Reward: -1, BF Gain pred: 106.74, BF Gain: 106.74\n",
      "Beam: 2, Iter: 40001, Reward pred: -1, Reward: -1, BF Gain pred: 230.11, BF Gain: 230.11\n",
      "Beam: 3, Iter: 40001, Reward pred: -1, Reward: -1, BF Gain pred: 215.81, BF Gain: 215.81\n",
      "Training for 500 iteration for each Beam uses 151.76266026496887 seconds.\n",
      "Beam: 0, Iter: 40501, Reward pred: -1, Reward: -1, BF Gain pred: 160.93, BF Gain: 160.93\n",
      "Beam: 1, Iter: 40501, Reward pred: -1, Reward: -1, BF Gain pred: 108.44, BF Gain: 108.44\n",
      "Beam: 2, Iter: 40501, Reward pred: 1, Reward: 1, BF Gain pred: 235.69, BF Gain: 235.69\n",
      "Beam: 3, Iter: 40501, Reward pred: 1, Reward: 1, BF Gain pred: 210.53, BF Gain: 210.53\n",
      "Training for 500 iteration for each Beam uses 149.2036488056183 seconds.\n",
      "Beam: 0, Iter: 41001, Reward pred: -1, Reward: -1, BF Gain pred: 160.30, BF Gain: 160.30\n",
      "Beam: 1, Iter: 41001, Reward pred: -1, Reward: -1, BF Gain pred: 108.28, BF Gain: 108.28\n",
      "Beam: 2, Iter: 41001, Reward pred: -1, Reward: -1, BF Gain pred: 234.49, BF Gain: 234.49\n",
      "Beam: 3, Iter: 41001, Reward pred: -1, Reward: -1, BF Gain pred: 210.89, BF Gain: 210.89\n",
      "Training for 500 iteration for each Beam uses 148.36369824409485 seconds.\n",
      "Beam: 0, Iter: 41501, Reward pred: -1, Reward: -1, BF Gain pred: 161.08, BF Gain: 161.08\n",
      "Beam: 1, Iter: 41501, Reward pred: -1, Reward: -1, BF Gain pred: 103.98, BF Gain: 103.98\n",
      "Beam: 2, Iter: 41501, Reward pred: 1, Reward: 1, BF Gain pred: 234.62, BF Gain: 234.62\n",
      "Beam: 3, Iter: 41501, Reward pred: -1, Reward: -1, BF Gain pred: 206.52, BF Gain: 206.52\n",
      "Training for 500 iteration for each Beam uses 146.90981483459473 seconds.\n",
      "Beam: 0, Iter: 42001, Reward pred: -1, Reward: -1, BF Gain pred: 159.32, BF Gain: 159.32\n",
      "Beam: 1, Iter: 42001, Reward pred: -1, Reward: -1, BF Gain pred: 106.55, BF Gain: 106.55\n",
      "Beam: 2, Iter: 42001, Reward pred: -1, Reward: -1, BF Gain pred: 235.16, BF Gain: 235.16\n",
      "Beam: 3, Iter: 42001, Reward pred: 1, Reward: 1, BF Gain pred: 210.50, BF Gain: 210.50\n",
      "Training for 500 iteration for each Beam uses 148.73672366142273 seconds.\n",
      "Beam: 0, Iter: 42501, Reward pred: 1, Reward: 1, BF Gain pred: 163.06, BF Gain: 163.06\n",
      "Beam: 1, Iter: 42501, Reward pred: 1, Reward: 1, BF Gain pred: 108.31, BF Gain: 108.31\n",
      "Beam: 2, Iter: 42501, Reward pred: 1, Reward: 1, BF Gain pred: 237.76, BF Gain: 237.76\n",
      "Beam: 3, Iter: 42501, Reward pred: -1, Reward: -1, BF Gain pred: 209.41, BF Gain: 209.41\n",
      "Training for 500 iteration for each Beam uses 149.38723039627075 seconds.\n",
      "Beam: 0, Iter: 43001, Reward pred: -1, Reward: -1, BF Gain pred: 163.27, BF Gain: 163.27\n",
      "Beam: 1, Iter: 43001, Reward pred: 1, Reward: 1, BF Gain pred: 108.48, BF Gain: 108.48\n",
      "Beam: 2, Iter: 43001, Reward pred: -1, Reward: -1, BF Gain pred: 230.11, BF Gain: 230.11\n",
      "Beam: 3, Iter: 43001, Reward pred: -1, Reward: -1, BF Gain pred: 208.01, BF Gain: 208.01\n",
      "Training for 500 iteration for each Beam uses 147.28244876861572 seconds.\n",
      "Beam: 0, Iter: 43501, Reward pred: -1, Reward: -1, BF Gain pred: 161.10, BF Gain: 161.10\n",
      "Beam: 1, Iter: 43501, Reward pred: 1, Reward: 1, BF Gain pred: 107.16, BF Gain: 107.16\n",
      "Beam: 2, Iter: 43501, Reward pred: 1, Reward: 1, BF Gain pred: 236.13, BF Gain: 236.13\n",
      "Beam: 3, Iter: 43501, Reward pred: -1, Reward: -1, BF Gain pred: 205.57, BF Gain: 205.57\n",
      "Training for 500 iteration for each Beam uses 149.78198885917664 seconds.\n",
      "Beam: 0, Iter: 44001, Reward pred: 1, Reward: 1, BF Gain pred: 163.07, BF Gain: 163.07\n",
      "Beam: 1, Iter: 44001, Reward pred: -1, Reward: -1, BF Gain pred: 104.45, BF Gain: 104.45\n",
      "Beam: 2, Iter: 44001, Reward pred: 1, Reward: 1, BF Gain pred: 234.48, BF Gain: 234.48\n",
      "Beam: 3, Iter: 44001, Reward pred: -1, Reward: -1, BF Gain pred: 212.55, BF Gain: 212.55\n",
      "Training for 500 iteration for each Beam uses 148.19717860221863 seconds.\n",
      "Beam: 0, Iter: 44501, Reward pred: -1, Reward: -1, BF Gain pred: 160.87, BF Gain: 160.87\n",
      "Beam: 1, Iter: 44501, Reward pred: -1, Reward: -1, BF Gain pred: 105.74, BF Gain: 105.74\n",
      "Beam: 2, Iter: 44501, Reward pred: -1, Reward: -1, BF Gain pred: 237.43, BF Gain: 237.43\n",
      "Beam: 3, Iter: 44501, Reward pred: -1, Reward: -1, BF Gain pred: 209.31, BF Gain: 209.31\n",
      "Training for 500 iteration for each Beam uses 148.11881160736084 seconds.\n",
      "Beam: 0, Iter: 45001, Reward pred: 1, Reward: 1, BF Gain pred: 164.17, BF Gain: 164.17\n",
      "Beam: 1, Iter: 45001, Reward pred: 1, Reward: 1, BF Gain pred: 107.87, BF Gain: 107.87\n",
      "Beam: 2, Iter: 45001, Reward pred: -1, Reward: -1, BF Gain pred: 226.09, BF Gain: 226.09\n",
      "Beam: 3, Iter: 45001, Reward pred: 1, Reward: 1, BF Gain pred: 211.19, BF Gain: 211.19\n",
      "Training for 500 iteration for each Beam uses 149.11520624160767 seconds.\n",
      "Beam: 0, Iter: 45501, Reward pred: -1, Reward: -1, BF Gain pred: 158.74, BF Gain: 158.74\n",
      "Beam: 1, Iter: 45501, Reward pred: 1, Reward: 1, BF Gain pred: 109.06, BF Gain: 109.06\n",
      "Beam: 2, Iter: 45501, Reward pred: -1, Reward: -1, BF Gain pred: 236.79, BF Gain: 236.79\n",
      "Beam: 3, Iter: 45501, Reward pred: -1, Reward: -1, BF Gain pred: 206.23, BF Gain: 206.23\n",
      "Training for 500 iteration for each Beam uses 147.97202134132385 seconds.\n",
      "Beam: 0, Iter: 46001, Reward pred: -1, Reward: -1, BF Gain pred: 161.24, BF Gain: 161.24\n",
      "Beam: 1, Iter: 46001, Reward pred: 1, Reward: 1, BF Gain pred: 107.11, BF Gain: 107.11\n",
      "Beam: 2, Iter: 46001, Reward pred: 1, Reward: 1, BF Gain pred: 235.77, BF Gain: 235.77\n",
      "Beam: 3, Iter: 46001, Reward pred: 1, Reward: 1, BF Gain pred: 213.53, BF Gain: 213.53\n",
      "Training for 500 iteration for each Beam uses 147.94103050231934 seconds.\n",
      "Beam: 0, Iter: 46501, Reward pred: -1, Reward: -1, BF Gain pred: 159.25, BF Gain: 159.25\n",
      "Beam: 1, Iter: 46501, Reward pred: -1, Reward: -1, BF Gain pred: 106.21, BF Gain: 106.21\n",
      "Beam: 2, Iter: 46501, Reward pred: 1, Reward: 1, BF Gain pred: 231.91, BF Gain: 231.91\n",
      "Beam: 3, Iter: 46501, Reward pred: -1, Reward: -1, BF Gain pred: 207.97, BF Gain: 207.97\n",
      "Training for 500 iteration for each Beam uses 148.87132120132446 seconds.\n",
      "Beam: 0, Iter: 47001, Reward pred: 1, Reward: 1, BF Gain pred: 165.75, BF Gain: 165.75\n",
      "Beam: 1, Iter: 47001, Reward pred: 1, Reward: 1, BF Gain pred: 107.23, BF Gain: 107.23\n",
      "Beam: 2, Iter: 47001, Reward pred: -1, Reward: -1, BF Gain pred: 224.89, BF Gain: 224.89\n",
      "Beam: 3, Iter: 47001, Reward pred: -1, Reward: -1, BF Gain pred: 210.00, BF Gain: 210.00\n",
      "Training for 500 iteration for each Beam uses 148.84222888946533 seconds.\n",
      "Beam: 0, Iter: 47501, Reward pred: -1, Reward: -1, BF Gain pred: 161.64, BF Gain: 161.64\n",
      "Beam: 1, Iter: 47501, Reward pred: 1, Reward: 1, BF Gain pred: 106.21, BF Gain: 106.21\n",
      "Beam: 2, Iter: 47501, Reward pred: -1, Reward: -1, BF Gain pred: 235.21, BF Gain: 235.21\n",
      "Beam: 3, Iter: 47501, Reward pred: 1, Reward: 1, BF Gain pred: 219.08, BF Gain: 219.08\n",
      "Training for 500 iteration for each Beam uses 148.59656810760498 seconds.\n",
      "Beam: 0, Iter: 48001, Reward pred: -1, Reward: -1, BF Gain pred: 163.66, BF Gain: 163.66\n",
      "Beam: 1, Iter: 48001, Reward pred: 1, Reward: 1, BF Gain pred: 108.14, BF Gain: 108.14\n",
      "Beam: 2, Iter: 48001, Reward pred: -1, Reward: -1, BF Gain pred: 231.31, BF Gain: 231.31\n",
      "Beam: 3, Iter: 48001, Reward pred: 1, Reward: 1, BF Gain pred: 217.07, BF Gain: 217.07\n",
      "Training for 500 iteration for each Beam uses 148.4895315170288 seconds.\n",
      "Beam: 0, Iter: 48501, Reward pred: -1, Reward: -1, BF Gain pred: 162.15, BF Gain: 162.15\n",
      "Beam: 1, Iter: 48501, Reward pred: 1, Reward: 1, BF Gain pred: 105.95, BF Gain: 105.95\n",
      "Beam: 2, Iter: 48501, Reward pred: -1, Reward: -1, BF Gain pred: 228.40, BF Gain: 228.40\n",
      "Beam: 3, Iter: 48501, Reward pred: -1, Reward: -1, BF Gain pred: 210.52, BF Gain: 210.52\n",
      "Training for 500 iteration for each Beam uses 149.25565361976624 seconds.\n",
      "Beam: 0, Iter: 49001, Reward pred: 1, Reward: 1, BF Gain pred: 165.16, BF Gain: 165.16\n",
      "Beam: 1, Iter: 49001, Reward pred: -1, Reward: -1, BF Gain pred: 109.20, BF Gain: 109.20\n",
      "Beam: 2, Iter: 49001, Reward pred: -1, Reward: -1, BF Gain pred: 233.33, BF Gain: 233.33\n",
      "Beam: 3, Iter: 49001, Reward pred: -1, Reward: -1, BF Gain pred: 203.67, BF Gain: 203.67\n",
      "Training for 500 iteration for each Beam uses 150.2842662334442 seconds.\n",
      "Beam: 0, Iter: 49501, Reward pred: -1, Reward: -1, BF Gain pred: 161.70, BF Gain: 161.70\n",
      "Beam: 1, Iter: 49501, Reward pred: 1, Reward: 1, BF Gain pred: 108.29, BF Gain: 108.29\n",
      "Beam: 2, Iter: 49501, Reward pred: -1, Reward: -1, BF Gain pred: 232.84, BF Gain: 232.84\n",
      "Beam: 3, Iter: 49501, Reward pred: 1, Reward: 1, BF Gain pred: 211.69, BF Gain: 211.69\n",
      "Training for 500 iteration for each Beam uses 148.82134556770325 seconds.\n",
      "Beam: 0, Iter: 50001, Reward pred: 1, Reward: 1, BF Gain pred: 164.11, BF Gain: 164.11\n",
      "Beam: 1, Iter: 50001, Reward pred: 1, Reward: 1, BF Gain pred: 109.13, BF Gain: 109.13\n",
      "Beam: 2, Iter: 50001, Reward pred: 1, Reward: 1, BF Gain pred: 231.14, BF Gain: 231.14\n",
      "Beam: 3, Iter: 50001, Reward pred: -1, Reward: -1, BF Gain pred: 204.36, BF Gain: 204.36\n",
      "Training for 500 iteration for each Beam uses 149.20823431015015 seconds.\n",
      "Beam: 0, Iter: 50501, Reward pred: -1, Reward: -1, BF Gain pred: 158.67, BF Gain: 158.67\n",
      "Beam: 1, Iter: 50501, Reward pred: -1, Reward: -1, BF Gain pred: 108.81, BF Gain: 108.81\n",
      "Beam: 2, Iter: 50501, Reward pred: 1, Reward: 1, BF Gain pred: 231.74, BF Gain: 231.74\n",
      "Beam: 3, Iter: 50501, Reward pred: 1, Reward: 1, BF Gain pred: 214.83, BF Gain: 214.83\n",
      "Training for 500 iteration for each Beam uses 149.04566383361816 seconds.\n",
      "Beam: 0, Iter: 51001, Reward pred: -1, Reward: -1, BF Gain pred: 160.78, BF Gain: 160.78\n",
      "Beam: 1, Iter: 51001, Reward pred: 1, Reward: 1, BF Gain pred: 109.25, BF Gain: 109.25\n",
      "Beam: 2, Iter: 51001, Reward pred: 1, Reward: 1, BF Gain pred: 236.02, BF Gain: 236.02\n",
      "Beam: 3, Iter: 51001, Reward pred: 1, Reward: 1, BF Gain pred: 209.67, BF Gain: 209.67\n",
      "Training for 500 iteration for each Beam uses 149.00194835662842 seconds.\n",
      "Beam: 0, Iter: 51501, Reward pred: -1, Reward: -1, BF Gain pred: 161.87, BF Gain: 161.87\n",
      "Beam: 1, Iter: 51501, Reward pred: 1, Reward: 1, BF Gain pred: 108.23, BF Gain: 108.23\n",
      "Beam: 2, Iter: 51501, Reward pred: -1, Reward: -1, BF Gain pred: 233.21, BF Gain: 233.21\n",
      "Beam: 3, Iter: 51501, Reward pred: -1, Reward: -1, BF Gain pred: 209.43, BF Gain: 209.43\n",
      "Training for 500 iteration for each Beam uses 148.56325101852417 seconds.\n",
      "Beam: 0, Iter: 52001, Reward pred: 1, Reward: 1, BF Gain pred: 162.65, BF Gain: 162.65\n",
      "Beam: 1, Iter: 52001, Reward pred: 1, Reward: 1, BF Gain pred: 107.21, BF Gain: 107.21\n",
      "Beam: 2, Iter: 52001, Reward pred: 1, Reward: 1, BF Gain pred: 236.78, BF Gain: 236.78\n",
      "Beam: 3, Iter: 52001, Reward pred: 1, Reward: 1, BF Gain pred: 210.68, BF Gain: 210.68\n",
      "Training for 500 iteration for each Beam uses 148.57199382781982 seconds.\n",
      "Beam: 0, Iter: 52501, Reward pred: 1, Reward: 1, BF Gain pred: 161.14, BF Gain: 161.14\n",
      "Beam: 1, Iter: 52501, Reward pred: 1, Reward: 1, BF Gain pred: 109.13, BF Gain: 109.13\n",
      "Beam: 2, Iter: 52501, Reward pred: -1, Reward: -1, BF Gain pred: 235.68, BF Gain: 235.68\n",
      "Beam: 3, Iter: 52501, Reward pred: 1, Reward: 1, BF Gain pred: 205.82, BF Gain: 205.82\n",
      "Training for 500 iteration for each Beam uses 148.1214838027954 seconds.\n",
      "Beam: 0, Iter: 53001, Reward pred: 1, Reward: 1, BF Gain pred: 160.28, BF Gain: 160.28\n",
      "Beam: 1, Iter: 53001, Reward pred: -1, Reward: -1, BF Gain pred: 108.77, BF Gain: 108.77\n",
      "Beam: 2, Iter: 53001, Reward pred: -1, Reward: -1, BF Gain pred: 231.94, BF Gain: 231.94\n",
      "Beam: 3, Iter: 53001, Reward pred: -1, Reward: -1, BF Gain pred: 211.50, BF Gain: 211.50\n",
      "Training for 500 iteration for each Beam uses 147.97792077064514 seconds.\n",
      "Beam: 0, Iter: 53501, Reward pred: -1, Reward: -1, BF Gain pred: 161.12, BF Gain: 161.12\n",
      "Beam: 1, Iter: 53501, Reward pred: 1, Reward: 1, BF Gain pred: 106.35, BF Gain: 106.35\n",
      "Beam: 2, Iter: 53501, Reward pred: -1, Reward: -1, BF Gain pred: 233.48, BF Gain: 233.48\n",
      "Beam: 3, Iter: 53501, Reward pred: 1, Reward: 1, BF Gain pred: 210.14, BF Gain: 210.14\n",
      "Training for 500 iteration for each Beam uses 148.51309442520142 seconds.\n",
      "Beam: 0, Iter: 54001, Reward pred: -1, Reward: -1, BF Gain pred: 161.95, BF Gain: 161.95\n",
      "Beam: 1, Iter: 54001, Reward pred: -1, Reward: -1, BF Gain pred: 106.89, BF Gain: 106.89\n",
      "Beam: 2, Iter: 54001, Reward pred: 1, Reward: 1, BF Gain pred: 233.80, BF Gain: 233.80\n",
      "Beam: 3, Iter: 54001, Reward pred: -1, Reward: -1, BF Gain pred: 215.33, BF Gain: 215.33\n",
      "Training for 500 iteration for each Beam uses 148.16584491729736 seconds.\n",
      "Beam: 0, Iter: 54501, Reward pred: -1, Reward: -1, BF Gain pred: 163.65, BF Gain: 163.65\n",
      "Beam: 1, Iter: 54501, Reward pred: 1, Reward: 1, BF Gain pred: 108.16, BF Gain: 108.16\n",
      "Beam: 2, Iter: 54501, Reward pred: 1, Reward: 1, BF Gain pred: 227.33, BF Gain: 227.33\n",
      "Beam: 3, Iter: 54501, Reward pred: 1, Reward: 1, BF Gain pred: 210.79, BF Gain: 210.79\n",
      "Training for 500 iteration for each Beam uses 148.75766277313232 seconds.\n",
      "Beam: 0, Iter: 55001, Reward pred: -1, Reward: -1, BF Gain pred: 163.47, BF Gain: 163.47\n",
      "Beam: 1, Iter: 55001, Reward pred: 1, Reward: 1, BF Gain pred: 106.36, BF Gain: 106.36\n",
      "Beam: 2, Iter: 55001, Reward pred: -1, Reward: -1, BF Gain pred: 230.88, BF Gain: 230.88\n",
      "Beam: 3, Iter: 55001, Reward pred: 1, Reward: 1, BF Gain pred: 213.55, BF Gain: 213.55\n",
      "Training for 500 iteration for each Beam uses 149.34040331840515 seconds.\n",
      "Beam: 0, Iter: 55501, Reward pred: 1, Reward: 1, BF Gain pred: 157.91, BF Gain: 157.91\n",
      "Beam: 1, Iter: 55501, Reward pred: -1, Reward: -1, BF Gain pred: 107.46, BF Gain: 107.46\n",
      "Beam: 2, Iter: 55501, Reward pred: -1, Reward: -1, BF Gain pred: 227.96, BF Gain: 227.96\n",
      "Beam: 3, Iter: 55501, Reward pred: -1, Reward: -1, BF Gain pred: 218.02, BF Gain: 218.02\n",
      "Training for 500 iteration for each Beam uses 149.36294269561768 seconds.\n",
      "Beam: 0, Iter: 56001, Reward pred: 1, Reward: 1, BF Gain pred: 162.91, BF Gain: 162.91\n",
      "Beam: 1, Iter: 56001, Reward pred: -1, Reward: -1, BF Gain pred: 105.41, BF Gain: 105.41\n",
      "Beam: 2, Iter: 56001, Reward pred: 1, Reward: 1, BF Gain pred: 233.78, BF Gain: 233.78\n",
      "Beam: 3, Iter: 56001, Reward pred: -1, Reward: -1, BF Gain pred: 212.22, BF Gain: 212.22\n",
      "Training for 500 iteration for each Beam uses 148.45227074623108 seconds.\n",
      "Beam: 0, Iter: 56501, Reward pred: 1, Reward: 1, BF Gain pred: 163.95, BF Gain: 163.95\n",
      "Beam: 1, Iter: 56501, Reward pred: 1, Reward: 1, BF Gain pred: 107.25, BF Gain: 107.25\n",
      "Beam: 2, Iter: 56501, Reward pred: -1, Reward: -1, BF Gain pred: 235.46, BF Gain: 235.46\n",
      "Beam: 3, Iter: 56501, Reward pred: -1, Reward: -1, BF Gain pred: 210.11, BF Gain: 210.11\n",
      "Training for 500 iteration for each Beam uses 147.39456844329834 seconds.\n",
      "Beam: 0, Iter: 57001, Reward pred: 1, Reward: 1, BF Gain pred: 161.09, BF Gain: 161.09\n",
      "Beam: 1, Iter: 57001, Reward pred: 1, Reward: 1, BF Gain pred: 107.94, BF Gain: 107.94\n",
      "Beam: 2, Iter: 57001, Reward pred: -1, Reward: -1, BF Gain pred: 242.15, BF Gain: 242.15\n",
      "Beam: 3, Iter: 57001, Reward pred: 1, Reward: 1, BF Gain pred: 210.83, BF Gain: 210.83\n",
      "Training for 500 iteration for each Beam uses 149.30201649665833 seconds.\n",
      "Beam: 0, Iter: 57501, Reward pred: 1, Reward: 1, BF Gain pred: 161.87, BF Gain: 161.87\n",
      "Beam: 1, Iter: 57501, Reward pred: 1, Reward: 1, BF Gain pred: 106.95, BF Gain: 106.95\n",
      "Beam: 2, Iter: 57501, Reward pred: 1, Reward: 1, BF Gain pred: 238.76, BF Gain: 238.76\n",
      "Beam: 3, Iter: 57501, Reward pred: -1, Reward: -1, BF Gain pred: 208.37, BF Gain: 208.37\n",
      "Training for 500 iteration for each Beam uses 147.76359057426453 seconds.\n",
      "Beam: 0, Iter: 58001, Reward pred: -1, Reward: -1, BF Gain pred: 159.40, BF Gain: 159.40\n",
      "Beam: 1, Iter: 58001, Reward pred: -1, Reward: -1, BF Gain pred: 107.95, BF Gain: 107.95\n",
      "Beam: 2, Iter: 58001, Reward pred: -1, Reward: -1, BF Gain pred: 232.42, BF Gain: 232.42\n",
      "Beam: 3, Iter: 58001, Reward pred: -1, Reward: -1, BF Gain pred: 211.69, BF Gain: 211.69\n",
      "Training for 500 iteration for each Beam uses 146.1972200870514 seconds.\n",
      "Beam: 0, Iter: 58501, Reward pred: 1, Reward: 1, BF Gain pred: 163.16, BF Gain: 163.16\n",
      "Beam: 1, Iter: 58501, Reward pred: -1, Reward: -1, BF Gain pred: 108.03, BF Gain: 108.03\n",
      "Beam: 2, Iter: 58501, Reward pred: -1, Reward: -1, BF Gain pred: 235.13, BF Gain: 235.13\n",
      "Beam: 3, Iter: 58501, Reward pred: -1, Reward: -1, BF Gain pred: 214.70, BF Gain: 214.70\n",
      "Training for 500 iteration for each Beam uses 148.67056846618652 seconds.\n",
      "Beam: 0, Iter: 59001, Reward pred: 1, Reward: 1, BF Gain pred: 163.87, BF Gain: 163.87\n",
      "Beam: 1, Iter: 59001, Reward pred: -1, Reward: -1, BF Gain pred: 108.01, BF Gain: 108.01\n",
      "Beam: 2, Iter: 59001, Reward pred: 1, Reward: 1, BF Gain pred: 238.82, BF Gain: 238.82\n",
      "Beam: 3, Iter: 59001, Reward pred: -1, Reward: -1, BF Gain pred: 208.37, BF Gain: 208.37\n",
      "Training for 500 iteration for each Beam uses 147.8331537246704 seconds.\n",
      "Beam: 0, Iter: 59501, Reward pred: 1, Reward: 1, BF Gain pred: 161.43, BF Gain: 161.43\n",
      "Beam: 1, Iter: 59501, Reward pred: 1, Reward: 1, BF Gain pred: 109.20, BF Gain: 109.20\n",
      "Beam: 2, Iter: 59501, Reward pred: 1, Reward: 1, BF Gain pred: 232.68, BF Gain: 232.68\n",
      "Beam: 3, Iter: 59501, Reward pred: -1, Reward: -1, BF Gain pred: 214.33, BF Gain: 214.33\n",
      "Training for 500 iteration for each Beam uses 147.00595808029175 seconds.\n",
      "Beam: 0, Iter: 60001, Reward pred: -1, Reward: -1, BF Gain pred: 160.79, BF Gain: 160.79\n",
      "Beam: 1, Iter: 60001, Reward pred: -1, Reward: -1, BF Gain pred: 106.75, BF Gain: 106.75\n",
      "Beam: 2, Iter: 60001, Reward pred: 1, Reward: 1, BF Gain pred: 229.44, BF Gain: 229.44\n",
      "Beam: 3, Iter: 60001, Reward pred: 1, Reward: 1, BF Gain pred: 207.75, BF Gain: 207.75\n",
      "Training for 500 iteration for each Beam uses 147.66367387771606 seconds.\n",
      "Beam: 0, Iter: 60501, Reward pred: 1, Reward: 1, BF Gain pred: 163.14, BF Gain: 163.14\n",
      "Beam: 1, Iter: 60501, Reward pred: 1, Reward: 1, BF Gain pred: 107.60, BF Gain: 107.60\n",
      "Beam: 2, Iter: 60501, Reward pred: 1, Reward: 1, BF Gain pred: 238.22, BF Gain: 238.22\n",
      "Beam: 3, Iter: 60501, Reward pred: -1, Reward: -1, BF Gain pred: 212.93, BF Gain: 212.93\n",
      "Training for 500 iteration for each Beam uses 148.42532658576965 seconds.\n",
      "Beam: 0, Iter: 61001, Reward pred: 1, Reward: 1, BF Gain pred: 160.91, BF Gain: 160.91\n",
      "Beam: 1, Iter: 61001, Reward pred: 1, Reward: 1, BF Gain pred: 106.35, BF Gain: 106.35\n",
      "Beam: 2, Iter: 61001, Reward pred: -1, Reward: -1, BF Gain pred: 228.94, BF Gain: 228.94\n",
      "Beam: 3, Iter: 61001, Reward pred: 1, Reward: 1, BF Gain pred: 204.57, BF Gain: 204.57\n",
      "Training for 500 iteration for each Beam uses 147.2328236103058 seconds.\n",
      "Beam: 0, Iter: 61501, Reward pred: 1, Reward: 1, BF Gain pred: 160.94, BF Gain: 160.94\n",
      "Beam: 1, Iter: 61501, Reward pred: -1, Reward: -1, BF Gain pred: 108.52, BF Gain: 108.52\n",
      "Beam: 2, Iter: 61501, Reward pred: -1, Reward: -1, BF Gain pred: 231.15, BF Gain: 231.15\n",
      "Beam: 3, Iter: 61501, Reward pred: 1, Reward: 1, BF Gain pred: 209.23, BF Gain: 209.23\n",
      "Training for 500 iteration for each Beam uses 148.45577454566956 seconds.\n",
      "Beam: 0, Iter: 62001, Reward pred: 1, Reward: 1, BF Gain pred: 163.54, BF Gain: 163.54\n",
      "Beam: 1, Iter: 62001, Reward pred: 1, Reward: 1, BF Gain pred: 106.08, BF Gain: 106.08\n",
      "Beam: 2, Iter: 62001, Reward pred: -1, Reward: -1, BF Gain pred: 235.30, BF Gain: 235.30\n",
      "Beam: 3, Iter: 62001, Reward pred: -1, Reward: -1, BF Gain pred: 211.69, BF Gain: 211.69\n",
      "Training for 500 iteration for each Beam uses 147.96591067314148 seconds.\n",
      "Beam: 0, Iter: 62501, Reward pred: 1, Reward: 1, BF Gain pred: 165.00, BF Gain: 165.00\n",
      "Beam: 1, Iter: 62501, Reward pred: -1, Reward: -1, BF Gain pred: 106.48, BF Gain: 106.48\n",
      "Beam: 2, Iter: 62501, Reward pred: -1, Reward: -1, BF Gain pred: 234.10, BF Gain: 234.10\n",
      "Beam: 3, Iter: 62501, Reward pred: -1, Reward: -1, BF Gain pred: 211.14, BF Gain: 211.14\n",
      "Training for 500 iteration for each Beam uses 147.63355827331543 seconds.\n",
      "Beam: 0, Iter: 63001, Reward pred: 1, Reward: 1, BF Gain pred: 160.56, BF Gain: 160.56\n",
      "Beam: 1, Iter: 63001, Reward pred: -1, Reward: -1, BF Gain pred: 107.21, BF Gain: 107.21\n",
      "Beam: 2, Iter: 63001, Reward pred: -1, Reward: -1, BF Gain pred: 235.17, BF Gain: 235.17\n",
      "Beam: 3, Iter: 63001, Reward pred: -1, Reward: -1, BF Gain pred: 206.78, BF Gain: 206.78\n",
      "Training for 500 iteration for each Beam uses 147.33471727371216 seconds.\n",
      "Beam: 0, Iter: 63501, Reward pred: -1, Reward: -1, BF Gain pred: 161.58, BF Gain: 161.58\n",
      "Beam: 1, Iter: 63501, Reward pred: -1, Reward: -1, BF Gain pred: 107.13, BF Gain: 107.13\n",
      "Beam: 2, Iter: 63501, Reward pred: 1, Reward: 1, BF Gain pred: 237.05, BF Gain: 237.05\n",
      "Beam: 3, Iter: 63501, Reward pred: 1, Reward: 1, BF Gain pred: 209.92, BF Gain: 209.92\n",
      "Training for 500 iteration for each Beam uses 147.8431694507599 seconds.\n",
      "Beam: 0, Iter: 64001, Reward pred: -1, Reward: -1, BF Gain pred: 158.03, BF Gain: 158.03\n",
      "Beam: 1, Iter: 64001, Reward pred: 1, Reward: 1, BF Gain pred: 105.04, BF Gain: 105.04\n",
      "Beam: 2, Iter: 64001, Reward pred: 1, Reward: 1, BF Gain pred: 232.02, BF Gain: 232.02\n",
      "Beam: 3, Iter: 64001, Reward pred: -1, Reward: -1, BF Gain pred: 208.10, BF Gain: 208.10\n",
      "Training for 500 iteration for each Beam uses 148.41807889938354 seconds.\n",
      "Beam: 0, Iter: 64501, Reward pred: 1, Reward: 1, BF Gain pred: 161.54, BF Gain: 161.54\n",
      "Beam: 1, Iter: 64501, Reward pred: 1, Reward: 1, BF Gain pred: 106.15, BF Gain: 106.15\n",
      "Beam: 2, Iter: 64501, Reward pred: -1, Reward: -1, BF Gain pred: 229.19, BF Gain: 229.19\n",
      "Beam: 3, Iter: 64501, Reward pred: 1, Reward: 1, BF Gain pred: 212.25, BF Gain: 212.25\n",
      "Training for 500 iteration for each Beam uses 148.76639413833618 seconds.\n",
      "Beam: 0, Iter: 65001, Reward pred: 1, Reward: 1, BF Gain pred: 158.64, BF Gain: 158.64\n",
      "Beam: 1, Iter: 65001, Reward pred: -1, Reward: -1, BF Gain pred: 107.70, BF Gain: 107.70\n",
      "Beam: 2, Iter: 65001, Reward pred: 1, Reward: 1, BF Gain pred: 239.24, BF Gain: 239.24\n",
      "Beam: 3, Iter: 65001, Reward pred: -1, Reward: -1, BF Gain pred: 214.37, BF Gain: 214.37\n",
      "Training for 500 iteration for each Beam uses 148.4423406124115 seconds.\n",
      "Beam: 0, Iter: 65501, Reward pred: -1, Reward: -1, BF Gain pred: 165.02, BF Gain: 165.02\n",
      "Beam: 1, Iter: 65501, Reward pred: 1, Reward: 1, BF Gain pred: 106.10, BF Gain: 106.10\n",
      "Beam: 2, Iter: 65501, Reward pred: -1, Reward: -1, BF Gain pred: 232.97, BF Gain: 232.97\n",
      "Beam: 3, Iter: 65501, Reward pred: 1, Reward: 1, BF Gain pred: 209.44, BF Gain: 209.44\n",
      "Training for 500 iteration for each Beam uses 148.09784650802612 seconds.\n",
      "Beam: 0, Iter: 66001, Reward pred: 1, Reward: 1, BF Gain pred: 163.33, BF Gain: 163.33\n",
      "Beam: 1, Iter: 66001, Reward pred: -1, Reward: -1, BF Gain pred: 108.04, BF Gain: 108.04\n",
      "Beam: 2, Iter: 66001, Reward pred: -1, Reward: -1, BF Gain pred: 226.88, BF Gain: 226.88\n",
      "Beam: 3, Iter: 66001, Reward pred: -1, Reward: -1, BF Gain pred: 208.94, BF Gain: 208.94\n",
      "Training for 500 iteration for each Beam uses 148.3589324951172 seconds.\n",
      "Beam: 0, Iter: 66501, Reward pred: -1, Reward: -1, BF Gain pred: 162.46, BF Gain: 162.46\n",
      "Beam: 1, Iter: 66501, Reward pred: -1, Reward: -1, BF Gain pred: 105.95, BF Gain: 105.95\n",
      "Beam: 2, Iter: 66501, Reward pred: -1, Reward: -1, BF Gain pred: 228.79, BF Gain: 228.79\n",
      "Beam: 3, Iter: 66501, Reward pred: 1, Reward: 1, BF Gain pred: 210.56, BF Gain: 210.56\n",
      "Training for 500 iteration for each Beam uses 148.74763822555542 seconds.\n",
      "Beam: 0, Iter: 67001, Reward pred: -1, Reward: -1, BF Gain pred: 158.29, BF Gain: 158.29\n",
      "Beam: 1, Iter: 67001, Reward pred: -1, Reward: -1, BF Gain pred: 109.09, BF Gain: 109.09\n",
      "Beam: 2, Iter: 67001, Reward pred: -1, Reward: -1, BF Gain pred: 231.48, BF Gain: 231.48\n",
      "Beam: 3, Iter: 67001, Reward pred: -1, Reward: -1, BF Gain pred: 213.76, BF Gain: 213.76\n",
      "Training for 500 iteration for each Beam uses 149.36015582084656 seconds.\n",
      "Beam: 0, Iter: 67501, Reward pred: -1, Reward: -1, BF Gain pred: 165.04, BF Gain: 165.04\n",
      "Beam: 1, Iter: 67501, Reward pred: 1, Reward: 1, BF Gain pred: 107.39, BF Gain: 107.39\n",
      "Beam: 2, Iter: 67501, Reward pred: -1, Reward: -1, BF Gain pred: 235.70, BF Gain: 235.70\n",
      "Beam: 3, Iter: 67501, Reward pred: -1, Reward: -1, BF Gain pred: 212.93, BF Gain: 212.93\n",
      "Training for 500 iteration for each Beam uses 149.376403093338 seconds.\n",
      "Beam: 0, Iter: 68001, Reward pred: 1, Reward: 1, BF Gain pred: 162.80, BF Gain: 162.80\n",
      "Beam: 1, Iter: 68001, Reward pred: 1, Reward: 1, BF Gain pred: 107.24, BF Gain: 107.24\n",
      "Beam: 2, Iter: 68001, Reward pred: 1, Reward: 1, BF Gain pred: 231.21, BF Gain: 231.21\n",
      "Beam: 3, Iter: 68001, Reward pred: -1, Reward: -1, BF Gain pred: 210.81, BF Gain: 210.81\n",
      "Training for 500 iteration for each Beam uses 149.83553910255432 seconds.\n",
      "Beam: 0, Iter: 68501, Reward pred: -1, Reward: -1, BF Gain pred: 161.33, BF Gain: 161.33\n",
      "Beam: 1, Iter: 68501, Reward pred: -1, Reward: -1, BF Gain pred: 106.55, BF Gain: 106.55\n",
      "Beam: 2, Iter: 68501, Reward pred: -1, Reward: -1, BF Gain pred: 231.70, BF Gain: 231.70\n",
      "Beam: 3, Iter: 68501, Reward pred: -1, Reward: -1, BF Gain pred: 213.70, BF Gain: 213.70\n",
      "Training for 500 iteration for each Beam uses 149.36327958106995 seconds.\n",
      "Beam: 0, Iter: 69001, Reward pred: -1, Reward: -1, BF Gain pred: 160.60, BF Gain: 160.60\n",
      "Beam: 1, Iter: 69001, Reward pred: 1, Reward: 1, BF Gain pred: 106.15, BF Gain: 106.15\n",
      "Beam: 2, Iter: 69001, Reward pred: 1, Reward: 1, BF Gain pred: 231.49, BF Gain: 231.49\n",
      "Beam: 3, Iter: 69001, Reward pred: -1, Reward: -1, BF Gain pred: 213.00, BF Gain: 213.00\n",
      "Training for 500 iteration for each Beam uses 149.43907237052917 seconds.\n",
      "Beam: 0, Iter: 69501, Reward pred: -1, Reward: -1, BF Gain pred: 161.39, BF Gain: 161.39\n",
      "Beam: 1, Iter: 69501, Reward pred: 1, Reward: 1, BF Gain pred: 104.96, BF Gain: 104.96\n",
      "Beam: 2, Iter: 69501, Reward pred: 1, Reward: 1, BF Gain pred: 237.58, BF Gain: 237.58\n",
      "Beam: 3, Iter: 69501, Reward pred: 1, Reward: 1, BF Gain pred: 212.93, BF Gain: 212.93\n",
      "Training for 500 iteration for each Beam uses 148.89769411087036 seconds.\n",
      "Beam: 0, Iter: 70001, Reward pred: -1, Reward: -1, BF Gain pred: 160.73, BF Gain: 160.73\n",
      "Beam: 1, Iter: 70001, Reward pred: -1, Reward: -1, BF Gain pred: 106.17, BF Gain: 106.17\n",
      "Beam: 2, Iter: 70001, Reward pred: 1, Reward: 1, BF Gain pred: 238.19, BF Gain: 238.19\n",
      "Beam: 3, Iter: 70001, Reward pred: 1, Reward: 1, BF Gain pred: 215.54, BF Gain: 215.54\n",
      "Training for 500 iteration for each Beam uses 148.41435885429382 seconds.\n",
      "Beam: 0, Iter: 70501, Reward pred: 1, Reward: 1, BF Gain pred: 161.96, BF Gain: 161.96\n",
      "Beam: 1, Iter: 70501, Reward pred: 1, Reward: 1, BF Gain pred: 109.30, BF Gain: 109.30\n",
      "Beam: 2, Iter: 70501, Reward pred: -1, Reward: -1, BF Gain pred: 229.52, BF Gain: 229.52\n",
      "Beam: 3, Iter: 70501, Reward pred: -1, Reward: -1, BF Gain pred: 215.80, BF Gain: 215.80\n",
      "Training for 500 iteration for each Beam uses 147.2198874950409 seconds.\n",
      "Beam: 0, Iter: 71001, Reward pred: -1, Reward: -1, BF Gain pred: 162.48, BF Gain: 162.48\n",
      "Beam: 1, Iter: 71001, Reward pred: 1, Reward: 1, BF Gain pred: 107.60, BF Gain: 107.60\n",
      "Beam: 2, Iter: 71001, Reward pred: -1, Reward: -1, BF Gain pred: 230.86, BF Gain: 230.86\n",
      "Beam: 3, Iter: 71001, Reward pred: -1, Reward: -1, BF Gain pred: 214.35, BF Gain: 214.35\n",
      "Training for 500 iteration for each Beam uses 148.33988976478577 seconds.\n",
      "Beam: 0, Iter: 71501, Reward pred: -1, Reward: -1, BF Gain pred: 160.87, BF Gain: 160.87\n",
      "Beam: 1, Iter: 71501, Reward pred: -1, Reward: -1, BF Gain pred: 107.04, BF Gain: 107.04\n",
      "Beam: 2, Iter: 71501, Reward pred: 1, Reward: 1, BF Gain pred: 232.48, BF Gain: 232.48\n",
      "Beam: 3, Iter: 71501, Reward pred: -1, Reward: -1, BF Gain pred: 216.61, BF Gain: 216.61\n",
      "Training for 500 iteration for each Beam uses 147.8365306854248 seconds.\n",
      "Beam: 0, Iter: 72001, Reward pred: -1, Reward: -1, BF Gain pred: 162.11, BF Gain: 162.11\n",
      "Beam: 1, Iter: 72001, Reward pred: -1, Reward: -1, BF Gain pred: 106.34, BF Gain: 106.34\n",
      "Beam: 2, Iter: 72001, Reward pred: -1, Reward: -1, BF Gain pred: 230.58, BF Gain: 230.58\n",
      "Beam: 3, Iter: 72001, Reward pred: -1, Reward: -1, BF Gain pred: 209.72, BF Gain: 209.72\n",
      "Training for 500 iteration for each Beam uses 147.47230696678162 seconds.\n",
      "Beam: 0, Iter: 72501, Reward pred: 1, Reward: 1, BF Gain pred: 163.37, BF Gain: 163.37\n",
      "Beam: 1, Iter: 72501, Reward pred: -1, Reward: -1, BF Gain pred: 106.53, BF Gain: 106.53\n",
      "Beam: 2, Iter: 72501, Reward pred: 1, Reward: 1, BF Gain pred: 233.64, BF Gain: 233.64\n",
      "Beam: 3, Iter: 72501, Reward pred: 1, Reward: 1, BF Gain pred: 215.13, BF Gain: 215.13\n",
      "Training for 500 iteration for each Beam uses 149.3125958442688 seconds.\n",
      "Beam: 0, Iter: 73001, Reward pred: -1, Reward: -1, BF Gain pred: 159.66, BF Gain: 159.66\n",
      "Beam: 1, Iter: 73001, Reward pred: 1, Reward: 1, BF Gain pred: 106.07, BF Gain: 106.07\n",
      "Beam: 2, Iter: 73001, Reward pred: 1, Reward: 1, BF Gain pred: 233.70, BF Gain: 233.70\n",
      "Beam: 3, Iter: 73001, Reward pred: -1, Reward: -1, BF Gain pred: 207.85, BF Gain: 207.85\n",
      "Training for 500 iteration for each Beam uses 149.4966597557068 seconds.\n",
      "Beam: 0, Iter: 73501, Reward pred: 1, Reward: 1, BF Gain pred: 164.30, BF Gain: 164.30\n",
      "Beam: 1, Iter: 73501, Reward pred: 1, Reward: 1, BF Gain pred: 108.11, BF Gain: 108.11\n",
      "Beam: 2, Iter: 73501, Reward pred: 1, Reward: 1, BF Gain pred: 232.45, BF Gain: 232.45\n",
      "Beam: 3, Iter: 73501, Reward pred: 1, Reward: 1, BF Gain pred: 208.59, BF Gain: 208.59\n",
      "Training for 500 iteration for each Beam uses 148.82032442092896 seconds.\n",
      "Beam: 0, Iter: 74001, Reward pred: 1, Reward: 1, BF Gain pred: 164.86, BF Gain: 164.86\n",
      "Beam: 1, Iter: 74001, Reward pred: -1, Reward: -1, BF Gain pred: 105.53, BF Gain: 105.53\n",
      "Beam: 2, Iter: 74001, Reward pred: -1, Reward: -1, BF Gain pred: 233.38, BF Gain: 233.38\n",
      "Beam: 3, Iter: 74001, Reward pred: -1, Reward: -1, BF Gain pred: 208.57, BF Gain: 208.57\n",
      "Training for 500 iteration for each Beam uses 149.33319878578186 seconds.\n",
      "Beam: 0, Iter: 74501, Reward pred: -1, Reward: -1, BF Gain pred: 159.94, BF Gain: 159.94\n",
      "Beam: 1, Iter: 74501, Reward pred: 1, Reward: 1, BF Gain pred: 106.00, BF Gain: 106.00\n",
      "Beam: 2, Iter: 74501, Reward pred: 1, Reward: 1, BF Gain pred: 230.08, BF Gain: 230.08\n",
      "Beam: 3, Iter: 74501, Reward pred: 1, Reward: 1, BF Gain pred: 214.97, BF Gain: 214.97\n",
      "Training for 500 iteration for each Beam uses 148.27558016777039 seconds.\n",
      "Beam: 0, Iter: 75001, Reward pred: -1, Reward: -1, BF Gain pred: 161.46, BF Gain: 161.46\n",
      "Beam: 1, Iter: 75001, Reward pred: 1, Reward: 1, BF Gain pred: 108.49, BF Gain: 108.49\n",
      "Beam: 2, Iter: 75001, Reward pred: 1, Reward: 1, BF Gain pred: 233.16, BF Gain: 233.16\n",
      "Beam: 3, Iter: 75001, Reward pred: 1, Reward: 1, BF Gain pred: 213.84, BF Gain: 213.84\n",
      "Training for 500 iteration for each Beam uses 146.8779969215393 seconds.\n",
      "Beam: 0, Iter: 75501, Reward pred: 1, Reward: 1, BF Gain pred: 160.75, BF Gain: 160.75\n",
      "Beam: 1, Iter: 75501, Reward pred: -1, Reward: -1, BF Gain pred: 108.91, BF Gain: 108.91\n",
      "Beam: 2, Iter: 75501, Reward pred: -1, Reward: -1, BF Gain pred: 229.49, BF Gain: 229.49\n",
      "Beam: 3, Iter: 75501, Reward pred: -1, Reward: -1, BF Gain pred: 209.57, BF Gain: 209.57\n",
      "Training for 500 iteration for each Beam uses 148.57711100578308 seconds.\n",
      "Beam: 0, Iter: 76001, Reward pred: -1, Reward: -1, BF Gain pred: 162.94, BF Gain: 162.94\n",
      "Beam: 1, Iter: 76001, Reward pred: -1, Reward: -1, BF Gain pred: 105.62, BF Gain: 105.62\n",
      "Beam: 2, Iter: 76001, Reward pred: 1, Reward: 1, BF Gain pred: 233.86, BF Gain: 233.86\n",
      "Beam: 3, Iter: 76001, Reward pred: 1, Reward: 1, BF Gain pred: 206.26, BF Gain: 206.26\n",
      "Training for 500 iteration for each Beam uses 148.4163999557495 seconds.\n",
      "Beam: 0, Iter: 76501, Reward pred: -1, Reward: -1, BF Gain pred: 159.84, BF Gain: 159.84\n",
      "Beam: 1, Iter: 76501, Reward pred: -1, Reward: -1, BF Gain pred: 109.53, BF Gain: 109.53\n",
      "Beam: 2, Iter: 76501, Reward pred: 1, Reward: 1, BF Gain pred: 242.69, BF Gain: 242.69\n",
      "Beam: 3, Iter: 76501, Reward pred: 1, Reward: 1, BF Gain pred: 208.52, BF Gain: 208.52\n",
      "Training for 500 iteration for each Beam uses 148.4804286956787 seconds.\n",
      "Beam: 0, Iter: 77001, Reward pred: 1, Reward: 1, BF Gain pred: 163.76, BF Gain: 163.76\n",
      "Beam: 1, Iter: 77001, Reward pred: 1, Reward: 1, BF Gain pred: 106.77, BF Gain: 106.77\n",
      "Beam: 2, Iter: 77001, Reward pred: 1, Reward: 1, BF Gain pred: 231.44, BF Gain: 231.44\n",
      "Beam: 3, Iter: 77001, Reward pred: -1, Reward: -1, BF Gain pred: 210.52, BF Gain: 210.52\n",
      "Training for 500 iteration for each Beam uses 148.95065832138062 seconds.\n",
      "Beam: 0, Iter: 77501, Reward pred: 1, Reward: 1, BF Gain pred: 162.26, BF Gain: 162.26\n",
      "Beam: 1, Iter: 77501, Reward pred: -1, Reward: -1, BF Gain pred: 107.49, BF Gain: 107.49\n",
      "Beam: 2, Iter: 77501, Reward pred: -1, Reward: -1, BF Gain pred: 234.67, BF Gain: 234.67\n",
      "Beam: 3, Iter: 77501, Reward pred: 1, Reward: 1, BF Gain pred: 211.50, BF Gain: 211.50\n",
      "Training for 500 iteration for each Beam uses 148.78952884674072 seconds.\n",
      "Beam: 0, Iter: 78001, Reward pred: 1, Reward: 1, BF Gain pred: 165.56, BF Gain: 165.56\n",
      "Beam: 1, Iter: 78001, Reward pred: 1, Reward: 1, BF Gain pred: 107.68, BF Gain: 107.68\n",
      "Beam: 2, Iter: 78001, Reward pred: -1, Reward: -1, BF Gain pred: 236.87, BF Gain: 236.87\n",
      "Beam: 3, Iter: 78001, Reward pred: -1, Reward: -1, BF Gain pred: 205.05, BF Gain: 205.05\n",
      "Training for 500 iteration for each Beam uses 149.52335596084595 seconds.\n",
      "Beam: 0, Iter: 78501, Reward pred: 1, Reward: 1, BF Gain pred: 164.60, BF Gain: 164.60\n",
      "Beam: 1, Iter: 78501, Reward pred: -1, Reward: -1, BF Gain pred: 108.08, BF Gain: 108.08\n",
      "Beam: 2, Iter: 78501, Reward pred: -1, Reward: -1, BF Gain pred: 229.25, BF Gain: 229.25\n",
      "Beam: 3, Iter: 78501, Reward pred: -1, Reward: -1, BF Gain pred: 208.26, BF Gain: 208.26\n",
      "Training for 500 iteration for each Beam uses 148.61220741271973 seconds.\n",
      "Beam: 0, Iter: 79001, Reward pred: 1, Reward: 1, BF Gain pred: 162.24, BF Gain: 162.24\n",
      "Beam: 1, Iter: 79001, Reward pred: -1, Reward: -1, BF Gain pred: 108.27, BF Gain: 108.27\n",
      "Beam: 2, Iter: 79001, Reward pred: -1, Reward: -1, BF Gain pred: 228.18, BF Gain: 228.18\n",
      "Beam: 3, Iter: 79001, Reward pred: -1, Reward: -1, BF Gain pred: 214.41, BF Gain: 214.41\n",
      "Training for 500 iteration for each Beam uses 148.3892593383789 seconds.\n",
      "Beam: 0, Iter: 79501, Reward pred: -1, Reward: -1, BF Gain pred: 161.04, BF Gain: 161.04\n",
      "Beam: 1, Iter: 79501, Reward pred: -1, Reward: -1, BF Gain pred: 107.19, BF Gain: 107.19\n",
      "Beam: 2, Iter: 79501, Reward pred: 1, Reward: 1, BF Gain pred: 235.88, BF Gain: 235.88\n",
      "Beam: 3, Iter: 79501, Reward pred: -1, Reward: -1, BF Gain pred: 214.39, BF Gain: 214.39\n",
      "Training for 500 iteration for each Beam uses 149.32023119926453 seconds.\n",
      "Beam: 0, Iter: 80001, Reward pred: 1, Reward: 1, BF Gain pred: 163.02, BF Gain: 163.02\n",
      "Beam: 1, Iter: 80001, Reward pred: -1, Reward: -1, BF Gain pred: 104.51, BF Gain: 104.51\n",
      "Beam: 2, Iter: 80001, Reward pred: -1, Reward: -1, BF Gain pred: 228.22, BF Gain: 228.22\n",
      "Beam: 3, Iter: 80001, Reward pred: -1, Reward: -1, BF Gain pred: 212.29, BF Gain: 212.29\n",
      "Training for 500 iteration for each Beam uses 149.76007962226868 seconds.\n",
      "Beam: 0, Iter: 80501, Reward pred: 1, Reward: 1, BF Gain pred: 161.69, BF Gain: 161.69\n",
      "Beam: 1, Iter: 80501, Reward pred: -1, Reward: -1, BF Gain pred: 106.58, BF Gain: 106.58\n",
      "Beam: 2, Iter: 80501, Reward pred: 1, Reward: 1, BF Gain pred: 231.94, BF Gain: 231.94\n",
      "Beam: 3, Iter: 80501, Reward pred: 1, Reward: 1, BF Gain pred: 212.58, BF Gain: 212.58\n",
      "Training for 500 iteration for each Beam uses 149.17644500732422 seconds.\n",
      "Beam: 0, Iter: 81001, Reward pred: 1, Reward: 1, BF Gain pred: 161.84, BF Gain: 161.84\n",
      "Beam: 1, Iter: 81001, Reward pred: 1, Reward: 1, BF Gain pred: 104.48, BF Gain: 104.48\n",
      "Beam: 2, Iter: 81001, Reward pred: 1, Reward: 1, BF Gain pred: 232.81, BF Gain: 232.81\n",
      "Beam: 3, Iter: 81001, Reward pred: -1, Reward: -1, BF Gain pred: 212.12, BF Gain: 212.12\n",
      "Training for 500 iteration for each Beam uses 149.7255437374115 seconds.\n",
      "Beam: 0, Iter: 81501, Reward pred: -1, Reward: -1, BF Gain pred: 164.49, BF Gain: 164.49\n",
      "Beam: 1, Iter: 81501, Reward pred: -1, Reward: -1, BF Gain pred: 105.94, BF Gain: 105.94\n",
      "Beam: 2, Iter: 81501, Reward pred: -1, Reward: -1, BF Gain pred: 229.85, BF Gain: 229.85\n",
      "Beam: 3, Iter: 81501, Reward pred: -1, Reward: -1, BF Gain pred: 215.85, BF Gain: 215.85\n",
      "Training for 500 iteration for each Beam uses 149.27830839157104 seconds.\n",
      "Beam: 0, Iter: 82001, Reward pred: -1, Reward: -1, BF Gain pred: 158.14, BF Gain: 158.14\n",
      "Beam: 1, Iter: 82001, Reward pred: 1, Reward: 1, BF Gain pred: 106.90, BF Gain: 106.90\n",
      "Beam: 2, Iter: 82001, Reward pred: -1, Reward: -1, BF Gain pred: 229.32, BF Gain: 229.32\n",
      "Beam: 3, Iter: 82001, Reward pred: -1, Reward: -1, BF Gain pred: 209.29, BF Gain: 209.29\n",
      "Training for 500 iteration for each Beam uses 149.8288733959198 seconds.\n",
      "Beam: 0, Iter: 82501, Reward pred: -1, Reward: -1, BF Gain pred: 163.34, BF Gain: 163.34\n",
      "Beam: 1, Iter: 82501, Reward pred: 1, Reward: 1, BF Gain pred: 109.11, BF Gain: 109.11\n",
      "Beam: 2, Iter: 82501, Reward pred: 1, Reward: 1, BF Gain pred: 230.42, BF Gain: 230.42\n",
      "Beam: 3, Iter: 82501, Reward pred: -1, Reward: -1, BF Gain pred: 211.30, BF Gain: 211.30\n",
      "Training for 500 iteration for each Beam uses 149.27714729309082 seconds.\n",
      "Beam: 0, Iter: 83001, Reward pred: 1, Reward: 1, BF Gain pred: 160.26, BF Gain: 160.26\n",
      "Beam: 1, Iter: 83001, Reward pred: -1, Reward: -1, BF Gain pred: 107.93, BF Gain: 107.93\n",
      "Beam: 2, Iter: 83001, Reward pred: 1, Reward: 1, BF Gain pred: 232.61, BF Gain: 232.61\n",
      "Beam: 3, Iter: 83001, Reward pred: 1, Reward: 1, BF Gain pred: 214.38, BF Gain: 214.38\n",
      "Training for 500 iteration for each Beam uses 147.97758555412292 seconds.\n",
      "Beam: 0, Iter: 83501, Reward pred: 1, Reward: 1, BF Gain pred: 162.44, BF Gain: 162.44\n",
      "Beam: 1, Iter: 83501, Reward pred: -1, Reward: -1, BF Gain pred: 107.62, BF Gain: 107.62\n",
      "Beam: 2, Iter: 83501, Reward pred: 1, Reward: 1, BF Gain pred: 235.50, BF Gain: 235.50\n",
      "Beam: 3, Iter: 83501, Reward pred: 1, Reward: 1, BF Gain pred: 211.00, BF Gain: 211.00\n",
      "Training for 500 iteration for each Beam uses 150.4564528465271 seconds.\n",
      "Beam: 0, Iter: 84001, Reward pred: -1, Reward: -1, BF Gain pred: 159.14, BF Gain: 159.14\n",
      "Beam: 1, Iter: 84001, Reward pred: -1, Reward: -1, BF Gain pred: 106.82, BF Gain: 106.82\n",
      "Beam: 2, Iter: 84001, Reward pred: -1, Reward: -1, BF Gain pred: 234.88, BF Gain: 234.88\n",
      "Beam: 3, Iter: 84001, Reward pred: 1, Reward: 1, BF Gain pred: 212.86, BF Gain: 212.86\n",
      "Training for 500 iteration for each Beam uses 149.19365620613098 seconds.\n",
      "Beam: 0, Iter: 84501, Reward pred: 1, Reward: 1, BF Gain pred: 162.65, BF Gain: 162.65\n",
      "Beam: 1, Iter: 84501, Reward pred: -1, Reward: -1, BF Gain pred: 104.70, BF Gain: 104.70\n",
      "Beam: 2, Iter: 84501, Reward pred: 1, Reward: 1, BF Gain pred: 233.67, BF Gain: 233.67\n",
      "Beam: 3, Iter: 84501, Reward pred: -1, Reward: -1, BF Gain pred: 212.38, BF Gain: 212.38\n",
      "Training for 500 iteration for each Beam uses 151.04693460464478 seconds.\n",
      "Beam: 0, Iter: 85001, Reward pred: -1, Reward: -1, BF Gain pred: 158.95, BF Gain: 158.95\n",
      "Beam: 1, Iter: 85001, Reward pred: -1, Reward: -1, BF Gain pred: 107.82, BF Gain: 107.82\n",
      "Beam: 2, Iter: 85001, Reward pred: 1, Reward: 1, BF Gain pred: 234.85, BF Gain: 234.85\n",
      "Beam: 3, Iter: 85001, Reward pred: -1, Reward: -1, BF Gain pred: 212.62, BF Gain: 212.62\n",
      "Training for 500 iteration for each Beam uses 150.27141332626343 seconds.\n",
      "Beam: 0, Iter: 85501, Reward pred: -1, Reward: -1, BF Gain pred: 162.25, BF Gain: 162.25\n",
      "Beam: 1, Iter: 85501, Reward pred: 1, Reward: 1, BF Gain pred: 106.87, BF Gain: 106.87\n",
      "Beam: 2, Iter: 85501, Reward pred: -1, Reward: -1, BF Gain pred: 231.56, BF Gain: 231.56\n",
      "Beam: 3, Iter: 85501, Reward pred: -1, Reward: -1, BF Gain pred: 214.01, BF Gain: 214.01\n",
      "Training for 500 iteration for each Beam uses 149.46040725708008 seconds.\n",
      "Beam: 0, Iter: 86001, Reward pred: -1, Reward: -1, BF Gain pred: 162.03, BF Gain: 162.03\n",
      "Beam: 1, Iter: 86001, Reward pred: -1, Reward: -1, BF Gain pred: 107.11, BF Gain: 107.11\n",
      "Beam: 2, Iter: 86001, Reward pred: 1, Reward: 1, BF Gain pred: 227.54, BF Gain: 227.54\n",
      "Beam: 3, Iter: 86001, Reward pred: -1, Reward: -1, BF Gain pred: 209.27, BF Gain: 209.27\n",
      "Training for 500 iteration for each Beam uses 150.25286030769348 seconds.\n",
      "Beam: 0, Iter: 86501, Reward pred: 1, Reward: 1, BF Gain pred: 161.94, BF Gain: 161.94\n",
      "Beam: 1, Iter: 86501, Reward pred: -1, Reward: -1, BF Gain pred: 105.05, BF Gain: 105.05\n",
      "Beam: 2, Iter: 86501, Reward pred: -1, Reward: -1, BF Gain pred: 229.99, BF Gain: 229.99\n",
      "Beam: 3, Iter: 86501, Reward pred: -1, Reward: -1, BF Gain pred: 210.62, BF Gain: 210.62\n",
      "Training for 500 iteration for each Beam uses 148.25118613243103 seconds.\n",
      "Beam: 0, Iter: 87001, Reward pred: 1, Reward: 1, BF Gain pred: 161.77, BF Gain: 161.77\n",
      "Beam: 1, Iter: 87001, Reward pred: -1, Reward: -1, BF Gain pred: 106.27, BF Gain: 106.27\n",
      "Beam: 2, Iter: 87001, Reward pred: -1, Reward: -1, BF Gain pred: 235.21, BF Gain: 235.21\n",
      "Beam: 3, Iter: 87001, Reward pred: 1, Reward: 1, BF Gain pred: 214.53, BF Gain: 214.53\n",
      "Training for 500 iteration for each Beam uses 149.41618156433105 seconds.\n",
      "Beam: 0, Iter: 87501, Reward pred: 1, Reward: 1, BF Gain pred: 164.08, BF Gain: 164.08\n",
      "Beam: 1, Iter: 87501, Reward pred: 1, Reward: 1, BF Gain pred: 108.28, BF Gain: 108.28\n",
      "Beam: 2, Iter: 87501, Reward pred: 1, Reward: 1, BF Gain pred: 233.46, BF Gain: 233.46\n",
      "Beam: 3, Iter: 87501, Reward pred: 1, Reward: 1, BF Gain pred: 211.72, BF Gain: 211.72\n",
      "Training for 500 iteration for each Beam uses 150.4451129436493 seconds.\n",
      "Beam: 0, Iter: 88001, Reward pred: -1, Reward: -1, BF Gain pred: 162.48, BF Gain: 162.48\n",
      "Beam: 1, Iter: 88001, Reward pred: -1, Reward: -1, BF Gain pred: 106.03, BF Gain: 106.03\n",
      "Beam: 2, Iter: 88001, Reward pred: 1, Reward: 1, BF Gain pred: 229.26, BF Gain: 229.26\n",
      "Beam: 3, Iter: 88001, Reward pred: -1, Reward: -1, BF Gain pred: 204.26, BF Gain: 204.26\n",
      "Training for 500 iteration for each Beam uses 149.73272371292114 seconds.\n",
      "Beam: 0, Iter: 88501, Reward pred: -1, Reward: -1, BF Gain pred: 164.09, BF Gain: 164.09\n",
      "Beam: 1, Iter: 88501, Reward pred: -1, Reward: -1, BF Gain pred: 107.17, BF Gain: 107.17\n",
      "Beam: 2, Iter: 88501, Reward pred: 1, Reward: 1, BF Gain pred: 228.13, BF Gain: 228.13\n",
      "Beam: 3, Iter: 88501, Reward pred: 1, Reward: 1, BF Gain pred: 214.93, BF Gain: 214.93\n",
      "Training for 500 iteration for each Beam uses 150.3736264705658 seconds.\n",
      "Beam: 0, Iter: 89001, Reward pred: -1, Reward: -1, BF Gain pred: 157.96, BF Gain: 157.96\n",
      "Beam: 1, Iter: 89001, Reward pred: -1, Reward: -1, BF Gain pred: 107.07, BF Gain: 107.07\n",
      "Beam: 2, Iter: 89001, Reward pred: -1, Reward: -1, BF Gain pred: 233.72, BF Gain: 233.72\n",
      "Beam: 3, Iter: 89001, Reward pred: -1, Reward: -1, BF Gain pred: 212.65, BF Gain: 212.65\n",
      "Training for 500 iteration for each Beam uses 150.18636059761047 seconds.\n",
      "Beam: 0, Iter: 89501, Reward pred: -1, Reward: -1, BF Gain pred: 157.39, BF Gain: 157.39\n",
      "Beam: 1, Iter: 89501, Reward pred: -1, Reward: -1, BF Gain pred: 107.77, BF Gain: 107.77\n",
      "Beam: 2, Iter: 89501, Reward pred: -1, Reward: -1, BF Gain pred: 233.69, BF Gain: 233.69\n",
      "Beam: 3, Iter: 89501, Reward pred: -1, Reward: -1, BF Gain pred: 213.76, BF Gain: 213.76\n",
      "Training for 500 iteration for each Beam uses 150.40344190597534 seconds.\n",
      "Beam: 0, Iter: 90001, Reward pred: -1, Reward: -1, BF Gain pred: 162.04, BF Gain: 162.04\n",
      "Beam: 1, Iter: 90001, Reward pred: -1, Reward: -1, BF Gain pred: 109.50, BF Gain: 109.50\n",
      "Beam: 2, Iter: 90001, Reward pred: -1, Reward: -1, BF Gain pred: 231.19, BF Gain: 231.19\n",
      "Beam: 3, Iter: 90001, Reward pred: 1, Reward: 1, BF Gain pred: 212.18, BF Gain: 212.18\n",
      "Training for 500 iteration for each Beam uses 151.01309084892273 seconds.\n",
      "Beam: 0, Iter: 90501, Reward pred: 1, Reward: 1, BF Gain pred: 159.69, BF Gain: 159.69\n",
      "Beam: 1, Iter: 90501, Reward pred: 1, Reward: 1, BF Gain pred: 107.43, BF Gain: 107.43\n",
      "Beam: 2, Iter: 90501, Reward pred: 1, Reward: 1, BF Gain pred: 234.01, BF Gain: 234.01\n",
      "Beam: 3, Iter: 90501, Reward pred: -1, Reward: -1, BF Gain pred: 208.24, BF Gain: 208.24\n",
      "Training for 500 iteration for each Beam uses 148.53221321105957 seconds.\n",
      "Beam: 0, Iter: 91001, Reward pred: -1, Reward: -1, BF Gain pred: 161.73, BF Gain: 161.73\n",
      "Beam: 1, Iter: 91001, Reward pred: -1, Reward: -1, BF Gain pred: 108.18, BF Gain: 108.18\n",
      "Beam: 2, Iter: 91001, Reward pred: 1, Reward: 1, BF Gain pred: 233.09, BF Gain: 233.09\n",
      "Beam: 3, Iter: 91001, Reward pred: 1, Reward: 1, BF Gain pred: 209.02, BF Gain: 209.02\n",
      "Training for 500 iteration for each Beam uses 149.99277663230896 seconds.\n",
      "Beam: 0, Iter: 91501, Reward pred: -1, Reward: -1, BF Gain pred: 158.21, BF Gain: 158.21\n",
      "Beam: 1, Iter: 91501, Reward pred: 1, Reward: 1, BF Gain pred: 108.00, BF Gain: 108.00\n",
      "Beam: 2, Iter: 91501, Reward pred: -1, Reward: -1, BF Gain pred: 230.45, BF Gain: 230.45\n",
      "Beam: 3, Iter: 91501, Reward pred: -1, Reward: -1, BF Gain pred: 213.06, BF Gain: 213.06\n",
      "Training for 500 iteration for each Beam uses 149.9634599685669 seconds.\n",
      "Beam: 0, Iter: 92001, Reward pred: -1, Reward: -1, BF Gain pred: 160.44, BF Gain: 160.44\n",
      "Beam: 1, Iter: 92001, Reward pred: -1, Reward: -1, BF Gain pred: 106.82, BF Gain: 106.82\n",
      "Beam: 2, Iter: 92001, Reward pred: 1, Reward: 1, BF Gain pred: 233.72, BF Gain: 233.72\n",
      "Beam: 3, Iter: 92001, Reward pred: 1, Reward: 1, BF Gain pred: 207.32, BF Gain: 207.32\n",
      "Training for 500 iteration for each Beam uses 150.74376368522644 seconds.\n",
      "Beam: 0, Iter: 92501, Reward pred: -1, Reward: -1, BF Gain pred: 161.69, BF Gain: 161.69\n",
      "Beam: 1, Iter: 92501, Reward pred: 1, Reward: 1, BF Gain pred: 105.50, BF Gain: 105.50\n",
      "Beam: 2, Iter: 92501, Reward pred: -1, Reward: -1, BF Gain pred: 236.99, BF Gain: 236.99\n",
      "Beam: 3, Iter: 92501, Reward pred: -1, Reward: -1, BF Gain pred: 209.63, BF Gain: 209.63\n",
      "Training for 500 iteration for each Beam uses 150.94859910011292 seconds.\n",
      "Beam: 0, Iter: 93001, Reward pred: 1, Reward: 1, BF Gain pred: 163.49, BF Gain: 163.49\n",
      "Beam: 1, Iter: 93001, Reward pred: -1, Reward: -1, BF Gain pred: 108.18, BF Gain: 108.18\n",
      "Beam: 2, Iter: 93001, Reward pred: -1, Reward: -1, BF Gain pred: 235.27, BF Gain: 235.27\n",
      "Beam: 3, Iter: 93001, Reward pred: 1, Reward: 1, BF Gain pred: 214.84, BF Gain: 214.84\n",
      "Training for 500 iteration for each Beam uses 150.6029896736145 seconds.\n",
      "Beam: 0, Iter: 93501, Reward pred: -1, Reward: -1, BF Gain pred: 162.61, BF Gain: 162.61\n",
      "Beam: 1, Iter: 93501, Reward pred: -1, Reward: -1, BF Gain pred: 107.36, BF Gain: 107.36\n",
      "Beam: 2, Iter: 93501, Reward pred: -1, Reward: -1, BF Gain pred: 228.79, BF Gain: 228.79\n",
      "Beam: 3, Iter: 93501, Reward pred: -1, Reward: -1, BF Gain pred: 213.15, BF Gain: 213.15\n",
      "Training for 500 iteration for each Beam uses 151.03508305549622 seconds.\n",
      "Beam: 0, Iter: 94001, Reward pred: -1, Reward: -1, BF Gain pred: 161.41, BF Gain: 161.41\n",
      "Beam: 1, Iter: 94001, Reward pred: -1, Reward: -1, BF Gain pred: 105.70, BF Gain: 105.70\n",
      "Beam: 2, Iter: 94001, Reward pred: 1, Reward: 1, BF Gain pred: 235.66, BF Gain: 235.66\n",
      "Beam: 3, Iter: 94001, Reward pred: -1, Reward: -1, BF Gain pred: 207.65, BF Gain: 207.65\n",
      "Training for 500 iteration for each Beam uses 149.79438614845276 seconds.\n",
      "Beam: 0, Iter: 94501, Reward pred: 1, Reward: 1, BF Gain pred: 163.03, BF Gain: 163.03\n",
      "Beam: 1, Iter: 94501, Reward pred: -1, Reward: -1, BF Gain pred: 105.20, BF Gain: 105.20\n",
      "Beam: 2, Iter: 94501, Reward pred: -1, Reward: -1, BF Gain pred: 232.35, BF Gain: 232.35\n",
      "Beam: 3, Iter: 94501, Reward pred: 1, Reward: 1, BF Gain pred: 207.60, BF Gain: 207.60\n",
      "Training for 500 iteration for each Beam uses 150.58201932907104 seconds.\n",
      "Beam: 0, Iter: 95001, Reward pred: -1, Reward: -1, BF Gain pred: 166.92, BF Gain: 166.92\n",
      "Beam: 1, Iter: 95001, Reward pred: -1, Reward: -1, BF Gain pred: 108.15, BF Gain: 108.15\n",
      "Beam: 2, Iter: 95001, Reward pred: -1, Reward: -1, BF Gain pred: 230.19, BF Gain: 230.19\n",
      "Beam: 3, Iter: 95001, Reward pred: -1, Reward: -1, BF Gain pred: 208.22, BF Gain: 208.22\n",
      "Training for 500 iteration for each Beam uses 148.67941737174988 seconds.\n",
      "Beam: 0, Iter: 95501, Reward pred: 1, Reward: 1, BF Gain pred: 163.32, BF Gain: 163.32\n",
      "Beam: 1, Iter: 95501, Reward pred: 1, Reward: 1, BF Gain pred: 105.97, BF Gain: 105.97\n",
      "Beam: 2, Iter: 95501, Reward pred: -1, Reward: -1, BF Gain pred: 228.69, BF Gain: 228.69\n",
      "Beam: 3, Iter: 95501, Reward pred: -1, Reward: -1, BF Gain pred: 213.97, BF Gain: 213.97\n",
      "Training for 500 iteration for each Beam uses 149.1969118118286 seconds.\n",
      "Beam: 0, Iter: 96001, Reward pred: -1, Reward: -1, BF Gain pred: 161.01, BF Gain: 161.01\n",
      "Beam: 1, Iter: 96001, Reward pred: -1, Reward: -1, BF Gain pred: 106.78, BF Gain: 106.78\n",
      "Beam: 2, Iter: 96001, Reward pred: 1, Reward: 1, BF Gain pred: 235.80, BF Gain: 235.80\n",
      "Beam: 3, Iter: 96001, Reward pred: -1, Reward: -1, BF Gain pred: 206.84, BF Gain: 206.84\n",
      "Training for 500 iteration for each Beam uses 148.34205961227417 seconds.\n",
      "Beam: 0, Iter: 97001, Reward pred: -1, Reward: -1, BF Gain pred: 160.09, BF Gain: 160.09\n",
      "Beam: 1, Iter: 97001, Reward pred: 1, Reward: 1, BF Gain pred: 110.41, BF Gain: 110.41\n",
      "Beam: 2, Iter: 97001, Reward pred: 1, Reward: 1, BF Gain pred: 226.12, BF Gain: 226.12\n",
      "Beam: 3, Iter: 97001, Reward pred: 1, Reward: 1, BF Gain pred: 215.60, BF Gain: 215.60\n",
      "Training for 500 iteration for each Beam uses 148.62076091766357 seconds.\n",
      "Beam: 0, Iter: 97501, Reward pred: -1, Reward: -1, BF Gain pred: 163.32, BF Gain: 163.32\n",
      "Beam: 1, Iter: 97501, Reward pred: -1, Reward: -1, BF Gain pred: 106.85, BF Gain: 106.85\n",
      "Beam: 2, Iter: 97501, Reward pred: 1, Reward: 1, BF Gain pred: 235.16, BF Gain: 235.16\n",
      "Beam: 3, Iter: 97501, Reward pred: -1, Reward: -1, BF Gain pred: 208.72, BF Gain: 208.72\n",
      "Training for 500 iteration for each Beam uses 149.64674544334412 seconds.\n",
      "Beam: 0, Iter: 98001, Reward pred: 1, Reward: 1, BF Gain pred: 160.05, BF Gain: 160.05\n",
      "Beam: 1, Iter: 98001, Reward pred: -1, Reward: -1, BF Gain pred: 104.92, BF Gain: 104.92\n",
      "Beam: 2, Iter: 98001, Reward pred: -1, Reward: -1, BF Gain pred: 232.89, BF Gain: 232.89\n",
      "Beam: 3, Iter: 98001, Reward pred: 1, Reward: 1, BF Gain pred: 211.47, BF Gain: 211.47\n",
      "Training for 500 iteration for each Beam uses 149.80001163482666 seconds.\n",
      "Beam: 0, Iter: 98501, Reward pred: 1, Reward: 1, BF Gain pred: 163.56, BF Gain: 163.56\n",
      "Beam: 1, Iter: 98501, Reward pred: 1, Reward: 1, BF Gain pred: 107.27, BF Gain: 107.27\n",
      "Beam: 2, Iter: 98501, Reward pred: -1, Reward: -1, BF Gain pred: 233.46, BF Gain: 233.46\n",
      "Beam: 3, Iter: 98501, Reward pred: -1, Reward: -1, BF Gain pred: 210.88, BF Gain: 210.88\n",
      "Training for 500 iteration for each Beam uses 149.5245864391327 seconds.\n",
      "Beam: 0, Iter: 99001, Reward pred: 1, Reward: 1, BF Gain pred: 164.01, BF Gain: 164.01\n",
      "Beam: 1, Iter: 99001, Reward pred: 1, Reward: 1, BF Gain pred: 105.77, BF Gain: 105.77\n",
      "Beam: 2, Iter: 99001, Reward pred: -1, Reward: -1, BF Gain pred: 233.35, BF Gain: 233.35\n",
      "Beam: 3, Iter: 99001, Reward pred: -1, Reward: -1, BF Gain pred: 209.72, BF Gain: 209.72\n",
      "Training for 500 iteration for each Beam uses 148.5120928287506 seconds.\n",
      "Beam: 0, Iter: 99501, Reward pred: 1, Reward: 1, BF Gain pred: 163.85, BF Gain: 163.85\n",
      "Beam: 1, Iter: 99501, Reward pred: 1, Reward: 1, BF Gain pred: 109.07, BF Gain: 109.07\n",
      "Beam: 2, Iter: 99501, Reward pred: 1, Reward: 1, BF Gain pred: 233.53, BF Gain: 233.53\n",
      "Beam: 3, Iter: 99501, Reward pred: 1, Reward: 1, BF Gain pred: 214.59, BF Gain: 214.59\n",
      "Training for 500 iteration for each Beam uses 148.72346472740173 seconds.\n",
      "Beam: 0, Iter: 100001, Reward pred: 1, Reward: 1, BF Gain pred: 163.10, BF Gain: 163.10\n",
      "Beam: 1, Iter: 100001, Reward pred: -1, Reward: -1, BF Gain pred: 106.06, BF Gain: 106.06\n",
      "Beam: 2, Iter: 100001, Reward pred: -1, Reward: -1, BF Gain pred: 231.42, BF Gain: 231.42\n",
      "Beam: 3, Iter: 100001, Reward pred: 1, Reward: 1, BF Gain pred: 211.44, BF Gain: 211.44\n",
      "Training for 500 iteration for each Beam uses 149.4642698764801 seconds.\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.device(options['gpu_idx']):\n",
    "    for sample_id in range(options['num_loop']):\n",
    "\n",
    "        # ---------- Sampling ---------- #\n",
    "        n_sample = int(ch.shape[0] * options['ch_sample_ratio'])\n",
    "        ch_sample_id = np.random.permutation(ch.shape[0])[0:n_sample]\n",
    "        ch_sample = torch.from_numpy(ch[ch_sample_id, :]).float().cuda()\n",
    "\n",
    "        # ---------- Clustering ---------- #\n",
    "    #     start_time = time.time()\n",
    "\n",
    "        bf_mat_sample = bf_gain_cal(sensing_beam, ch_sample)\n",
    "        # print(\"Clustering -1 uses %s seconds.\" % (time.time() - start_time))\n",
    "        # start_time = time.time()\n",
    "        f_matrix = corr_mining(bf_mat_sample)\n",
    "        f_matrix_np = torch.Tensor.cpu(f_matrix).numpy()\n",
    "        # print(\"Clustering 0 uses %s seconds.\" % (time.time() - start_time))\n",
    "        # start_time = time.time()\n",
    "        labels = u_classifier.predict(np.transpose(f_matrix_np).astype(float))\n",
    "\n",
    "        # print(\"Clustering 1 uses %s seconds.\" % (time.time() - start_time))\n",
    "        # start_time = time.time()\n",
    "\n",
    "        user_group = []  # order: clusters\n",
    "        ch_group = []  # order: clusters\n",
    "        for ii in range(options['num_NNs']):\n",
    "            user_group.append(np.where(labels == ii)[0].tolist())\n",
    "            ch_group.append(ch_sample[user_group[ii], :])\n",
    "\n",
    "    #     print(\"Clustering 2 uses %s seconds.\" % (time.time() - start_time))\n",
    "\n",
    "        # ---------- Assignment ---------- #\n",
    "    #     start_time = time.time()\n",
    "\n",
    "        # best_state matrix\n",
    "        best_beam_mtx = torch.zeros((options['num_NNs'], 2 * options['num_ant'])).float().cuda()\n",
    "        for pp in range(options['num_NNs']):\n",
    "            best_beam_mtx[pp, :] = env_list[pp].best_bf_vec\n",
    "        gain_mtx = bf_gain_cal(best_beam_mtx, ch_sample)  # (n_beam, n_user)\n",
    "        for ii in range(options['num_NNs']):\n",
    "            if ii == 0:\n",
    "                cost_mtx = torch.mean(gain_mtx[:, user_group[ii]], dim=1).reshape(options['num_NNs'], -1)\n",
    "            else:\n",
    "                sub = torch.mean(gain_mtx[:, user_group[ii]], dim=1).reshape(options['num_NNs'], -1)\n",
    "                cost_mtx = torch.cat((cost_mtx, sub), dim=1)\n",
    "        cost_mtx = -torch.Tensor.cpu(cost_mtx).numpy()\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_mtx)\n",
    "        assignment_record = dict(zip(row_ind.tolist(), col_ind.tolist()))  # key: network, value: cluster\n",
    "        for ii in range(options['num_NNs']):\n",
    "            env_list[ii].ch = ch_group[assignment_record[ii]]\n",
    "\n",
    "    #     print(\"Assignment uses %s seconds.\" % (time.time() - start_time))\n",
    "        if (train_opt_list[beam_id]['overall_iter']-1)%500==0 or train_opt_list[beam_id]['overall_iter']==1:\n",
    "            start_time = time.time()\n",
    "        for beam_id in range(options['num_NNs']):\n",
    "            train_opt_list[beam_id] = train(env_list[beam_id],options, train_opt_list[beam_id],agent_list[beam_id],ounoise_list[beam_id], beam_id)\n",
    "        if (train_opt_list[beam_id]['overall_iter']-1)%500==0: \n",
    "            print(\"Training for 500 iteration for each Beam uses %s seconds.\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "\n",
    "num_ant = 32\n",
    "num_beam = 4\n",
    "results = np.empty((num_beam, 2*num_ant))\n",
    "\n",
    "path = './beams/'\n",
    "\n",
    "for beam_id in range(num_beam):\n",
    "    fname = 'beams_' + str(beam_id) + '_max.txt'\n",
    "    with open(path + fname, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        last_line = lines[-1]\n",
    "        results[beam_id, :] = np.fromstring(last_line.replace(\"\\n\", \"\"), sep=',').reshape(1, -1)\n",
    "\n",
    "results = (1 / np.sqrt(num_ant)) * (results[:, ::2] + 1j * results[:, 1::2])\n",
    "\n",
    "scio.savemat('beam_codebook.mat', {'beams': results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9oAAAK9CAYAAADbkRQRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACNYUlEQVR4nOzdd3xUdd7+/2tKeiWBJJTQUboKKMYGCBKaZXVvLKjYXW+w/lyRXeyuuu7aVgHrV3cVxMXbsiLq2igqTRSRIlKlJqGmkDoz5/fHZE4yZBJSpiV5PR+PLGfOfGbOe8jZyJVPsxiGYQgAAAAAAPiFNdQFAAAAAADQkhC0AQAAAADwI4I2AAAAAAB+RNAGAAAAAMCPCNoAAAAAAPgRQRsAAAAAAD8iaAMAAAAA4EcEbQAAAAAA/IigDQAAAACAHxG0AQCoxRtvvCGLxaLvv/8+1KU0Sm5urn7/+98rNTVVFotFzz77bKhLkiQtWrRIFotFixYtCnUpfjF8+HANHz481GUAAMIIQRsAEHSeAFv9Ky0tTSNGjNAnn3wSsOsWFxfrwQcfbDEB73juvPNOffbZZ5o+fbrefPNNjRkzJtQlAQDQKthDXQAAoPV6+OGH1a1bNxmGodzcXL3xxhsaN26cPvroI02YMMHv1ysuLtZDDz0kSa2iB/Krr77ShRdeqLvvvjvUpXg555xzVFJSosjIyFCX4hf//e9/Q10CACDMELQBACEzduxYDRkyxHx8/fXXKz09XW+//XZAgnZrk5eXp+TkZL+9X2lpqSIjI2W1Nm1AnNVqVXR0tJ+qCr2W8gsDAID/MHQcABA2kpOTFRMTI7vd+/fALpdLzz77rPr166fo6Gilp6fr5ptv1uHDh73aff/998rOzlbbtm0VExOjbt266brrrpMk7dixQ+3atZMkPfTQQ+aQ9QcffPC4dRUXF+vmm29WamqqEhMTdfXVV9e4tiR98sknOvvssxUXF6eEhASNHz9e69ev92qzdu1aXXPNNerevbuio6OVkZGh6667TgcPHvRq9+CDD8pisejXX3/VlVdeqaSkJLVr10733XefDMPQrl27dOGFFyoxMVEZGRl66qmnzNd6huYbhqGZM2ean9Vj27Zt+p//+R+lpKQoNjZWp59+uj7++GOv63vmUc+bN08zZsxQx44dFRsbq4KCAl1zzTWKj4/Xzp07NWHCBMXHx6tjx46aOXOmJOnnn3/Wueeeq7i4OHXp0kVz5871+d7Vh/APHz5c/fv314YNGzRixAjFxsaqY8eOevLJJ2v8Pf/222+64IILFBcXp7S0NHOIfH3nfS9atEhDhgxRdHS0evTooZdeesn8+67u9ddf17nnnqu0tDRFRUWpb9++mj17do33O3aOtufz/fvf/9Zf/vIXderUSdHR0Ro5cqS2bNly3PoAAM0fPdoAgJDJz8/XgQMHZBiG8vLy9Pzzz6uoqEhXXnmlV7ubb75Zb7zxhq699lrddttt2r59u1544QX9+OOP+vbbbxUREaG8vDyNHj1a7dq107333qvk5GTt2LFD7733niSpXbt2mj17tm655Rb97ne/08UXXyxJGjhw4HHrnDp1qpKTk/Xggw9q06ZNmj17tn777TczUEnSm2++qcmTJys7O1t//etfVVxcrNmzZ+uss87Sjz/+qK5du0qSPv/8c23btk3XXnutMjIytH79er388stav369li9fXiPsXXrpperTp4+eeOIJffzxx3r00UeVkpKil156Seeee67++te/as6cObr77rt16qmn6pxzztE555yjN998U1dddZXOO+88XX311eb75ebm6owzzlBxcbFuu+02paam6p///KcuuOACvfvuu/rd737ndf1HHnlEkZGRuvvuu1VWVmb23jqdTo0dO1bnnHOOnnzySc2ZM0dTp05VXFyc/vznP2vSpEm6+OKL9eKLL+rqq69WVlaWunXrVuff8+HDhzVmzBhdfPHFmjhxot59911NmzZNAwYM0NixYyVJR48e1bnnnqt9+/bp9ttvV0ZGhubOnauvv/76uN9HSfrxxx81ZswYtW/fXg899JCcTqcefvhh85cw1c2ePVv9+vXTBRdcILvdro8++kj/+7//K5fLpSlTphz3Wk888YSsVqvuvvtu5efn68knn9SkSZO0YsWKetUKAGjGDAAAguz11183JNX4ioqKMt544w2vtkuXLjUkGXPmzPE6/+mnn3qdf//99w1JxqpVq2q97v79+w1JxgMPPNCgOgcPHmyUl5eb55988klDkvHhhx8ahmEYhYWFRnJysnHjjTd6vT4nJ8dISkryOl9cXFzjOm+//bYhyViyZIl57oEHHjAkGTfddJN5zuFwGJ06dTIsFovxxBNPmOcPHz5sxMTEGJMnT/Z6X0nGlClTvM7dcccdhiRj6dKl5rnCwkKjW7duRteuXQ2n02kYhmF8/fXXhiSje/fuNWqePHmyIcl47LHHatRgsViMefPmmed/+eWXGn/nnvf++uuvzXPDhg0zJBn/+te/zHNlZWVGRkaGcckll5jnnnrqKUOS8cEHH5jnSkpKjN69e9d4T1/OP/98IzY21tizZ495bvPmzYbdbjeO/WeRr+9Vdna20b17d69zw4YNM4YNG1bj8/Xp08coKyszzz/33HOGJOPnn3+us0YAQPPH0HEAQMjMnDlTn3/+uT7//HO99dZbGjFihG644QazF1qS5s+fr6SkJJ133nk6cOCA+TV48GDFx8ebPZmeucgLFixQRUWFX+u86aabFBERYT6+5ZZbZLfbtXDhQknuXuojR47o8ssv96rRZrNp6NChXr2tMTEx5nFpaakOHDig008/XZL0ww8/1Lj2DTfcYB7bbDYNGTJEhmHo+uuvN88nJyfrxBNP1LZt2477WRYuXKjTTjtNZ511lnkuPj5eN910k3bs2KENGzZ4tZ88ebJXzbXV5qkhLi5OEydONM+feOKJSk5Orldt8fHxXqMZIiMjddppp3m99tNPP1XHjh11wQUXmOeio6N14403Hvf9nU6nvvjiC1100UXq0KGDeb5nz55mj3l11T+3Z/TFsGHDtG3bNuXn5x/3etdee63X/O2zzz5bkur1dwEAaN4YOg4ACJnTTjvNazG0yy+/XKeccoqmTp2qCRMmKDIyUps3b1Z+fr7S0tJ8vkdeXp4kadiwYbrkkkv00EMP6ZlnntHw4cN10UUX6YorrlBUVFST6uzVq5fX4/j4eLVv3147duyQJG3evFmSdO655/p8fWJionl86NAhPfTQQ5o3b55Zu4ev8Na5c2evx0lJSYqOjlbbtm1rnD92nrcvv/32m4YOHVrjfJ8+fczn+/fvb56vbbh3dHR0jeHWSUlJ6tSpU43h70lJST7ntB/L12vbtGmjtWvXetXfo0ePGu169ux53PfPy8tTSUmJz7a+zn377bd64IEHtGzZMhUXF3s9l5+fr6SkpDqvd+z3rk2bNpJUr78LAEDzRtAGAIQNq9WqESNG6LnnntPmzZvVr18/uVwupaWlac6cOT5f4wl7FotF7777rpYvX66PPvpIn332ma677jo99dRTWr58ueLj4wNWt8vlkuSep52RkVHj+eqLu02cOFHfffed/vjHP+rkk09WfHy8XC6XxowZY75PdTabrV7nJMkwjMZ+hFrV1ptdWw1NqS2Yn+t4tm7dqpEjR6p37956+umnlZmZqcjISC1cuFDPPPOMz+/VscLp8wAAgougDQAIKw6HQ5JUVFQkSerRo4e++OILnXnmmbWGvupOP/10nX766frLX/6iuXPnatKkSZo3b55uuOGGGr2g9bV582aNGDHCfFxUVKR9+/Zp3LhxZo2SlJaWplGjRtX6PocPH9aXX36phx56SPfff7/X+wdLly5dtGnTphrnf/nlF/P5cNalSxdt2LBBhmF4fT/rs5p3WlqaoqOjfbY99txHH32ksrIy/ec///Hqma7vomsAgNaNOdoAgLBRUVGh//73v4qMjDSHMk+cOFFOp1OPPPJIjfYOh0NHjhyR5A6xx/YUnnzyyZKksrIySVJsbKwkma+pr5dfftlr3vfs2bPlcDjMeb3Z2dlKTEzUY4895nN++P79+yVV9XAeW+ezzz7boHqaYty4cVq5cqWWLVtmnjt69Khefvllde3aVX379g1aLY2RnZ2tPXv26D//+Y95rrS0VK+88spxX2uz2TRq1Ch98MEH2rt3r3l+y5Yt+uSTT2q0lby/V/n5+Xr99deb+hEAAK0APdoAgJD55JNPzJ7UvLw8zZ07V5s3b9a9995rzmseNmyYbr75Zj3++ONas2aNRo8erYiICG3evFnz58/Xc889p9///vf65z//qVmzZul3v/udevToocLCQr3yyitKTEw0e55jYmLUt29fvfPOOzrhhBOUkpKi/v37e81J9qW8vFwjR47UxIkTtWnTJs2aNUtnnXWWuSBXYmKiZs+erauuukqDBg3SZZddpnbt2mnnzp36+OOPdeaZZ+qFF15QYmKiuR1WRUWFOnbsqP/+97/avn17AP+Wvd177716++23NXbsWN12221KSUnRP//5T23fvl3/93//J6s1vH8Hf/PNN+uFF17Q5Zdfrttvv13t27fXnDlzFB0dLUnHHbXw4IMP6r///a/OPPNM3XLLLXI6nXrhhRfUv39/rVmzxmw3evRoRUZG6vzzz9fNN9+soqIivfLKK0pLS9O+ffsC+REBAC0AQRsAEDLVh09HR0erd+/emj17tm6++Wavdi+++KIGDx6sl156SX/6059kt9vVtWtXXXnllTrzzDMluQP5ypUrNW/ePOXm5iopKUmnnXaa5syZ47Wg16uvvqpbb71Vd955p8rLy/XAAw8cN2i/8MILmjNnju6//35VVFTo8ssv1z/+8Q+vUHfFFVeoQ4cOeuKJJ/S3v/1NZWVl6tixo84++2xde+21Zru5c+fq1ltv1cyZM2UYhkaPHq1PPvnEaxXsQEpPT9d3332nadOm6fnnn1dpaakGDhyojz76SOPHjw9KDU0RHx+vr776Srfeequee+45xcfH6+qrr9YZZ5yhSy65xAzctRk8eLA++eQT3X333brvvvuUmZmphx9+WBs3bjR/6SO5V0t/9913NWPGDN19993KyMjQLbfconbt2um6664L9McEADRzFoMVOQAAQDP37LPP6s4779Tu3bvVsWPHBr/+oosu0vr164M6Xx4A0HKF9/gwAACAY5SUlHg9Li0t1UsvvaRevXrVK2Qf+/rNmzdr4cKFGj58uD/LBAC0YgwdBwAAzcrFF1+szp076+STT1Z+fr7eeust/fLLL7VuAXes7t2765prrlH37t3122+/afbs2YqMjNQ999wT4MoBAK0FQRsAADQr2dnZevXVVzVnzhw5nU717dtX8+bN06WXXlqv148ZM0Zvv/22cnJyFBUVpaysLD322GPq1atXgCsHALQWzNEGAAAAAMCPmKMNAAAAAIAfEbQBAAAAAPCjZjlH2+Vyae/evUpISPDawxQAAAAAgEAwDEOFhYXq0KGDrNa6+6ybZdDeu3evMjMzQ10GAAAAAKCV2bVrlzp16lRnm2YZtBMSEiS5P2BiYmKIqwEAAAAAtHQFBQXKzMw082hdmmXQ9gwXT0xMJGgDAAAAAIKmPtOXWQwNAAAAAAA/ImgDAAAAAOBHBG0AAAAAAPyIoA0AAAAAgB8RtAEAAAAA8COCNgAAAAAAfkTQBgAAAADAjxoUtB9//HGdeuqpSkhIUFpami666CJt2rTJq83w4cNlsVi8vv7whz94tdm5c6fGjx+v2NhYpaWl6Y9//KMcDkfTPw0AAAAAACFmb0jjxYsXa8qUKTr11FPlcDj0pz/9SaNHj9aGDRsUFxdntrvxxhv18MMPm49jY2PNY6fTqfHjxysjI0Pfffed9u3bp6uvvloRERF67LHH/PCRAAAAAAAIHYthGEZjX7x//36lpaVp8eLFOueccyS5e7RPPvlkPfvssz5f88knn2jChAnau3ev0tPTJUkvvviipk2bpv379ysyMvK41y0oKFBSUpLy8/OVmJjY2PIBAAAAAKiXhuTQJs3Rzs/PlySlpKR4nZ8zZ47atm2r/v37a/r06SouLjafW7ZsmQYMGGCGbEnKzs5WQUGB1q9f7/M6ZWVlKigo8PoCAAAAACAcNWjoeHUul0t33HGHzjzzTPXv3988f8UVV6hLly7q0KGD1q5dq2nTpmnTpk167733JEk5OTleIVuS+TgnJ8fntR5//HE99NBDjS0VAAAAAICgaXTQnjJlitatW6dvvvnG6/xNN91kHg8YMEDt27fXyJEjtXXrVvXo0aNR15o+fbruuusu83FBQYEyMzMbVzgAAAAAAAHUqKHjU6dO1YIFC/T111+rU6dOdbYdOnSoJGnLli2SpIyMDOXm5nq18TzOyMjw+R5RUVFKTEz0+gIAAAAAIBw1KGgbhqGpU6fq/fff11dffaVu3bod9zVr1qyRJLVv316SlJWVpZ9//ll5eXlmm88//1yJiYnq27dvQ8oBAAAAACDsNGjo+JQpUzR37lx9+OGHSkhIMOdUJyUlKSYmRlu3btXcuXM1btw4paamau3atbrzzjt1zjnnaODAgZKk0aNHq2/fvrrqqqv05JNPKicnRzNmzNCUKVMUFRXl/08IAAAAAEAQNWh7L4vF4vP866+/rmuuuUa7du3SlVdeqXXr1uno0aPKzMzU7373O82YMcNruPdvv/2mW265RYsWLVJcXJwmT56sJ554QnZ7/XI/23sBAAAAAIKpITm0SftohwpBGwAAAAAQTEHbRxsAAAAAAHgjaAMAAAAA4EcEbQAAAAAA/IigDQAAAACAHxG0AQAAAADwowbtow0AAACgeSguKNfmVblyOlyhLgU4rsgYu/qf0zHUZfgNQRsAAKAJnA6Xfl2Zq7LiilCXAnj59t0toS4BqLfEttEEbQAAgHB3NL9MBQdKA36dX5bv04alewN+HaCxIqJt6nFyu1CXAdQpOiEy1CX4FUEbAAC0OIdzjmrugyuCft0ThqYH/ZpAXeISozT0gu6yRbA0ExBMBG0AANDibFuz3zxOahcT8OtFRNs07IoTldEtKeDXAgCEP4I24EeGYeinL3fpSG5x09/LD/X4500kGU1/I3+V4o838s/frZ8+Ubh8Hj+9keGvavxRi9/+YpqmpLBcJYXlkiyhLqVVKS4okyT1PbuDRkzqHeJqAACtDUEbqIWzwiWXy5BhGJLhDtFG5Z/ux6rx3J5fD7PwCACEEXqYAQChQNAGfFjx0TZ9//GOJr3Haed380stFr92gvnpzfxYk38/n39YwrEoyY/fPn9+A/34VmH41+6ve8HlNJTSMU7WcPyQLVhkjF1pXRNCXQYAoBUiaAM+7Fx3sF7tLJbKf4hX/mmxSLYIq4ZdfqJ6ncqCOAAAAEBrRNAGfHBUuCRJ46cMVKcT27jDtNXdeVc9WAMAAADAsQjaaDHyfivQhm/3yXA1fQWkokPufVejYiNkj7Q1+f0AAAAAtB4EbbQY3723VXs2Hfbre8YmRvj1/QAAAAC0fARttBiH9hZJknpnZfhlz9Tk9DgltYtt8vsAAAAAaF0I2mgRig6XqaSwQpLU7+yOyujOdi4AAAAAQsMa6gIAf9j+037zuF1ntnIBAAAAEDr0aKNZ++Gz3/TbuoPau/mIJKlDr2TZ7Pz+CAAAAEDoELTRbDkdLi37YKtUbZHxgSM6ha4gAAAAABBBG82Yo9xphuzRN/RTcnqs2mUybBwAAABAaBG00WyVlzolSRaL1HNwmiwWS4grAgAAAAAWQ0MztubznZKkyFg7IRsAAABA2CBoo9kqLXZv55WSERfiSgAAAACgCkEbzZaz3CVJ6nVqeogrAQAAAIAqzNFG2DMMw2tlcQ9HhTto2yL4fREAAACA8EHQRlg7klus9/6+WiWFFbW2sRO0AQAAAIQREgrC2t4tR+oM2RHRNqV1SQxiRQAAAABQN3q0EdaclcPDuw5I1cjJfWs8b4+yyh5hC3ZZAAAAAFArgjbCWsGBEklSZIxd0fERIa4GAAAAAI6PoeMIW4bL0JovdklyB20AAAAAaA4I2ghb5aUO87j7Ke1CWAkAAAAA1B9BG2GrvNQpSbJaLep0YpsQVwMAAAAA9cN43AC777779NFHH9X6/Ndff602bdwh8i9/+Yvmz59fa9uFCxeqQ4cOkqSnn35a//rXv2pt+95776l79+6SpNmzZ+ull16qte2cOXPUr18/SdLrr7+u5557rta2r776qoYMGSJJmjdvnp544ola277wwgs666yzJEkffvihHnjggVrb/u1vf9N5550nSfrss880bdo0FR4sVVmJu1f7pe/izbYPP/ywLrjgAknSkiVLdNttt9X6vn/60580ceJESdKqVat044031tr2zjvv1OTJkyVJP//8s6666qpa2/7hD3/QH/7wB0nSli1b9Pvf/77Wttdcc43uuOMOSdKePXs0fvz4Wtteeumlmj59uiTp4MGDGjlyZK1tL7jgAj388MOSpOLiYp1xxhm1th09erSefPJJSe59yU855ZRa25599tl6/vnnzcdDhw5VWVmZz7ZDhgzRq6++aj4ePny4jhw54rNt//799dZbb5mPx44dq3379vls26NHD/3f//2f+fh3v/udtm/f7rNthw4dtHDhQvPxFVdcoQ0bNvhsm5KSoq+++sp8fP3112v16tU+28bExGjZsmXm4ylTpujbb7/12VaS1qxZYx7ffffd+uKLL2ptu3z5ckVHR0viZ0RTfkbUhp8RbvyMcONnRBV+RrjxM8KNnxFu/IyoEsqfET179tS7775b63s0RwTtANu5c6d++umnWp93Op3m8e7du+tsW1FRtc3V3r1762xb/QdaTk5OnW1LSkrM47y8vDrbHj161Dw+cOBAnW0LCwvN40OHDtXZNj8/3zw+cuRIjbZ7DlYdHz582DwuKCio830PHqx6YVFRUZ1t9+/fbx4XFxfX2TY3N9c8Li0trbNt9f8IlJeX19nW8w8KSXI4HHW2rf4fOZfLVWfbE0880etxXW07derk9Xjt2rUqLS312dbzg9tj3bp1Xn/n1UVFRXk93rBhg3bu3OmzrcPh8Hr8yy+/6JdffvHZtvq9I0m//vprrZ8vLS3N6/GWLVtqbRsXF+f1eNu2bXX+vVW3Y8eOOtsahmEe8zPCrak/I6rjZ4QbPyPc+BlRhZ8RbvyMcONnhBs/I6qE8mdE9TpbCotR/TvVTBQUFCgpKUn5+flKTAzvPZR//vnnWn/bJrl/cxcZGSnJ/QNj9+7dtbY9++yzFRMTI0natGmTfvvtt1rbnnHGGYqPd/cCb9myRdu2bau17dChQ5WUlCRJ2r59uzZv3lxr2yFDhiglJUWS+//Ytf3QkqRBgwapbdu2kty/gV2/fn2tbU866SSlp6dLktau+FWvPPqBDPfOXjrvur6KSYg02/bv39/8bdv+/fv1448/1vq+ffr0UWZmpiT3f6S///77WtuecMIJ6tq1qyT3f6RXrlxZa9sePXqoR48ektz/EKj+G8tjde3aVSeccIIk9394v/nmm1rbZmZmqk+fPpLcP8AWL15ca9sOHTqof//+ktz/Qan+G9Zjpaen66STTpLk/uH8+eef19q2bdu2GjRokPn4iy++kMvl8tm2TZs2OvXUU83HX3/9tdcP6eqSkpI0dOhQ8/GSJUtq/Q9vfHy812/Wv/32W69/nFUXExOjs88+23y8fPlyFRQU+GwbGRmp4cOHm49XrVrl9Q+u6mw2m1dPwOrVq2v9j7/k/m2/x5o1a5SXl1dr25EjR8pmc29Lx8+Ihv+MyMnJ0dq1a2tty88IN35GuPEzogo/I9z4GeHGzwg3fkZUCeXPiGO/Z+GqITmUoI2ws/brXVr6jvv/gJl9U3T+rSfJYrGEuCoAAAAArVlDcihDxxF2XE73735OOC1d513XL8TVAAAAAEDDsOo4wo4naFtt9GIDAAAAaH4I2gg7ZtC2ErQBAAAAND8EbYQdl9O9YIbVxu0JAAAAoPkhySDslBW7t2SwMHQcAAAAQDNE0EbYOZrv3pfPcDa7BfEBAAAAgKCN8BMdFyGpagg5AAAAADQnBG2EHUe5O2AnpceGuBIAAAAAaDj20UZYyN9frCO5JZKkTStyJEkRkbZQlgQAAAAAjULQRsiVFJZr7kMr5HJ4z8mOiuX2BAAAAND8kGQQcjnb8uVyGLLaLUrtEC/JHbI790sNcWUAAAAA0HAEbYTcsve3SpKS2sZo4p9ODXE1AAAAANA0BG2E1K5fDulwTrEkqfvJ7UJcDRA4hsvQkf9slau4ItSlAAAAhB1rXITaXNgz1GX4DUEbIfX5a+vN4z5ntg9hJUBgHfh/61S25UioywAAAAhLtpToUJfgVwRthFRZsUOSdPpF3ZXUju280DIZhqGyrUckuf8jknBWx9AWBAAAEGYsUS1rxyGCNkLGMAy5nO6Vxvuc0SHE1QAB5DKkykX106eeLGtsRGjrAQAAQEBZQ10AWi+Xq2o7L6vNEsJKgMAynNW2rrPxYxcAAKCl4198CBmXk6CNVqLavW7hXgcAAGjxCNoIGWeFyzy20cuHFsxwVt3rImgDAAC0eMzRRsgUHiw1j612wgeapnTrER15f4uMCmeoS6nB8ORsq0UWC/c6AABAS0fQRsgYhns4rT3KRvhAkxV8/pscB0pCXUadItJYWR8AAKA1IGgjZDxztGMTWIEZflA5DTrmpHZKODs8t8+KSCdoAwAAtAYEbYSMJ2hb/Tw/u+DLnSpcukcyjOM3bqgGv2UDXhCAcs23btB7N7CQQNXd0PetXMU+9pQ0RXZK8H89AAAAQD0RtBEynu29LFb/Dhsv+m6PjFKHX98TzYMl2q7IjvGhLgMAAACtHEEbIWOYPdr+C9plOwvkOuoO2W2v6y97SnT9XxzIaeKBnIPekLduUBkNrLkBzQNVhjXGLkuErSHvDgAAAPgdQRshU3DQvXCV1Y892qUbDpnHUT2S2bMYAAAAQNCxeTFCZvvaA5KkijL/bcdkONz7KMUNzSBkAwAAAAgJgjZCxm53334dT0j223saTnfQtsaxkjkAAACA0CBoI2Sclb3PaV0T/fimgVlgDQAAAADqi6CNkPEEbZvdf7ehZ4E1+XnLMAAAAACoLxZDQ8js2nhYUtNWHTccrqpwrao52szPBgAAABAqBG2ETExChEoKK2RrZO9zyS+HdPCtDZLDqPkkQ8cBAAAAhAjjaxEyhrvzWUnpMY16fdn2fJ8h2xJlU5Q/530DAAAAQAPQo42QcVWuEN7YHm3PwmfxZ3VU4ugu5mmLzSILc7QBAAAAhAhBGyHjqgzKjZ2j7dnKyxJplTXS5re6AAAAAKAp6PZDyDQ1aMvFVl4AAAAAwg9BGyHhqHDK5fIE7cbdhmzlBQAAACAckVAQEp6tvSQpIqqRw77p0QYAAAAQhgjaCImKUockyRZhbXTQrurRJmgDAAAACB8EbYSEs3Jbro4ntGn8m3h6tAnaAAAAAMIIQRshYW7tZW9cSK7IOaqSnw+4HzB0HAAAAEAYIWgjJDw92o1dCO3gnI3msTU2wi81AQAAAIA/ELQRElt/yJPUuB5to8Ilx/4SSVJ031TF9Enxa20AAAAA0BQEbQRd4aFS7d18RJIUGW1v8OtdJQ7zOPWK3rLYuY0BAAAAhA8SCoKutKjCPD75vMwGv96ocEqSLBFWQjYAAACAsNPw7kSgiZwO90JoiW2jldQutt6vO/CvDSr95VDVauMRhGwAAAAA4YegjaDzBG1bA3qjDYdLpRsOep2L6pbk17oAAAAAwB8I2ggqp9OlD57+UZJkbUjQrtwOTJIy/jhElgirrPGRfq8PAAAAAJqKoI2gOpJbbB6nd0us9+uMyu3AJMnWJloW9s4GAAAAEKaY5IqgMlxVgXn4FSfW/4WeHm2rhZANAAAAIKwRtBFULqc7aMe3iZLFUv/AbFS4g7bFRsgGAAAAEN4I2ggqT9C2NjAwH/0+131AbzYAAACAMEfQRlC5PFtzNTAwu0od7tdF2vxeEwAAAAD4E0EbQWV4erQbGLSNcvfQ8YSzOvi9JgAAAADwJ4I2gqr0aIWkhg8dN8qdkujRBgAAABD+CNoIKqNy0fHDOcV1NzyGq4ygDQAAAKB5YB9thETbTvH1aucqrpCzqEJlvx6WJFmjCNoAAAAAwhtBG0Hl2Uc7Irr2wOw4VKrcZ1bLMAzJYXg9Z02MDGh9AAAAANBUBG0ElbnqeB17aBd8udPcN9vDEmNXRLsYRWYmBLQ+AAAAAGgqgjaCyjDqsep4ZZuI9nFKvbqvbElRDd4ODAAAAABChaCNoHI567GPdmWvd+ygdNnbRAejLAAAAADwG1YdR1B55mjXFbSr2gSlJAAAAADwK6IMgsoTouscOl7Z660G7rUNAAAAAOGAoI2gKjpcJqnu3mpPGBfzsgEAAAA0QwRtBJXT4V5NvKzYUWsbo8wpSbJYuT0BAAAAND8NSjKPP/64Tj31VCUkJCgtLU0XXXSRNm3a5NWmtLRUU6ZMUWpqquLj43XJJZcoNzfXq83OnTs1fvx4xcbGKi0tTX/84x/lcNQevNAyHNxbpDVf7JIkJaXF+mxjGIbKtuW7H5CzAQAAADRDDYoyixcv1pQpU7R8+XJ9/vnnqqio0OjRo3X06FGzzZ133qmPPvpI8+fP1+LFi7V3715dfPHF5vNOp1Pjx49XeXm5vvvuO/3zn//UG2+8ofvvv99/nwphafcvh83j7ie19dnGKK/aPzuyc2LAawIAAAAAf7MYno2NG2H//v1KS0vT4sWLdc455yg/P1/t2rXT3Llz9fvf/16S9Msvv6hPnz5atmyZTj/9dH3yySeaMGGC9u7dq/T0dEnSiy++qGnTpmn//v2KjIw87nULCgqUlJSk/Px8JSYSxpqLlQu2a9WC7ep1arpGX9/PZ5uynQXaP+snySJ1fOwsWSzM0wYAAAAQeg3JoU0anJuf7x7im5KSIklavXq1KioqNGrUKLNN79691blzZy1btkyStGzZMg0YMMAM2ZKUnZ2tgoICrV+/3ud1ysrKVFBQ4PWF5qeicu51XFLtv0xx5BW7DwwRsgEAAAA0S40O2i6XS3fccYfOPPNM9e/fX5KUk5OjyMhIJScne7VNT09XTk6O2aZ6yPY873nOl8cff1xJSUnmV2ZmZmPLRgg5y91B2x5pq71R5cjxqBPaBKEiAAAAAPC/RgftKVOmaN26dZo3b54/6/Fp+vTpys/PN7927doV8GvCfwzD0JJ5v2rrj/slSfbI2m87z0wGi52V0AAAAAA0T/bGvGjq1KlasGCBlixZok6dOpnnMzIyVF5eriNHjnj1aufm5iojI8Nss3LlSq/386xK7mlzrKioKEVFRTWmVISB/LwS/bxot/k4ITW69saeoM2ocQAAAADNVIO6DQ3D0NSpU/X+++/rq6++Urdu3byeHzx4sCIiIvTll1+a5zZt2qSdO3cqKytLkpSVlaWff/5ZeXl5ZpvPP/9ciYmJ6tu3b1M+C8KUZ+/siGibLrzjZPUcnF57Y8+i41aSNgAAAIDmqUE92lOmTNHcuXP14YcfKiEhwZxTnZSUpJiYGCUlJen666/XXXfdpZSUFCUmJurWW29VVlaWTj/9dEnS6NGj1bdvX1111VV68sknlZOToxkzZmjKlCn0WrdQnnXt7ZE2deqdUndbV2VjgjYAAACAZqpBQXv27NmSpOHDh3udf/3113XNNddIkp555hlZrVZdcsklKisrU3Z2tmbNmmW2tdlsWrBggW655RZlZWUpLi5OkydP1sMPP9y0T4KwZTRkOHhlzmboOAAAAIDmqkFBuz5bbkdHR2vmzJmaOXNmrW26dOmihQsXNuTSaM7M8FyP9GyQtAEAAAA0byztjIBrSI82Q8cBAAAANHcEbQSc0age7cDVAwAAAACBRNBGwJk92se52wzDUGHlNmAWerQBAAAANFMEbQTe8af2S5IOv7tZRplTkmRvFxPAggAAAAAgcAjaCDjPvOu6ho5X5B5V8epc83H8mR0DXhcAAAAABAJBGwFnTruuYzh4RW6xedz+vtMZOg4AAACg2SJoI+Dqtep4ZZuo7kmyxUUEoSoAAAAACAyCNgLO3H69rqTtaUNPNgAAAIBmjqCNgKtPj7a5fzY5GwAAAEAzR9BG4NVnH+369HoDAAAAQDNA0EbAlZc4JB1nH21zr22CNgAAAIDmjaCNgLPa3bfZgd1FtTdyVf5JzgYAAADQzBG0ETTtMhNqfc4w9wAjaQMAAABo3gjaCDjPQmc2e11ztOuxBRgAAAAANAMEbQScJ2jXOf+a7b0AAAAAtBAEbQScUY9Vx9neCwAAAEBLYQ91AWj5qnq0fT9f+M0eFX+f437A2HEAAAAAzRxBGwFnmPOva4ZoV6lD+Qu2mY9t8RFBqwsAAAAAAoGgjYCra462UeZ0H1ilNhefoJj+qcEsDQAAAAD8jqCNgHNV7pHtq0fbqHA/aYmwKW5IejDLAgAAAICAYDE0BJxn6LjVx93mKnFIkix1bf0FAAAAAM0IQRsBV1pU4T7w1aNdOazcddQRzJIAAAAAIGAI2gi4fVvzJUnOCmfNJ53uoG1vFxPMkgAAAAAgYAjaCLjoOPdSADGJkTWeMzwTuH0slAYAAAAAzRFBGwHnqhwe3rZTQs0nnbWvSA4AAAAAzRFBGwFneDqtfW3vVRnCZSNoAwAAAGgZCNoIOFctvdauMqcOzdvkfs7GrQgAAACgZSDdIOA8vdbHbu9VtvWIjDL3Amm2pJrztwEAAACgOSJoI+A8c7SP7dE2nC7zuM3vegW1JgAAAAAIFII2As6oJWircnp2VPckWWPsQa4KAAAAAAKDoI2AcpQ79du6g5Ik67ELnhlGCCoCAAAAgMAiaCOgDuwuMo9T2sd5P+nJ2WztBQAAAKAFIWgjoFyV87CjYu1K65Lo/aQnaJOzAQAAALQgBG0ElGdrr7jkqBrPGZ6h4xaSNgAAAICWg6CNgPIE7RrzsyWzR5ucDQAAAKAlIWgjoFzmHtq+gjZJGwAAAEDLQ9BGQHl6tGts7SUxRxsAAABAi0TQRsAYhqHPXlknyffQcYPtvQAAAAC0QARtBExFqdPs0W6XmVCzgdmjTZc2AAAAgJaDoI2A2bvliHl81sReNRswRRsAAABAC0TQRsBs/SHPPLb4StOeoeO+5m8DAAAAQDNF0EbAOB3uIH3SuZm+G7AYGgAAAIAWiKCNgNm/s1CSlJwR6/N5g+29AAAAALRABG0ETNGh0rob0KMNAAAAoAUiaCNgouIiJEkpHeJ8N6js0fY5fxsAAAAAmimCNgLGMzQ8IspWS4MgFgMAAAAAQULQRsBUTcGupceaoeMAAAAAWiB7qAtAC2YODfc+XfrrYR1ZsFXOwgr5bAAAAAAAzRhBGwFjuNx/HtujXfxjnhx5JeZje7uYYJYFAAAAAAFF0EbAeOZoW46ZoGA43Qk8/qyOih2crohatv8CAAAAgOaIoI2AqzFH2+UO4Pa20YpsX8uK5AAAAADQTLEYGgLGcHlWQzv2fOWBlbnZAAAAAFoegjYCptZVx12eIeUEbQAAAAAtD0EbAWPUsuq42dNN0AYAAADQAjFHGwFjHLNPtmEYKl6dJ8cB94rj9GgDAAAAaIkI2giYqh5td6Au31Wow+/+aj5vieb2AwAAANDykHQQOMfso22UOCRJ1li74s/sqOieySEqDAAAAAACh6CNgDFHjleuBOAZSm5rE63EkZ1DUhMAAAAABBqLoSFgPIue1Vh1nKnZAAAAAFowgjYCorig3Dz29GhXrY4GAAAAAC0XQRsBcfRImXkcEx/pPqhtX20AAAAAaEEI2ggIV+Ww8fg2UVUnj9nuCwAAAABaIoI2AsLc2qv6XtkMHQcAAADQChC0ERjmMHEfzzF0HAAAAEALRtBGQPhacdzTy83QcQAAAAAtGUEbAeF76HjlnwRtAAAAAC2YPdQFoGUyXO4/fY8SJ2kDQEOUO1xysc5Fk5VVuHTbvB+1L78k1KUAAI6RkRSjf113WqjL8BuCNgKirsXQQjFF2+F0BfwaW/YX6UhxRcCv4xHsf3MbCvoFW/LlWvz3L/ifL8jXC+IH/Od3O/T1pv1Bux4AAKFQUuEMdQl+RdBGQFT1aId+6PhN//pe/92QG9yLAgDC1lk92+p/R/QIdRkAgGqi7LZQl+BXBG0ERFWPdvWTlX8GsUu73OEKasi2WS3q1jYuaNcL9uCAYI9GsAT5E7b0BfEtQf6ALf7+DOL12sRG6m+/P0lxUS3rHyGhYLNaFBvJP38AAIHFf2kQEOYC49VXHfccBPEfp4eLyyW5/2G16s+jZA3gte02q+Kj+L8UAAAA0NqRChAQrso50V49PiFYyGf3YfeCNzERNqXERQb9+gAAAABaH7b3QkAs/fdmSbVt7xW8Lu3DR6t6tAEAAAAgGAjaCIjCg6WSpJT21eYrm8PJg1eHZ/XCPu0TgndRAAAAAK0aQRsB4enJPnlU56qTIRg6XuZwD2FvaasYAgAAAAhfBG0EhLUyaEdE+wi4QezSLnO4e7Sj7NzqAAAAAIKDxdAQEIarcnuv6lO0zaXI/Xcdp8vQ9PfWaktekc/n8wrLJEmRBG0AAAAAQULQRkBU7aNdLVVXuPx+nTe+26F/f7/7uO0yU2L9fm0AAAAA8IWgDb8zDMPnPtpFy/dVNvDftX7efcQ8fumqwT7bRNmtyuqR6r+LAgAAAEAdCNrwv2pB2lJtxLY1PlLaXyJbon/2sy6tcGrd3gJJ0p/H9VF2vwy/vC8AAAAANAVBG35nVFtd3OI9SVuSFNWrTZPe/6XFW7V2d74+/nmfeS4tMapJ7wkAAAAA/kLQht8Z1aZie83R9gwnb8K6ZLsOFevxT37xOtejXZxO787QcAAAAADhgaANv/Pu0a72hMvwcfL4th84qpXbD0qS9hwplSQlRtt1x6gTdHr3VPXtkNikegEAAADAnwja8DuXq1rQrtajbQZwq++gfce8H7V652Gvc4Yh7T5cUqNt+6QYXXdWNz9UCwAAAAD+RdCG/1VfDM1Hj7avDu3DR8v1wZq9db7t2b3aKspulcVi0WWnZvqhUAAAAADwP4I2/M5r6LiPOdq+erQPHi0zj9//3zNqPN+9bbySYiP8ViMAAAAABApBG36Xt6PQPK6+6rhhztGu+ZrSCvcKahmJ0Tqlc9NWJQcAAACAUGrC+s+AbwUHq+ZUW601t/c6duz4uj35mvD8N5KkqAhuSQAAAADNG6kGfudyugN191PameccR8rkyHMHcMsxQ8e/2JhrHvfOSAhChQAAAAAQOARt+J0naFttVYH60Lyqva8tdu/bzlk5pPzUrm0084pBQagQAAAAAAKHoA2/8xW0XYXlkqTIzgmK6Bjv1d4TtPt1SJLdxi0JAAAAoHkj1cDvnE73wma2aqHZs+B40rhuNYaOOyvnbttq2V8bAAAAAJoTgjb8bsWH2yRJFpuPrb18bKLtquzRthO0AQAAALQABG34nb1y5fDUDnH1al/ZAe69QjkAAAAANFMEbfidp/O668C21U7Wvoe2yzN03EdvNwAAAAA0NwRt+J+5XbaPoeM+eBZDo0cbAAAAQEtA0IbfGZWp2lcHtcXHSSc92gAAAABaEII2/M/lOahfj/Z/1+dIktjZCwAAAEBLQLSB3/leYLz2OdoHitx7bNus3I4AAAAAmj+SDfzP18JntfRo7z1SYh5n90sPXE0AAAAAECT2UBeAlsfwsRiambMrz9369o/6fEOOXK6q13VvFx+cAgEAAAAggAjaCI5qPdqGYeizdTkqd1al7ItO7hCCogAAAADA/wja8CvDqErUFq+JCVXDyZ0uwwzZH992ltrERqp9UnTwigQAAACAACJow6+q5WxZall13OGqetA5JVYJ0RFBqAwAAAAAgoPF0OBf3km7BovVIle1NnZWGgcAAADQwjQ45SxZskTnn3++OnToIIvFog8++MDr+WuuuUYWi8Xra8yYMV5tDh06pEmTJikxMVHJycm6/vrrVVRU1KQPgvBQ63bZtfRok7MBAAAAtDQNjjlHjx7VSSedpJkzZ9baZsyYMdq3b5/59fbbb3s9P2nSJK1fv16ff/65FixYoCVLluimm25qePUIP9U7tK0+9veySK5qQdtm8dHtDQAAAADNWIPnaI8dO1Zjx46ts01UVJQyMjJ8Prdx40Z9+umnWrVqlYYMGSJJev755zVu3Dj9/e9/V4cONVefLisrU1lZmfm4oKCgoWUjSLwWQ/N6ouqweo+2zUrQBgAAANCyBGTg7qJFi5SWlqYTTzxRt9xyiw4ePGg+t2zZMiUnJ5shW5JGjRolq9WqFStW+Hy/xx9/XElJSeZXZmZmIMqGP1QfO+4rQ1ssZo+21eK91zYAAAAAtAR+D9pjxozRv/71L3355Zf661//qsWLF2vs2LFyOp2SpJycHKWlpXm9xm63KyUlRTk5OT7fc/r06crPzze/du3a5e+y4SdeObsyRL+7ercKSiokSde8vlK/f3GZJBZCAwAAANAy+X17r8suu8w8HjBggAYOHKgePXpo0aJFGjlyZKPeMyoqSlFRUf4qEQFkuGquOv7m8t/0ROXpnYdLtEvuPbS7pMYGuToAAAAACLyA76PdvXt3tW3bVlu2bNHIkSOVkZGhvLw8rzYOh0OHDh2qdV43mifPoPByh8s89+ylJ8uZ7P6lSZ/2CSGoCgAAAAACK+BBe/fu3Tp48KDat28vScrKytKRI0e0evVqDR48WJL01VdfyeVyaejQoYEuB4FWvUO7cui4w+mSpTJ2D8xMVkTbmFBUBgAAAABB0eCgXVRUpC1btpiPt2/frjVr1iglJUUpKSl66KGHdMkllygjI0Nbt27VPffco549eyo7O1uS1KdPH40ZM0Y33nijXnzxRVVUVGjq1Km67LLLfK44jual+qrjni5th8swgzZLnwEAAABo6Rq8GtX333+vU045Raeccook6a677tIpp5yi+++/XzabTWvXrtUFF1ygE044Qddff70GDx6spUuXes2xnjNnjnr37q2RI0dq3LhxOuuss/Tyyy/771MhZAwfq447XK6qgE3SBgAAANDCNbhHe/jw4d69lsf47LPPjvseKSkpmjt3bkMvjWZg18ZD5rEnUzuctd8vAAAAANDSsL8S/Cp3R4F5bLG6o3aF06jWo02XNgAAAICWjaANv3JV9l4Pyu5StRha9aHjAAAAANDCEbThV56gbbNXRWun01C0J2qTuAEAAAC0cARt+JXL6d4z22pz31ol5U4VlTnM5z3DyQEAAACgpSJow28Mw1BJYYUkyWpzB+r/b/4ar5vMEsEtBwAAAKBlI/XAb754fYN2rD0gqSpobz9QzGhxAAAAAK0KQRt+s3vTYfO4Q69kSVJZhdM7aDN0HAAAAEALR9CG/1Rulz3xT6cqrUuiJGnbgaP0aAMAAABoVQja8BvDcCdtz7BxSUqItnsHbfbRBgAAANDCEbThN5U522sLL6fL8L7JyNkAAAAAWjiCNvzG06NtqdZr7XAZXm3o0AYAAADQ0hG04T+VmdpSV482XdoAAAAAWjiCNvzGMIO2pfKxIafLOGaOdtDLAgAAAICgImjDbwwzabv/8AwbJ2gDAAAAaE0I2vCbY3u0nWbQrpaumaQNAAAAoIUjaMN/zMXQ3A/p0QYAAADQGhG04TfHbu/ldNYM2hZ6tAEAAAC0cARt+M2x23sdLXe4H3sakLEBAAAAtAIEbfiPy/2HIWnZ1oM644mvJEn20FUEAAAAAEFH0IbfeEaOv79mjy5/Zbl5fkz/DPcBw8YBAAAAtAIEbfiNZ+j4niPF5rlHLuqvB87v535AzgYAAADQChC04ReGYZhd2o7KIeT/O7yHrjq9S1VXN0EbAAAAQCtA0IZfbPx2n3nsrOzZtlsrk/Wxy5EDAAAAQAtG0IZfbFqRYx6XV+Zpu819e5XvLpTEFG0AAAAArQNBG37hmZ997tV95KwcK26r7NF25pe721S4QlMcAAAAAAQRQRt+FRltU3G5U5IUYavswna5g3fsKWmhKgsAAAAAgoagDb9bvu2gJMlSOSfbcFbO0bYxdhwAAABAy0fQhn9ZpMyUWEmS3ROsne4h4xaCNgAAAIBWgKAN/zCqDssq52J3bxfvfqpy6LisBG0AAAAALZ891AWgZbHIog37CiRJcUcrdOBfG1Sxt8j9HEEbAAAAQCtA0IZfeLbKLq1wmufabC9U6YaD5mNbclSwywIAAACAoCNow0/cSbvcWbWFV2pMpIokRZ/YRnFD2yv6hDYhqg0AAAAAgoegDb/y7KdtqzZM3N4uVjF9U0NVEgAAAAAEFYuhwS88Q8eNyi29mI4NAAAAoLUiaMOvXJWJ22KxVK1ETugGAAAA0IoQtOFXRmW6tlks8trzCwAAAABaCYI2/MowfAwdp0cbAAAAQCtC0IZfeOZouyp7sa3Vh44DAAAAQCtC0IZ/VCZtz9Bxi1cvNl3aAAAAAFoPgjb8ytOzbbOyGBoAAACA1omgDb/yZGurhXQNAAAAoHUiaMOvXK5q23tVInIDAAAAaE0I2vALz5DxPUdKJB2z6jgAAAAAtCIEbfjF/qIySdJLS7ZJ8szRrkzfhG4AAAAArQhBG35RVuGUJMVG2tSpTYyuPL1LiCsCAAAAgNCwh7oAtCzjB3bQ/17eX5J05KOtlWfp0gYAAADQetCjDf9glDgAAAAASCJow8+8tvViH20AAAAArRBBG35hZmpCNQAAAIBWjqAN/6D3GgAAAAAkEbThZ+RsAAAAAK0dQRt+4u7Srj5H22AfbQAAAACtEEEb/uHJ1NxRAAAAAFo5YhH8is5rAAAAAK0dQRt+UbXquI/tvQAAAACgFSFow68svvb3Ys8vAAAAAK0IQRv+4ZmjTaYGAAAA0MoRtOFXBG0AAAAArR1BG37lPUfbqDwXomIAAAAAIAQI2vArMjUAAACA1o6gDf8we6+J2gAAAABaN4I2/MLhYpg4AAAAAEgEbfhJucMlSXIa1TbPrtpcO/gFAQAAAECIELThF3arO0ynJ0SHuBIAAAAACC2CNvzC03mdEB0R0joAAAAAINQI2vCPyqRtszJMHAAAAEDrRtCGX9mq31HmHO1QVAIAAAAAoUHQhl8YlanaSo82AAAAgFaOoA3/8DF03PCsQE72BgAAANCKELTRZLMXbTWPmaMNAAAAoLUjaKPJVu04ZB4nxfhadZzwDQAAAKD1IGijyZwuwzy2WrmlAAAAALRupCI0mdNl+O6zZtVxAAAAAK0QQRtNVr1H20KoBgAAANDKEbTRZE7DOH4jAAAAAGglCNposuo92l4qAzi93AAAAABaE4I2miy62KlkV+WtRKgGAAAA0MrZQ10AmjeH06WeW8vk+Z2NPcLmoxXpGwAANH8Ol0Pf7PlGJY6SUJcCtDgx9hgNzxwe6jL8hqCNJnn/xz2KMdxBOvmEJKV2jAtxRQAAAIExbck0/fe3/4a6DKBF6hjfkaANeOw9Umr2V4+4pKcs1Sdks70XAABoQdYfXC9JsllsGpI+JMTVAC1L29i2oS7BrwjaaJJyp9PM0VF2X8PGAQAAWoZyZ7kkad6Eeeqd0jvE1QAIZwRtNEm5w6XIymMLy4ubyp3lWrt/rZyGM9SlAAAQ1vLL8nWw9GCoy6iXoooiSVKkLfI4LQG0dgRtNIphGPrfOT9o0ab9ul4RkiQLa9ibHl72sD7c+mGoywAAAAEQHxEf6hIAhDmCNhrlQFG5PlmXI0myGpVB+9ge7cp9tFvjHO1dhbskSRlxGfzHGACA40iMTFRqTGqoy6iX/m37Ky02LdRlAAhzBG00iqsyRNusFsVG2uQsd9GjXU2Fq0KSNGPoDA3LHBbiagAAAAAEE9EIjWIGbYvF7LA+tkfbMI9aX5e2Z7GUCGtEiCsBAAAAEGz0aKNRXJ5R4RbJcFUeW8MvUG86tEk/H/g56Nc9VHpIkhRhI2gDAAAArQ1BG43iqkzaVotFRmXvtqdDu3x3oQ5/sEWO/SXuEwHM30crjmr+pvkqKC+o8VyFq0JvrH8jcBevhxh7TEivDwAAACD4CNpoFM86Z1aLZLg8QdudqIvX7lfF7iKzrT0lOmB1fLjlQz21+qnjthueOVyWIA9h75LYRX1T+wb1mgAAAABCj6CNRvHM0Xb3aLvPmUPHne4TMSe3U+LwTEVkxAWsjgMlByRJJ7Y5UUMyhvhsM6zTMGV1yApYDQAAAABQHUEbjeIJ2lFVK57JaqsM2pXn7G2iAxqyJWnN/jWSpLM7na3bB90e0GsBAAAAQH0QtNEonsXQYqoNx46Ocy/8ZQRh/+z8snwt3L5Qq3JWVV4q/BZiAwAAANA6EbTRKEa1oeOSFBFtq9nI4v/wW1xRrFU5qzT1q6le50/NONXv1wIAAACAxiBoo1E8PdqeeO21h3a1rb8aq9xZri1HtuhoxVHd9tVtKneWy2W45DAcXu16JPXQmG5jdHr70xt/MQAAAADwI4I2GsVleK807hWqjaYn7T988QdzWLgvMfYYPXTGQxrbbWyjrwEAAAAAgUDQRqOYq45XPvbVo92UadO/Hv5VkpQanaoIW4Qu6HGB/ueE/5HdalfbmLaNf2MAAAAACDCCNhrF3Ee7Mk1brNWeczW9R7vMUSZJmjN+jjrGd2z0+wAAAABAsFmP3wSoaX+ROwh7dvSSn3q0DcPQn7/5s0qdpZKkKFtU44sEAAAAgBAgaKNR5izfKUkqd7ok+e68bkyH9p6iPfrP1v9IktpEtVFSZFKjawQAAACAUCBoo1HsVneK7pORKOnYOdqNHzpe4igxj9+/8H1F2CIaXyQAAAAAhABBG43icLl7sod2bSPp2FXHK/9sYM4+UHJAF//nYklSx/iOSo1JbWKVAAAAABB8LIaGRnFULnhmNbf3qkrVRgN7tF2GS07DqRv/e6N5rm9qXz9VCgAAAADBRdBGozic7jBtt9Rcddzs0a6HI6VH9D8L/kc5R3PMc1nts/T3YX/3R5kAAAAAEHQMHUejVFQugubp0fb0Xjvzy1Ty0/7qp+q04eAGr5DdJqqNZo6aKauFWxMAAABA89TgNLNkyRKdf/756tChgywWiz744AOv5w3D0P3336/27dsrJiZGo0aN0ubNm73aHDp0SJMmTVJiYqKSk5N1/fXXq6ioqEkfBMFzpLhcK7YfkiRV5BRLqgrVh979taphRN23V0F5gW7+4mZJ0sB2A/XNZd/o64lfK8LKAmgAAAAAmq8GB+2jR4/qpJNO0syZM30+/+STT+of//iHXnzxRa1YsUJxcXHKzs5WaWmp2WbSpElav369Pv/8cy1YsEBLlizRTTfd1PhPgaD6ZssBSVJbp0WHlrl7r212963kLCh3P06JVky/trW+h2EYGvbOMPNx/9T+SopKks1qC1TZAAAAABAUDZ6jPXbsWI0dO9bnc4Zh6Nlnn9WMGTN04YUXSpL+9a9/KT09XR988IEuu+wybdy4UZ9++qlWrVqlIUOGSJKef/55jRs3Tn//+9/VoUOHJnwcBINn2HiHqAip0H3utPO7uQ8qF0Jrc0kv2eJ890z/cugXTfp4khwuhyTpgh4XaNpp0wJbNAAAAAAEiV8nwm7fvl05OTkaNWqUeS4pKUlDhw7VsmXLJEnLli1TcnKyGbIladSoUbJarVqxYoXP9y0rK1NBQYHXF0KnMmerS2qcJKltZrx6nJLmPln5nMVa+wTtFftWqNzl7vkekj5EfznrL8zJBgAAANBi+DXd5OS4F7VKT0/3Op+enm4+l5OTo7S0NK/n7Xa7UlJSzDbHevzxx5WUlGR+ZWZm+rNsNJDLs7WXj+fMrb3qCNqlDvc0grM7nq3Xsl/zd3kAAAAAEFLNohtx+vTpys/PN7927doV6pJaNVdlmLb52ENbLs8e2rW/ftGuRZKkzomd6ckGAAAA0OL4NeVkZGRIknJzc73O5+bmms9lZGQoLy/P63mHw6FDhw6ZbY4VFRWlxMREry+EjrMyaHuytNc2XpVBu66h48UO90rlTpczEOUBAAAAQEj5NWh369ZNGRkZ+vLLL81zBQUFWrFihbKysiRJWVlZOnLkiFavXm22+eqrr+RyuTR06FB/loMAMYeO+9go26ico13X0PEyZ5kk6bwu5/m9NgAAAAAItQavOl5UVKQtW7aYj7dv3641a9YoJSVFnTt31h133KFHH31UvXr1Urdu3XTfffepQ4cOuuiiiyRJffr00ZgxY3TjjTfqxRdfVEVFhaZOnarLLruMFcebCZc5Dbty6LjVIsPhUsX+EnOltNp6tB9Z9oj2FO2RJCVFJQW+WAAAAAAIsgYH7e+//14jRowwH991112SpMmTJ+uNN97QPffco6NHj+qmm27SkSNHdNZZZ+nTTz9VdHS0+Zo5c+Zo6tSpGjlypKxWqy655BL94x//8MPHQTA4zR7tqnN5L/6kit1FVSdqCdpf7/raPM5MYFE7AAAAAC1Pg4P28OHDq1aW9sFisejhhx/Www8/XGublJQUzZ07t6GXRpjwLIbmmXdgsVjkyHXPu7bG2RXRIV721Bifr/UMG3/vgvcUGxEb8FoBAAAAINgaHLSB5dsOSao2dLxa53Xa1FNkbxPt62WSpHKne/9sQjYAAACAloq9ldBgX2ysXFXeM7DBItUxyEGS9O9N/9bQOUNV6nTvoR1liwpcgQAAAAAQQgRtNFhcpE2SdEaPVEmefbTr3j/7i9++MLf16pzQWclRyQGuEgAAAABCg6CNBitzuFcWbxdfrVfa7NH2nbRdlft+TT9tuj648APZrcxaAAAAANAykXZQb06Xode/3S5H5arjdqtne69qjWrp0XYYDklSakyqImwRgSwTAAAAAEKKHm3U28rth/ToxxslSZE2qyJs5rrjxxs5bvZo2yy2wBYJAAAAACFG0Ea9FZU5zOOZkwYpwlZ91fG6k7bTcEqSrBZuOQAAAAAtG6kH9ebZP3tQ52Sd1zfdXGm8+lpotc7RdtGjDQAAAKB1IGij3ozKZO3ZP9s7adeNHm0AAAAArQWpB/VWuQaaGbR95uzjDB23WenRBgAAANCyEbRRb56h48d2aFdfDK02vx7+VRJDxwEAAAC0fGzvhXor3lOsGwqiFPdzif7fPd/IWe7upT5ej7ZnxXFJirJF1WwAAAAAAC0IPdqot5JthWrjsirSIZUUlKu81B20U9rH1vqa3KO5uvCDC83HfVP7BrxOAAAAAAglerRRb57F0HLb2nXbHwZJkmx2i5LaxWjvqhx3o2MWRvtu73faUbBDktQ9qbsibZFBqxcAAAAAQoGgjXrzzMmuiLCobaf4qvOu2idorzuwTpLUMb6j3h7/dkDrAwAAAIBwwNBx1JtxzGJoVU/4bv/d3u/071//LUk6vf3pio2ofYg5AAAAALQUBG3UW322zfY89+n2T3Xz5zeb5y/rfVkAKwMAAACA8EHQRv15eq7r0aX97uZ3zePnz31evVN6B64uAAAAAAgjBG3Um1Fbl3b1nF35nMPlkCTNGDpDwzOHB744AAAAAAgTBG3UW32Gjnv20fbsnZ0SkxLYogAAAAAgzBC0UW/mYmg1nqjZ1hO0rRZuMQAAAACtCykI9VZQUuE+qEfS9gRtm8UW2KIAAAAAIMwQtFFv0Xb37XKouMLrvOE1R9v9h9NwSqJHGwAAAEDrQwpCvXkCdaeUmDpauZM2PdoAAAAAWiuCNurNM0fbWudG2u4/PD3aljpXTgMAAACAloegjXpzuTupa4ZnX4uhuejRBgAAANA6EbRRb2aPdo27pippezJ4qbPU3ZY52gAAAABaGVIQ6q1qH+26e7Q/3fGp9hTtkUSPNgAAAIDWh6CNeqvq0a5r3rVFK/etNB/1SO4R4KoAAAAAILwQtFEvB4vKtOPAUUk+FkM7Znsvh8shSZpy8hQlRSUFqUIAAAAACA8EbdTLN1sOmMdt4iK9nzS8x457gnaULSrgdQEAAABAuLGHugCEr4N7irR/V6Ekqei3fCW53L+XOaVzcu0vqtajbbdyewEAAABofUhC8Kmi3Kl3n1wtR5nTPNe+cgCELaKugRAWOQyCNgAAAIDWiyQEnypKnWbIPrFXkjofKJbVZchikaI/36E9X/xW1bja0PHLPr5MOwp3SCJoAwAAAGidmKON4zptUDvFGlK0xaIoWWSUOmWUOKq+St2B/GBCgTYc2qBiR7EkqUtCl1CWDQAAAAAhQZcjfDLMTbMludyHX6lCP3eL1WMXD/T5mufXPSjtlK7ofYUuPfFSdU/uHpRaAQAAACCcELRRb4UydDDKpoh2sT6fd1jcPdtdErsQsgEAAAC0Wgwdh2/VOrQ9c7ANz+NaX+JuZ7VwWwEAAABovUhEqJulKlob3g9rcBnuMeYEbQAAAACtGYkIPlWfou2t9qTtmddtqSuNAwAAAEALR9BGLaq27PKE7uP2aFeummbltgIAAADQipGI4JO5NfYxwbrOOdr0aAMAAAAAQRvHYZH3Ymj1mKNtqTOOAwAAAEDLRtBGnaqHZkOStY6kbQ4dZzE0AAAAAK0YiQg+GdVXQzPnaBt19mh7XkPQBgAAANCakYjgm+H7dF3Dws2h48zRBgAAANCKEbRRp+qR2Tj2xDGMynTOquMAAAAAWjMSEepWbei452FtGDoOAAAAAARt1KJqe6+qpO1edZyh4wAAAABQF4I2alG5J3bVoTto19J6ye4l+j73e0n0aAMAAABo3UhE8MmoNly8+rpotXVWP7P6GfO4TVSbwBQFAAAAAM0AQRt189req/Ye7RJHiSTp+v7Xa1D6oGBUBgAAAABhiaAN36pto139VG3zrz0LoY3qMoqh4wAAAABaNRIR6nbsYmi1NHMazsrmLIQGAAAAoHUjaMMnTw+15ZjtvWpL2p4Vx20WW2ALAwAAAIAwR9DG8VUL2tZaeqzNrb3q3GkbAAAAAFo+gjbqra6h4/RoAwAAAIAbQRs+mdt7WaoeGJJsVt9R2zNHm4XQAAAAALR2pCL4Zq46bvE6FR3hu8faM6eboA0AAACgtSMVoW7HdGBfeHIHn808PdoMHQcAAADQ2hG04ZNRfQW0aoeRdt+3jLkYGtt7AQAAAGjlCNrwzTN03OJ9ym6tecv8tP8nlTpLJdGjDQAAAAAEbRyXcZzF0FbuW2ket41pG6yyAAAAACAsEbRRN4v3Ymh2H0HbYTgkSRf3ulgRtohgVQYAAAAAYYmgDZ+MavOy9xe4h4XX1qPtdLkXQouyRQWjNAAAAAAIawRt1MKdtA8WlenT9bnm2Sgfi6Gx4jgAAAAAVCFowydPj7bLqNrh65TMZKUlRtdo6+nRJmgDAAAAAEEbtTGq/oi0uaP2sBPb+WzqmaNtsxK0AQAAAMAe6gIQfsr3HZVjda5OiLKqzGIoIypSKnbV2r64olgSPdoAAAAAIBG04cPhd36RM6dYfWIqg3NlyLZE1gzSTpdT/7f5/yTRow0AAAAAEkG72SgvdWjrD3mqKKu9Z9lfkg+XySppb7lLR2WovGO0hvZJV+ygtBptiyqKzOMzO5wZ8NoAAAAAINwRtJuJn77cpZUfbQ/KtbIT7Yq2WrSpzKnfDJdKe8Qpe1w3n20rXBXm8UntTgpKfQAAAAAQzgjazURpkTvQtsmIVUqH+IBey/bbEclpyJkWra8L8nWWpebe2R4Ol3shtAhrhCx1tAMAAACA1oKg3UxULgKuHoPSNPSC7gG91t5Hl8tVVKE9XaO0daNLw6y1L05f4XT/AiDCGhHQmgAAAACguWB7r+bCOH4T/13LfbEvNuZJkuy22nuqtxe4h7PbrfzOBgAAAAAkgjZ8qQz1nmXXxvbPqLXpsr3LJEkF5QUBLgoAAAAAmgeCdnNR2cusIEyDNioTtktS74wEndK5zXFfc16X8wJbFAAAAAA0EwTtZiKYI8c9VzMk2ax1J3tXZSrvmtg1wDUBAAAAQPNA0G5mgrKut+H5w6h30LZZbYGuCgAAAACaBYJ2cxGCxdAM6bhbdnmCtpVbCQAAAAAkEbSbn2DsVV1tMbQ6FhyXJDkNpyTJauFWAgAAAACJoN1seDq0g5KzzaHjkvU4FzQqK2PoOAAAAAC4EbSbCyOIY8crr+WSZD3OHG2ny92jbQnO7HEAAAAACHsEbdTk1aNdd1NzMTQLPdoAAAAAIBG0m42w3d5LlYuhMUcbAAAAACRJ9lAXgHqqTNq1TZk2XIacheV+vZZLdc/RNgxDH2/7WBJBGwAAAAA8CNrNju/ge/Cf61W66bBfr2RIirLXPiR83YF15nFcRJxfrw0AAAAAzRVBu7k4zmJoZb8Vug+sltqyeL05XYZ+NBw6IkP/O6JHre0Ol1UF++yu2U27KAAAAAC0EATtZsKM2bWF6Mognn7XYEW0jWnStZ77YrOe+eJXXTG0swZ1blNru1JHqSRpUNogxUbENumaAAAAANBSMLG2mal1ynRl0PbHPtuuyveqax20H/N+1P+3+P+TJEXaIpt+UQAAAABoIQjazcVxlh2v3GXLL0nbc6m69sb+Pud783ho+6FNviYAAAAAtBQE7WbiuNt7eeZwH2/j6/pcqx492p79s8d0HaMbBtzQ5GsCAAAAQEtB0G4pXJVDx/3wHXWZw9BrT9qeoJ0Ymdj0CwIAAABAC0LQbi6OF37N8d7+6NF2/1nXHtouuequBwAAAABaKYJ2C2C4qg0s98PQcc/b1ZWhPT3aVn90oQMAAABAC0JKai7qmqRdbY9tf3Qw12eOdlUbbiEAAAAAqI6U1EzUtY+266ij6oFferQ9Ifr4c7TrWpkcAAAAAFojgnYz4yvYFn27p+p5W9O/pYY5dPz4c7Tp0QYAAAAAb6Sk5qKOoeOuUnePtj01WpaIpn9LXfXYKYyh4wAAAADgGympmTA8SdtH+DWc7udih6T75VpV23vV1YZVxwEAAADAF4J2S2Duoe2fb6fRgDnaVm4hAAAAAPBCSmou6thyy9OjLZt/epertuSu/f08PewMHQcAAAAAb6SkZsKoa3svZ+Uwbj8E7fySChVWzvmua442Q8cBAAAAwDd7qAtA05WsO+g+aOLWXh/8uEd3vLPGfFzb1l0ljhKVOkorL8nvagAAAACgOr+npAcffFAWi8Xrq3fv3ubzpaWlmjJlilJTUxUfH69LLrlEubm5/i6jxfLZg2x3fxvtbaIb/b77C8u8Qna7hCidfULbGu2W71uuM98+U+9veV8Sc7QBAAAA4FgB6dHu16+fvvjii6qL2Ksuc+edd+rjjz/W/PnzlZSUpKlTp+riiy/Wt99+G4hSWofKoeMRGXGNfosFa/eaxx9MOVMnZyb7bLcmb40qXBWSpLiIOJ2acWqjrwkAAAAALVFAgrbdbldGRkaN8/n5+Xrttdc0d+5cnXvuuZKk119/XX369NHy5ct1+umnB6KclqGWSdqG06havawJc7R/2VcoSUqNi6w1ZEuS03BKki498VL9aeifGDoOAAAAAMcISNDevHmzOnTooOjoaGVlZenxxx9X586dtXr1alVUVGjUqFFm2969e6tz585atmxZrUG7rKxMZWVl5uOCgoJAlB1yFQdKdPS7vTIcrhrPZeQdVWyMVXEbD+hwfql53nBVBXCLvfGh11H5PpedlllnO6fLHbQjrBGEbAAAAADwwe9Be+jQoXrjjTd04oknat++fXrooYd09tlna926dcrJyVFkZKSSk5O9XpOenq6cnJxa3/Pxxx/XQw895O9Sw07hol0q/t73fPUUSSlRNml3kY7uLqrxvCXS1qSgXVE5/DwlLqrOdg7DvSK5zWJr9LUAAAAAoCXze9AeO3aseTxw4EANHTpUXbp00b///W/FxMQ06j2nT5+uu+66y3xcUFCgzMy6e16bI6PM3Vsc3TtFkZkJXs9tXp2rQ3uOqsuAVGV0S6rx2qhuiU3a3ssTtCOP8x4Olzto260sWA8AAAAAvgQ8LSUnJ+uEE07Qli1bdN5556m8vFxHjhzx6tXOzc31OafbIyoqSlFRdfe0tgiVw7eje7dR/OkdvJ7av7VAW7cVKr1HGyWO6OS3Sy7belCfrc/Rur35klwqM47oQMmBWtsfrTgqSbJZ6dEGAAAAAF8CHrSLioq0detWXXXVVRo8eLAiIiL05Zdf6pJLLpEkbdq0STt37lRWVlagSwl75npnvrbwCpB7/u8n7TpUIsml2G7P67nN+/Tc5uO/zm6hRxsAAAAAfPF7Wrr77rt1/vnnq0uXLtq7d68eeOAB2Ww2XX755UpKStL111+vu+66SykpKUpMTNStt96qrKwsVhyXzB5ti9VX0K58zs8Z/MhR91Zdlw5N08KCfe5ryOJ7v+5K8RHxGtp+qH8LAQAAAIAWwu9Be/fu3br88st18OBBtWvXTmeddZaWL1+udu3aSZKeeeYZWa1WXXLJJSorK1N2drZmzZrl7zKaJ0+Xtq+Q63t3ryYrq1zh/IazumvhQve5VVeuUpStFQzVBwAAAIAA8HvQnjdvXp3PR0dHa+bMmZo5c6a/L93smVt1BWnXrM25hSr3LIJWbcVyi4I3dB0AAAAAWho2Qg4ndQwdD8T07RXbD5nHidFVv3MhaAMAAABA4xG0w4jhqjwI0mJozspgn90vXdbqdwI5GwAAAAAajaWjg6RwyW6V/nKozjYV+4rcB74WQwtAl7Zn7+zoCJsMo2oSOD3aAAAAANB4BO0gMFyG8j/ZXu8FzfbuKZSzcjVwj8JDpX6vy1HZo223WmWIoA0AAAAA/kDQDgbDMEN2m0t6yRJpk8vl8tn0+6/3aP0H22t9K6vNfyHYM3Q8wmbxDtpB3McbAAAAAFoagnYwVOvJjunfVuuW5+ib+ZurVhn3wWK1KLN3G69zMQmR6nZS2wZfvsLp0vzvd2t/YZnX+eXbDkqSbFYLQ8cBAAAAwE8I2sFQPU9bpJ3rD9YZsmMSI3XF/UMVHR/hl8sv3rRff3r/51qfj4+206MNAAAAAH5C0A6KaqHaIrkqQ/awK05UryFpNVrbo2yy2fy3IPyh4nJJUsfkGA07sZ3Xc7ERNl1zRldJR/12PQAAAABozQjaQWB49Whb5HK6T0TG2BQV659e67p4Vhfv1yFRj/1ugM82+4vdK54zbBwAAAAAmoZ9tIPhmFHinmHjVmtw/vorHO6gHWGv/XqeoeMMGwcAAACApqFHOwg27s1XUuXxhz/t0f4C91ZdK3cc1FpLecCv/8POI5KkyDqGo3sWQ6NHGwAAAACahqAdYPklFbrylRX6SPGSpD/+38+6tDBKHWTVa9/t0NZVvrf5CoToCFutz5k92gRtAAAAAGgSgnaA5RdXyFFtz+zRHduow3p3j3bfjknqmBycb0G03aqrTu9y/IbkbAAAAABoEoJ2gBkyvHqJsx3R2i130P7zRf2U0T2ptpcGFUPHAQAAAMA/WAwtwAzDu5O49GiFJKn7ye2U3i0xNEX5wNBxAAAAAPAPgnaAGfIO2s4K9zDygSM6hdUK36w6DgAAAAD+QdAOIJfTpY2f/qbso1Uj9PMPuoeN2yLC66/eM3TcagmvugAAAACguWGOdgDlbMvX9m9y1Mvi/ms2DEMuhyGr1aKE1OgQV+fNOHazbwAAAABAoxC0A8hR7h4mXmSpXHXcatH5t52kpHYxikuKCmFlPlTmbOZoAwAAAEDTELQDyOVyp9fSytHYFqtFmX1TQ1hR7ZijDQAAAAD+wYTcQDp2NHYYZ1hWHQcAAAAA/6BHO4AMw1CcVYqxeX6fEb4hln20AQAAAMA/CNqBdKhUoxIjzIfhvKC3UTVJGwAAAADQBGEc/VqAwnJJksMwtNPiUsKwzBAXVDuGjgMAAACAfxC0A6hyNLbynIZuiytX4sjOoS2oLp4ObRZDAwAAAIAmIWgHkGfes2GRwn1M9rd7v5VEjzYAAAAANBVBO5Aqt/cyJIVzR3GJo0RPrnpSkmQN54nkAAAAANAMkKoCyDN03DCMsO4nnr9pvnn8xNlPhLASAAAAAGj+CNqBZDSPHu0DJQckSQmRCcrqkBXiagAAAACgeSNoB1BFqVNSZdAO4z7tUmepJOny3peHuBIAAAAAaP7YRzuASgrLlSD3bzPCoUf7h9wf9MzqZ1TmLPM6v+/oPklSlC0qFGUBAAAAQItC0A4ge4RNkuRUeKw5/t7m97Rm/5pan89MCN99vgEAAACguSBoB5ClcnPqCkt47E/tMBySpEt6XaJRXUZ5PZcYmagBbQeEoiwAAAAAaFEI2gFkrjoe2jJMLsMlSeqR3ENndTwrxNUAAAAAQMvEYmhBEDarjlcmfvbKBgAAAIDAIXEFkmd7LyM8grZL7h7tcF4BHQAAAACaO4J2IFUbOh4O4dYzdJwebQAAAAAIHOZoB0G4DB03KnvYwyH0AwAAAK2V0+lURUVFqMvAMSIiImSz2fzyXgTtQPLq0Q49T492OKyADgAAALQ2hmEoJydHR44cCXUpqEVycrIyMjKanJkI2gHk+dYYYbK9l2eONkPHAQAAgODzhOy0tDTFxsaGRUaAm2EYKi4uVl5eniSpffv2TXo/gnYAWVzuYCvDCI/h2qw6DgAAAISE0+k0Q3Zqamqoy4EPMTExkqS8vDylpaU1aRg5iSuAIorc8y5sFktYjB1n1XEAAAAgNDxzsmNjY0NcCeri+f40dQ49QTuAnBHu34DYFBY5mznaAAAAQIjxb/Hw5q/vD0E7oNxjtfe5XGHxfyjPquMMHQcAAACAwCFxBZDFqHYcujJMZo92WFQDAAAAAC0TQTuAjGP+DDWjspJw6F0HAAAAgJaKoB0EhqRwyLbm0HG+7QAAAADq6ZprrpHFYjG/UlNTNWbMGK1duzbUpR3XokWLNGjQIEVFRalnz5564403gnJdEleQhGq49vJ9y3Xrl7fqli9u0S+Hf5HEHG0AAAAADTNmzBjt27dP+/bt05dffim73a4JEyaEuqw6bd++XePHj9eIESO0Zs0a3XHHHbrhhhv02WefBfzaJK5Aqj5HO4A52zAMbT68WatzV9f4evr7p7Vo9yJ9s+cb5ZflS5JSY9i3DwAAAAg1wzBUXO4I+pdnpGtDREVFKSMjQxkZGTr55JN17733ateuXdq/f78kadeuXZo4caKSk5OVkpKiCy+8UDt27DBfv2rVKp133nlq27atkpKSNGzYMP3www9e17BYLHrppZc0YcIExcbGqk+fPlq2bJm2bNmi4cOHKy4uTmeccYa2bt1ar5pffPFFdevWTU899ZT69OmjqVOn6ve//72eeeaZBn/+hrIH/Arw+xxtwzA095e52l24W5K0ZPcS7SzcWedrbhp4kzondFa72HYakj7EzxUBAAAAaKiSCqf63h/43tVjbXg4W7GRjY+CRUVFeuutt9SzZ0+lpqaqoqJC2dnZysrK0tKlS2W32/Xoo4+aw8sjIyNVWFioyZMn6/nnn5dhGHrqqac0btw4bd68WQkJCeZ7P/LII3r66af19NNPa9q0abriiivUvXt3TZ8+XZ07d9Z1112nqVOn6pNPPjluncuWLdOoUaO8zmVnZ+uOO+5o9GevL4J2ELjnaDe+S3vFvhW6Z8k9OlpxVJJU5iyrtW3XxK41znVJ7KKbB96sSFtko2sAAAAA0HotWLBA8fHxkqSjR4+qffv2WrBggaxWq+bOnSuXy6VXX33VzD2vv/66kpOTtWjRIo0ePVrnnnuu1/u9/PLLSk5O1uLFi72GoF977bWaOHGiJGnatGnKysrSfffdp+zsbEnS7bffrmuvvbZeNefk5Cg9Pd3rXHp6ugoKClRSUqKYmJjG/WXUA0E7kKoNyWjKyPGlu5fqUOkhn89d3/96SVKMPUYTT5yoNtFtmnAlAAAAAMESE2HThoezQ3LdhhoxYoRmz54tSTp8+LBmzZqlsWPHauXKlfrpp5+0ZcsWr55pSSotLTWHeefm5mrGjBlatGiR8vLy5HQ6VVxcrJ07vUfmDhw40Dz2hOQBAwZ4nSstLVVBQYESExMb/DmChaAdBE1ddbzCVSFJurz35bqm3zWSJJvFprTYNLbqAgAAAJopi8XSpCHcwRQXF6eePXuaj1999VUlJSXplVdeUVFRkQYPHqw5c+bUeF27du0kSZMnT9bBgwf13HPPqUuXLoqKilJWVpbKy8u92kdERJjHnqzj65zL5TpuzRkZGcrNzfU6l5ubq8TExID2ZksE7SAx/BK020S3UYf4Dn6qCQAAAAAax2KxyGq1qqSkRIMGDdI777yjtLS0WnuZv/32W82aNUvjxo2T5F487cCBAwGtMSsrSwsXLvQ69/nnnysrKyug15VYdTygSsqd5nGb2MbPj/YE7QhrxHFaAgAAAID/lZWVKScnRzk5Odq4caNuvfVWFRUV6fzzz9ekSZPUtm1bXXjhhVq6dKm2b9+uRYsW6bbbbtPu3e4FnHv16qU333xTGzdu1IoVKzRp0qSA9yr/4Q9/0LZt23TPPffol19+0axZs/Tvf/9bd955Z0CvK9GjHVCuyjnahqS/XDSgzrZLdi/RZzt8rzj4Q6572XuCNgAAAIBQ+PTTT9W+fXtJUkJCgnr37q358+dr+PDhkqQlS5Zo2rRpuvjii1VYWKiOHTtq5MiRZg/3a6+9pptuukmDBg1SZmamHnvsMd19990Brblbt276+OOPdeedd+q5555Tp06d9Oqrr5oLqwWSxWjMJmohVlBQoKSkJOXn54f1BPhVjy5X+6IKfWt16dLHhtXZdtT8Ucotzq2zzWNnPabze5zvzxIBAAAABEFpaam2b9+ubt26KTo6OtTloBZ1fZ8akkPp0Q4TRRVFkqRr+1+r5KjkGs8nRyUru2vwVyQEAAAAADQMQTtMePbGvqL3FcqIywhxNQAAAAAQ3vr166fffvvN53MvvfSSJk2aFOSKqhC0A6meg/J3F+6Ww+WQJEXaGr9oGgAAAAC0FgsXLlRFRYXP5zx7cIcKQTugKhdDO87WXnM2Vu03FxcRF8iCAAAAAKBF6NKlS6hLqBVBO4AM80+nPt72sYxaurg3HtooSRreabiibFFBqg4AAAAAEAgE7QCyVObqcutu3bv0b8dtPyyz7pXJAQAAAADhj6AdBC655w0MbDdQcXbfQ8PbRLfRyM4jg1kWAAAAACAACNoBVDV03H30UNZD6tmmZ+gKAgAAAAAEnDXUBbRonqRdOYbcbuX3GgAAAADQ0hG0A6py1XERtAEAAACgtSBoB4EhlySCNgAAAIDm55prrpHFYjG/UlNTNWbMGK1duzbUpdVp3759uuKKK3TCCSfIarXqjjvuCNq1CdpBYFQOHWfrLgAAAADN0ZgxY7Rv3z7t27dPX375pex2uyZMmBDqsupUVlamdu3aacaMGTrppJOCem2CdiAds212m+g2oakDAAAAQNgxDEPFFcVB/zIM4/jFHSMqKkoZGRnKyMjQySefrHvvvVe7du3S/v37JUm7du3SxIkTlZycrJSUFF144YXasWOH+fpVq1bpvPPOU9u2bZWUlKRhw4bphx9+8LqGxWLRSy+9pAkTJig2NlZ9+vTRsmXLtGXLFg0fPlxxcXE644wztHXr1nrV3LVrVz333HO6+uqrlZSU1ODP3BSMZQ4CQ9LvT/h9qMsAAAAAEEZKHCUaOndo0K+74ooVio2IbfTri4qK9NZbb6lnz55KTU1VRUWFsrOzlZWVpaVLl8put+vRRx81h5dHRkaqsLBQkydP1vPPPy/DMPTUU09p3Lhx2rx5sxISEsz3fuSRR/T000/r6aef1rRp03TFFVeoe/fumj59ujp37qzrrrtOU6dO1SeffOKPv4qAIWgHkMNZtcHX0fKjIa0FAAAAABprwYIFio+PlyQdPXpU7du314IFC2S1WjV37ly5XC69+uqrslgskqTXX39dycnJWrRokUaPHq1zzz3X6/1efvllJScna/HixV5D0K+99lpNnDhRkjRt2jRlZWXpvvvuU3Z2tiTp9ttv17XXXhuMj9wkBO0AchlVq46flBbcOQEAAAAAwluMPUYrrlgRkus21IgRIzR79mxJ0uHDhzVr1iyNHTtWK1eu1E8//aQtW7Z49UxLUmlpqTnMOzc3VzNmzNCiRYuUl5cnp9Op4uJi7dy50+s1AwcONI/T09MlSQMGDPA6V1paqoKCAiUmJjb4cwQLQTuALNWOI22RIasDAAAAQPixWCxNGsIdTHFxcerZs6f5+NVXX1VSUpJeeeUVFRUVafDgwZozZ06N17Vr106SNHnyZB08eFDPPfecunTpoqioKGVlZam8vNyrfUREhHns6R33dc7lcvnvwwUAQTugDPN/WXEcAAAAQEthsVhktVpVUlKiQYMG6Z133lFaWlqtvczffvutZs2apXHjxklyL5524MCBYJYcVKw6HkCG4fQc6cQ2J4a0FgAAAABorLKyMuXk5CgnJ0cbN27UrbfeqqKiIp1//vmaNGmS2rZtqwsvvFBLly7V9u3btWjRIt12223avXu3JKlXr1568803tXHjRq1YsUKTJk1STEzDh7A31Jo1a7RmzRoVFRVp//79WrNmjTZs2BDw69KjHRQWnZhC0AYAAADQPH366adq3769JCkhIUG9e/fW/PnzNXz4cEnSkiVLNG3aNF188cUqLCxUx44dNXLkSLOH+7XXXtNNN92kQYMGKTMzU4899pjuvvvugNd9yimnmMerV6/W3Llz1aVLF6+txwLBYjRmE7UQKygoUFJSkvLz88N6Avw3f/5aXZ12fRy9QTc/eHOoywEAAAAQIqWlpdq+fbu6deum6OjoUJeDWtT1fWpIDmXoeBBYjt8EAAAAANBCELSDwCBpAwAAAIBf9evXT/Hx8T6/fK2AHkzM0Q6kykH5Fvq0AQAAAMCvFi5cqIqKCp/PefbgDhWCdkAZ1f4XAAAAAOAvXbp0CXUJtWLoOAAAAAAAfkTQDihz7DgAAAAAoJVg6HgArbEVaH9hsjYn5Ia6FAAAAABAkNCjHUBbjC3aW2HoaAR/zQAAAADQWpAAAyjakiJJahORGeJKAAAAAADBQtAOoERrZ0lS++h+Ia4EAAAAABAsBO0AsitWkmS1sBoaAAAAgObpmmuukcViMb9SU1M1ZswYrV27NtSl1em9997Teeedp3bt2ikxMVFZWVn67LPPgnJtgnZAsYM2AAAAgOZvzJgx2rdvn/bt26cvv/xSdrtdEyZMCHVZdVqyZInOO+88LVy4UKtXr9aIESN0/vnn68cffwz4tQnaAWTGbDq0AQAAABzDMAxVlDmD/mUYDe8QjIqKUkZGhjIyMnTyySfr3nvv1a5du7R//35J0q5duzRx4kQlJycrJSVFF154oXbs2GG+ftWqVTrvvPPUtm1bJSUladiwYfrhhx+8rmGxWPTSSy9pwoQJio2NVZ8+fbRs2TJt2bJFw4cPV1xcnM444wxt3bq1XjU/++yzuueee3TqqaeqV69eeuyxx9SrVy999NFHDf78DcX2XoFkbqNN0gYAAADgzVHu0su3Lw76dW96bpgiomyNfn1RUZHeeust9ezZU6mpqaqoqFB2draysrK0dOlS2e12Pfroo+bw8sjISBUWFmry5Ml6/vnnZRiGnnrqKY0bN06bN29WQkKC+d6PPPKInn76aT399NOaNm2arrjiCnXv3l3Tp09X586ddd1112nq1Kn65JNPGly3y+VSYWGhUlJSGv3Z64ugHQzkbAAAAADN2IIFCxQfHy9JOnr0qNq3b68FCxbIarVq7ty5crlcevXVV2WpXJ/q9ddfV3JyshYtWqTRo0fr3HPP9Xq/l19+WcnJyVq8eLHXEPRrr71WEydOlCRNmzZNWVlZuu+++5SdnS1Juv3223Xttdc26jP8/e9/V1FRkfn+gUTQDgJyNgAAAIBj2SOtuum5YSG5bkONGDFCs2fPliQdPnxYs2bN0tixY7Vy5Ur99NNP2rJli1fPtCSVlpaaw7xzc3M1Y8YMLVq0SHl5eXI6nSouLtbOnTu9XjNw4EDzOD09XZI0YMAAr3OlpaUqKChQYmJiveufO3euHnroIX344YdKS0tr2IdvBIJ2MJC0AQAAABzDYrE0aQh3MMXFxalnz57m41dffVVJSUl65ZVXVFRUpMGDB2vOnDk1XteuXTtJ0uTJk3Xw4EE999xz6tKli6KiopSVlaXy8nKv9hEREeaxp3fc1zmXy1Xv2ufNm6cbbrhB8+fP16hRo+r9uqYgaAeQwRxtAAAAAC2QxWKR1WpVSUmJBg0apHfeeUdpaWm19jJ/++23mjVrlsaNGyfJvXjagQMHAl7n22+/reuuu07z5s3T+PHjA349D1YdDyh30mYbbQAAAADNWVlZmXJycpSTk6ONGzfq1ltvVVFRkc4//3xNmjRJbdu21YUXXqilS5dq+/btWrRokW677Tbt3r1bktSrVy+9+eab2rhxo1asWKFJkyYpJiYmoDXPnTtXV199tZ566ikNHTrUrD8/Pz+g15UI2gAAAACA4/j000/Vvn17tW/fXkOHDtWqVas0f/58DR8+XLGxsVqyZIk6d+6siy++WH369NH111+v0tJSs4f7tdde0+HDhzVo0CBdddVVuu222wI+V/rll1+Ww+HQlClTzNrbt2+v22+/PaDXlSSL0ZhN1EKsoKBASUlJys/Pb9AE+GB7YsZSJRyokP3UVN18/UmhLgcAAABAiJSWlmr79u3q1q2boqOjQ10OalHX96khOZQe7SBg5DgAAAAAtB4E7QAyhwqQtAEAAADAr/r166f4+HifX75WQA8mVh0PoIJ2Efq5qFgjU6NCXQoAAAAAtCgLFy5URUWFz+c8e3CHCkE7gAraRuq7fQ5lt2MOBgAAAAD4U5cuXUJdQq0YOh5ABtt7AQAAAECrQ9AOIM967uRsAAAAAGg9Qhq0Z86cqa5duyo6OlpDhw7VypUrQ1mO35kbp9GlDQAAAACtRsiC9jvvvKO77rpLDzzwgH744QeddNJJys7OVl5eXqhK8rusHqm6ZFAn9WgbF+pSAAAAAABBYjEMs981qIYOHapTTz1VL7zwgiTJ5XIpMzNTt956q+69916vtmVlZSorKzMfFxQUKDMzs14bhQMAAABAqJWWlmr79u3q1q2boqNZLDlc1fV9KigoUFJSUr1yaEh6tMvLy7V69WqNGjWqqhCrVaNGjdKyZctqtH/88ceVlJRkfmVmZgazXAAAAAAA6i0kQfvAgQNyOp019jZLT09XTk5OjfbTp09Xfn6++bVr165glQoAAAAArdo111wji8VifqWmpmrMmDFau3ZtqEur0zfffKMzzzxTqampiomJUe/evfXMM88E5drNYh/tqKgoRUVFhboMAAAAAGiVxowZo9dff12SlJOToxkzZmjChAnauXNniCurXVxcnKZOnaqBAwcqLi5O33zzjW6++WbFxcXppptuCui1Q9Kj3bZtW9lsNuXm5nqdz83NVUZGRihKAgAAAICgMgxDrnJn0L8as0xXVFSUMjIylJGRoZNPPln33nuvdu3apf3790uSdu3apYkTJyo5OVkpKSm68MILtWPHDvP1q1at0nnnnae2bdsqKSlJw4YN0w8//OB1DYvFopdeekkTJkxQbGys+vTpo2XLlmnLli0aPny44uLidMYZZ2jr1q31qvmUU07R5Zdfrn79+qlr16668sorlZ2draVLlzb48zdUSHq0IyMjNXjwYH355Ze66KKLJLkXQ/vyyy81derUUJQEAAAAAEFlVLi09/7vgn7dDg+fIUukrdGvLyoq0ltvvaWePXsqNTVVFRUVys7OVlZWlpYuXSq73a5HH33UHF4eGRmpwsJCTZ48Wc8//7wMw9BTTz2lcePGafPmzUpISDDf+5FHHtHTTz+tp59+WtOmTdMVV1yh7t27a/r06ercubOuu+46TZ06VZ988kmD6/7xxx/13Xff6dFHH230Z6+vkA0dv+uuuzR58mQNGTJEp512mp599lkdPXpU1157bahKAgAAAAD4sGDBAsXHx0uSjh49qvbt22vBggWyWq2aO3euXC6XXn31VVksFknS66+/ruTkZC1atEijR4/Wueee6/V+L7/8spKTk7V48WJNmDDBPH/ttddq4sSJkqRp06YpKytL9913n7KzsyVJt99+e4MzY6dOnbR//345HA49+OCDuuGGGxr991BfIQval156qfbv36/7779fOTk5Ovnkk/Xpp5/WWCANAAAAAFoiS4RVHR4+IyTXbagRI0Zo9uzZkqTDhw9r1qxZGjt2rFauXKmffvpJW7Zs8eqZltxbZXmGeefm5mrGjBlatGiR8vLy5HQ6VVxcXGOO98CBA81jTzYcMGCA17nS0lIVFBTUe6vnpUuXqqioSMuXL9e9996rnj176vLLL2/w30FDhHQxtKlTpzJUHAAAAECrZLFYmjSEO5ji4uLUs2dP8/Grr76qpKQkvfLKKyoqKtLgwYM1Z86cGq9r166dJGny5Mk6ePCgnnvuOXXp0kVRUVHKyspSeXm5V/uIiAjz2NM77uucy+Wqd+3dunWT5A7subm5evDBB1t20AYAAAAAND8Wi0VWq1UlJSUaNGiQ3nnnHaWlpdXay/ztt99q1qxZGjdunCT34mkHDhwIZsmS3AG9rKws4NcJyarjAAAAAIDmo6ysTDk5OcrJydHGjRt16623qqioSOeff74mTZqktm3b6sILL9TSpUu1fft2LVq0SLfddpt2794tSerVq5fefPNNbdy4UStWrNCkSZMUExMT0Jpnzpypjz76SJs3b9bmzZv12muv6e9//7uuvPLKgF5XokcbAAAAAHAcn376qdq3by9JSkhIUO/evTV//nwNHz5ckrRkyRJNmzZNF198sQoLC9WxY0eNHDnS7OF+7bXXdNNNN2nQoEHKzMzUY489prvvvjugNbtcLk2fPl3bt2+X3W5Xjx499Ne//lU333xzQK8rSRajMZuohVhBQYGSkpKUn59f7wnwAAAAABAqpaWl2r59u7p166bo6OhQl4Na1PV9akgOZeg4AAAAAAB+RNAGAAAAADQ7/fr1U3x8vM8vXyugBxNztAEAAAAAzc7ChQtVUVHh8znPHtyhQtAGAAAAADQ7Xbp0CXUJtWLoOAAAAAAESTNci7pV8df3h6ANAAAAAAEWEREhSSouLg5xJaiL5/vj+X41FkPHAQAAACDAbDabkpOTlZeXJ0mKjY2VxWIJcVXwMAxDxcXFysvLU3Jysmw2W5Pej6ANAAAAAEGQkZEhSWbYRvhJTk42v09NQdAGAAAAgCCwWCxq37690tLSal0tG6ETERHR5J5sD4I2AAAAAASRzWbzW6BDeGIxNAAAAAAA/IigDQAAAACAHxG0AQAAAADwo2Y5R9uziXhBQUGIKwEAAAAAtAae/OnJo3VplkG7sLBQkpSZmRniSgAAAAAArUlhYaGSkpLqbGMx6hPHw4zL5dLevXuVkJAQ1pu8FxQUKDMzU7t27VJiYmKoywFq4B5Fc8B9inDHPYpwxz2KcNdc7lHDMFRYWKgOHTrIaq17Fnaz7NG2Wq3q1KlTqMuot8TExLC+YQDuUTQH3KcId9yjCHfcowh3zeEePV5PtgeLoQEAAAAA4EcEbQAAAAAA/IigHUBRUVF64IEHFBUVFepSAJ+4R9EccJ8i3HGPItxxjyLctcR7tFkuhgYAAAAAQLiiRxsAAAAAAD8iaAMAAAAA4EcEbQAAAAAA/IigDQAAAACAHxG0A2jmzJnq2rWroqOjNXToUK1cuTLUJaEFePzxx3XqqacqISFBaWlpuuiii7Rp0yavNqWlpZoyZYpSU1MVHx+vSy65RLm5uV5tdu7cqfHjxys2NlZpaWn64x//KIfD4dVm0aJFGjRokKKiotSzZ0+98cYbNerhPkddnnjiCVksFt1xxx3mOe5PhIM9e/boyiuvVGpqqmJiYjRgwAB9//335vOGYej+++9X+/btFRMTo1GjRmnz5s1e73Ho0CFNmjRJiYmJSk5O1vXXX6+ioiKvNmvXrtXZZ5+t6OhoZWZm6sknn6xRy/z589W7d29FR0drwIABWrhwYWA+NJoNp9Op++67T926dVNMTIx69OihRx55RNXXMOYeRTAtWbJE559/vjp06CCLxaIPPvjA6/lwuh/rU0tQGAiIefPmGZGRkcb/+3//z1i/fr1x4403GsnJyUZubm6oS0Mzl52dbbz++uvGunXrjDVr1hjjxo0zOnfubBQVFZlt/vCHPxiZmZnGl19+aXz//ffG6aefbpxxxhnm8w6Hw+jfv78xatQo48cffzQWLlxotG3b1pg+fbrZZtu2bUZsbKxx1113GRs2bDCef/55w2azGZ9++qnZhvscdVm5cqXRtWtXY+DAgcbtt99unuf+RKgdOnTI6NKli3HNNdcYK1asMLZt22Z89tlnxpYtW8w2TzzxhJGUlGR88MEHxk8//WRccMEFRrdu3YySkhKzzZgxY4yTTjrJWL58ubF06VKjZ8+exuWXX24+n5+fb6SnpxuTJk0y1q1bZ7z99ttGTEyM8dJLL5ltvv32W8NmsxlPPvmksWHDBmPGjBlGRESE8fPPPwfnLwNh6S9/+YuRmppqLFiwwNi+fbsxf/58Iz4+3njuuefMNtyjCKaFCxcaf/7zn4333nvPkGS8//77Xs+H0/1Yn1qCgaAdIKeddpoxZcoU87HT6TQ6dOhgPP744yGsCi1RXl6eIclYvHixYRiGceTIESMiIsKYP3++2Wbjxo2GJGPZsmWGYbh/WFqtViMnJ8dsM3v2bCMxMdEoKyszDMMw7rnnHqNfv35e17r00kuN7Oxs8zH3OWpTWFho9OrVy/j888+NYcOGmUGb+xPhYNq0acZZZ51V6/Mul8vIyMgw/va3v5nnjhw5YkRFRRlvv/22YRiGsWHDBkOSsWrVKrPNJ598YlgsFmPPnj2GYRjGrFmzjDZt2pj3refaJ554ovl44sSJxvjx472uP3ToUOPmm29u2odEszZ+/Hjjuuuu8zp38cUXG5MmTTIMg3sUoXVs0A6n+7E+tQQLQ8cDoLy8XKtXr9aoUaPMc1arVaNGjdKyZctCWBlaovz8fElSSkqKJGn16tWqqKjwuv969+6tzp07m/ffsmXLNGDAAKWnp5ttsrOzVVBQoPXr15ttqr+Hp43nPbjPUZcpU6Zo/PjxNe4h7k+Eg//85z8aMmSI/ud//kdpaWk65ZRT9Morr5jPb9++XTk5OV73T1JSkoYOHep1nyYnJ2vIkCFmm1GjRslqtWrFihVmm3POOUeRkZFmm+zsbG3atEmHDx8229R1L6N1OuOMM/Tll1/q119/lST99NNP+uabbzR27FhJ3KMIL+F0P9anlmAhaAfAgQMH5HQ6vf6RKEnp6enKyckJUVVoiVwul+644w6deeaZ6t+/vyQpJydHkZGRSk5O9mpb/f7LycnxeX96nqurTUFBgUpKSrjPUat58+bphx9+0OOPP17jOe5PhINt27Zp9uzZ6tWrlz777DPdcsstuu222/TPf/5TUtV9Vtf9k5OTo7S0NK/n7Xa7UlJS/HIvc5+2bvfee68uu+wy9e7dWxERETrllFN0xx13aNKkSZK4RxFewul+rE8twWIP6tUA+NWUKVO0bt06ffPNN6EuBZAk7dq1S7fffrs+//xzRUdHh7ocwCeXy6UhQ4bosccekySdcsopWrdunV588UVNnjw5xNUB0r///W/NmTNHc+fOVb9+/bRmzRrdcccd6tChA/co0EzQox0Abdu2lc1mq7GKbm5urjIyMkJUFVqaqVOnasGCBfr666/VqVMn83xGRobKy8t15MgRr/bV77+MjAyf96fnubraJCYmKiYmhvscPq1evVp5eXkaNGiQ7Ha77Ha7Fi9erH/84x+y2+1KT0/n/kTItW/fXn379vU616dPH+3cuVNS1X1W1/2TkZGhvLw8r+cdDocOHTrkl3uZ+7R1++Mf/2j2ag8YMEBXXXWV7vz/27v/mCqrPw7g78uvC5fLD/khEAGBEpDoSER2FSfNmWIw/LF0anZ1lpaaTiusQHG5yqm5WLMW4NKERbj8MdeSIegEUgThIk1Cb5MfbaCGKZBCzPv5/uHX59tVRPR75aK8X9vd7nOec57nc7hn7H72nHvO2rXKTCGOURpMBtN47E8sA4WJ9mPg4OCA6OhoFBUVKWUmkwlFRUXQ6XRWjIyeBiKCVatW4cCBAyguLkZwcLDZ+ejoaNjb25uNv/r6ejQ1NSnjT6fToba21uwfXmFhIVxdXZUvnzqdzuwad+rcuQbHOfVmypQpqK2thcFgUF7jxo3DwoULlfccn2RtEydOvGdbxPPnzyMoKAgAEBwcDF9fX7Px097ejvLycrNxeu3aNZw5c0apU1xcDJPJhNjYWKXOiRMn0NPTo9QpLCxEWFgYhg0bptTpayzT0HTjxg3Y2Jh/Tbe1tYXJZALAMUqDy2Aaj/2JZcAM6NJrQ0heXp6o1WrZvXu3nDt3TpYtWybu7u5mq+gSPYq3335b3Nzc5Pjx49LS0qK8bty4odR56623JDAwUIqLi6WyslJ0Op3odDrl/J3tk15++WUxGAxy5MgR8fb27nX7pPfff1/q6upk586dvW6fxHFOD/LvVcdFOD7J+k6fPi12dnbyySefyIULFyQ3N1c0Go3k5OQodbZs2SLu7u5y6NAhOXv2rCQnJ/e6Vc2LL74o5eXlUlpaKqGhoWZb1Vy7dk18fHxk0aJF8uuvv0peXp5oNJp7tqqxs7OT7du3S11dnaSnp3PrJBK9Xi/+/v7K9l779+8XLy8vSUlJUepwjNJA6ujokOrqaqmurhYAsmPHDqmurpbGxkYRGVzjsT+xDAQm2o/Rl19+KYGBgeLg4CDjx4+XU6dOWTskegoA6PX17bffKnVu3rwpK1askGHDholGo5FZs2ZJS0uL2XUaGhokISFBnJycxMvLS959913p6ekxq3Ps2DGJiooSBwcHCQkJMbvHHRzn9CB3J9ocnzQYHD58WCIjI0WtVkt4eLhkZmaanTeZTLJhwwbx8fERtVotU6ZMkfr6erM6bW1tMn/+fNFqteLq6ipLliyRjo4Oszo1NTUSFxcnarVa/P39ZcuWLffEkp+fL88//7w4ODjIqFGj5KeffrJ8h+mJ0t7eLmvWrJHAwEBxdHSUkJAQSU1NNdv2iGOUBtKxY8d6/f6p1+tFZHCNx/7EMhBUIiID+wydiIiIiIiI6OnF32gTERERERERWRATbSIiIiIiIiILYqJNREREREREZEFMtImIiIiIiIgsiIk2ERERERERkQUx0SYiIiIiIiKyICbaRERERERERBbERJuIiIiIiIjIgphoExERDWKZmZkICAiAjY0NvvjiiwG//+LFizFz5swBv+//60mNm4iIng4qERFrB0FERPQ4LV68GHv27FGOPTw8EBMTg61bt2LMmDFWjKxv7e3t8PLywo4dOzBnzhy4ublBo9EMaAzXr1+HiMDd3X1A7/v/elLjJiKipwOfaBMR0ZAwffp0tLS0oKWlBUVFRbCzs0NiYqK1w+pTU1MTenp68Morr8DPz6/XJPuff/55rDG4ubk9kcnqkxo3ERE9HZhoExHRkKBWq+Hr6wtfX19ERUXhgw8+QHNzM65cuQIAaG5uxty5c+Hu7g4PDw8kJyejoaFBaV9RUYGpU6fCy8sLbm5umDx5MqqqqszuoVKp8M033yAxMREajQYRERE4efIkjEYj4uPj4ezsjAkTJuD3339/YLy7d+/G6NGjAQAhISFQqVRoaGjApk2bEBUVhezsbAQHB8PR0REAcOTIEcTFxcHd3R2enp5ITEw0u09DQwNUKhXy8/MxadIkODk5ISYmBufPn0dFRQXGjRsHrVaLhIQE5W8C3DsFOz4+HqtXr0ZKSgo8PDzg6+uLTZs2mcX+22+/IS4uDo6OjnjhhRdw9OhRqFQqHDx4sD8fFX755RdERUXB0dER48aNw8GDB6FSqWAwGAAAt27dwtKlSxEcHAwnJyeEhYUhIyPD7BqPEjcREZGlMNEmIqIhp7OzEzk5ORg5ciQ8PT3R09ODadOmwcXFBSUlJSgrK4NWq8X06dOVJ8YdHR3Q6/UoLS3FqVOnEBoaihkzZqCjo8Ps2ps3b8brr78Og8GA8PBwLFiwAMuXL8eHH36IyspKiAhWrVr1wBjnzZuHo0ePAgBOnz6NlpYWBAQEAACMRiN+/PFH7N+/X0k+//77b6xbtw6VlZUoKiqCjY0NZs2aBZPJZHbd9PR0pKWloaqqCnZ2dliwYAFSUlKQkZGBkpISGI1GbNy4sc/Y9uzZA2dnZ5SXl2Pr1q34+OOPUVhYCOB2Ejxz5kxoNBqUl5cjMzMTqampD/5Q/qu9vR1JSUkYPXo0qqqqsHnzZqxfv96sjslkwrPPPot9+/bh3Llz2LhxIz766CPk5+c/ctxEREQWJURERE85vV4vtra24uzsLM7OzgJA/Pz85MyZMyIisnfvXgkLCxOTyaS06e7uFicnJykoKOj1mrdu3RIXFxc5fPiwUgZA0tLSlOOTJ08KANm1a5dS9v3334ujo2O/4q6urhYAcvHiRaUsPT1d7O3t5fLly322vXLligCQ2tpaERG5ePGiAJDs7GyzWABIUVGRUvbZZ59JWFiYcqzX6yU5OVk5njx5ssTFxZndKyYmRtavXy8iIj///LPY2dlJS0uLcr6wsFAAyIEDBx7Y56+//lo8PT3l5s2bSllWVpYAkOrq6vu2W7lypcyZM+eR4yYiIrIkPtEmIqIh4aWXXoLBYIDBYMDp06cxbdo0JCQkoLGxETU1NTAajXBxcYFWq4VWq4WHhwe6urqU6deXLl3Cm2++idDQULi5ucHV1RWdnZ1oamoyu8+/F1fz8fEBAGUK+J2yrq4utLe3P3JfgoKC4O3tbVZ24cIFzJ8/HyEhIXB1dcVzzz0HAI8U3+XLl/u8/90LyPn5+Slt6uvrERAQAF9fX+X8+PHj+9mz2+3HjBmjTIm/X/udO3ciOjoa3t7e0Gq1yMzMvKevDxM3ERGRJdlZOwAiIqKB4OzsjJEjRyrH2dnZcHNzQ1ZWFjo7OxEdHY3c3Nx72t1JaPV6Pdra2pCRkYGgoCCo1WrodLp7FiOzt7dX3qtUqvuW3T2l+2H7crekpCQEBQUhKysLzzzzDEwmEyIjIx8pvgfF9u/6/W1jSXl5eXjvvffw+eefQ6fTwcXFBdu2bUN5eXmf7awdNxERDR1MtImIaEhSqVSwsbHBzZs3MXbsWPzwww8YPnw4XF1de61fVlaGr776CjNmzABwe/G0P//8cyBDvq+2tjbU19cjKysLkyZNAgCUlpZaJZawsDA0Nzfj0qVLyhPzioqKh2qfk5OD7u5uqNXqXtuXlZVhwoQJWLFihVLWnwXmiIiIBgqnjhMR0ZDQ3d2N1tZWtLa2oq6uDu+88w46OzuRlJSEhQsXwsvLC8nJySgpKcHFixdx/PhxrF69Gn/88QcAIDQ0FHv37kVdXR3Ky8uxcOFCODk5WblXtw0bNgyenp7IzMyE0WhEcXEx1q1bZ5VYpk6dihEjRkCv1+Ps2bMoKytDWloagP89Qe/LggULYDKZsGzZMtTV1aGgoADbt283ax8aGorKykoUFBTg/Pnz2LBhw0Ml80RERI8bE20iIhoSjhw5Aj8/P/j5+SE2NhYVFRXYt28f4uPjodFocOLECQQGBmL27NmIiIjA0qVL0dXVpTzh3rVrF/766y+MHTsWixYtwurVqzF8+HAr9+o2Gxsb5OXl4cyZM4iMjMTatWuxbds2q8Ria2uLgwcPorOzEzExMXjjjTeUVcf//bvr+3F1dcXhw4dhMBgQFRWF1NRUZRX0O+2XL1+O2bNnY968eYiNjUVbW5vZ020iIiJrU4mIWDsIIiIienqVlZUhLi4ORqMRI0aMeOj2ubm5WLJkCa5fvz5oZhEQERH1hb/RJiIiIos6cOAAtFotQkNDYTQasWbNGkycOLHfSfZ3332HkJAQ+Pv7o6amBuvXr8fcuXOZZBMR0RODU8eJiIisZNSoUcp2Yne/elsB/UnR0dGBlStXIjw8HIsXL0ZMTAwOHToEAPj000/v2+eEhAQAQGtrK1577TVERERg7dq1ePXVV5GZmWnNLhERET0UTh0nIiKyksbGRvT09PR6zsfHBy4uLgMc0eN39epVXL16tddzTk5O8Pf3H+CIiIiILI+JNhEREREREZEFceo4ERERERERkQUx0SYiIiIiIiKyICbaRERERERERBbERJuIiIiIiIjIgphoExEREREREVkQE20iIiIiIiIiC2KiTURERERERGRB/wGosADgJcruRwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(f'Best beamforming gain')\n",
    "plt.xlabel('Beam_framing_gain')   \n",
    "for beam_id in range(options['num_NNs']):\n",
    "    gain_record=np.array(env_list[beam_id].gain_history[1:])\n",
    "    np.save(f'beam_{beam_id}_gain_records',gain_record)\n",
    "    plt.plot(gain_record, label=f'Beam_{beam_id}')\n",
    "    plt.plot([])\n",
    "plt.plot([231.17094]*100000,linestyle='dashed',color='black')\n",
    "plt.legend(loc=\"lower right\")  \n",
    "plt.show()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
