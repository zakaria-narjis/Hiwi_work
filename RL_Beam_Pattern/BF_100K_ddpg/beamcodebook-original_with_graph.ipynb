{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.22.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:11:41.814118Z",
     "iopub.status.busy": "2023-11-06T19:11:41.813672Z",
     "iopub.status.idle": "2023-11-06T19:11:41.820061Z",
     "shell.execute_reply": "2023-11-06T19:11:41.819166Z",
     "shell.execute_reply.started": "2023-11-06T19:11:41.814078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Codebook_Learning_RL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/workspace/Codebook_Learning_RL')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:11:41.822153Z",
     "iopub.status.busy": "2023-11-06T19:11:41.821414Z",
     "iopub.status.idle": "2023-11-06T19:11:41.829714Z",
     "shell.execute_reply": "2023-11-06T19:11:41.828927Z",
     "shell.execute_reply.started": "2023-11-06T19:11:41.822104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1699731847.008369] [36ccdc8c3f8d:25312:f]        vfs_fuse.c:281  UCX  ERROR inotify_add_watch(/tmp) failed: No space left on device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from DataPrep import dataPrep\n",
    "from env_ddpg import envCB\n",
    "from clustering import KMeans_only\n",
    "from function_lib import bf_gain_cal, corr_mining\n",
    "from DDPG_classes import Actor, Critic, OUNoise, init_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1321,\n",
       " 27148,\n",
       " 26104,\n",
       " 30786,\n",
       " 31404,\n",
       " 31885,\n",
       " 31885,\n",
       " 15171,\n",
       " 31573,\n",
       " 31573,\n",
       " 31883,\n",
       " 31883,\n",
       " 31275,\n",
       " 31883,\n",
       " 1,\n",
       " 2035]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "\n",
    "def get_gpu_memory():\n",
    "    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    return memory_free_values\n",
    "\n",
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:11:41.832115Z",
     "iopub.status.busy": "2023-11-06T19:11:41.831852Z",
     "iopub.status.idle": "2023-11-06T19:11:41.839296Z",
     "shell.execute_reply": "2023-11-06T19:11:41.838469Z",
     "shell.execute_reply.started": "2023-11-06T19:11:41.832092Z"
    }
   },
   "outputs": [],
   "source": [
    "class envCB_(envCB):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.gain_history = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:11:41.840735Z",
     "iopub.status.busy": "2023-11-06T19:11:41.840417Z",
     "iopub.status.idle": "2023-11-06T19:11:41.849278Z",
     "shell.execute_reply": "2023-11-06T19:11:41.848337Z",
     "shell.execute_reply.started": "2023-11-06T19:11:41.840704Z"
    }
   },
   "outputs": [],
   "source": [
    "# new_gain = torch.Tensor.cpu(CB_Env.achievement).detach().numpy().reshape((1, 1))\n",
    "# max_previous_gain = max(CB_Env.gain_history)\n",
    "#         if new_gain > max_previous_gain:\n",
    "#             CB_Env.gain_history.append(float(new_gain))                   \n",
    "#         else:\n",
    "#             CB_Env.gain_history.append(float(max_previous_gain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:11:41.850556Z",
     "iopub.status.busy": "2023-11-06T19:11:41.850276Z",
     "iopub.status.idle": "2023-11-06T19:11:43.328947Z",
     "shell.execute_reply": "2023-11-06T19:11:43.328078Z",
     "shell.execute_reply.started": "2023-11-06T19:11:41.850533Z"
    }
   },
   "outputs": [],
   "source": [
    "options = {\n",
    "        'gpu_idx': 12,\n",
    "        'num_ant': 32,\n",
    "        'num_bits': 4,\n",
    "        'num_NNs': 4,  # codebook size\n",
    "        'ch_sample_ratio': 0.5,\n",
    "        'num_loop': 1000,  # outer loop\n",
    "        'target_update': 3,\n",
    "        'path': './grid1101-1400.mat',\n",
    "        'clustering_mode': 'random',\n",
    "    }\n",
    "\n",
    "train_opt = {\n",
    "        'state': 0,\n",
    "        'best_state': 0,\n",
    "        'num_iter': 100,  # inner loop\n",
    "        'tau': 1e-2,\n",
    "        'overall_iter': 1,\n",
    "        'replay_memory': [],\n",
    "        'replay_memory_size': 8192,\n",
    "        'minibatch_size': 1024,\n",
    "        'gamma': 0\n",
    "    }\n",
    "if not os.path.exists('beams/'):\n",
    "    os.mkdir('beams/')\n",
    "\n",
    "ch = dataPrep(options['path'])\n",
    "ch = np.concatenate((ch[:, :options['num_ant']],\n",
    "                     ch[:, int(ch.shape[1] / 2):int(ch.shape[1] / 2) + options['num_ant']]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:11:43.330618Z",
     "iopub.status.busy": "2023-11-06T19:11:43.330331Z",
     "iopub.status.idle": "2023-11-06T19:11:43.359268Z",
     "shell.execute_reply": "2023-11-06T19:11:43.358251Z",
     "shell.execute_reply.started": "2023-11-06T19:11:43.330594Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def train(actor_net,\n",
    "          critic_net,\n",
    "          actor_net_t,\n",
    "          critic_net_t,\n",
    "          ounoise,\n",
    "          env,\n",
    "          options,\n",
    "          train_options,\n",
    "          beam_id):\n",
    "    CB_Env = env\n",
    "    critic_optimizer = optim.Adam(critic_net.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "    actor_optimizer = optim.Adam(actor_net.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "    critic_criterion = nn.MSELoss()\n",
    "\n",
    "    if train_options['overall_iter'] == 1:\n",
    "        state = torch.zeros((1, options['num_ant'])).float().cuda()\n",
    "        print('Initial State Activated.')\n",
    "    else:\n",
    "        state = train_options['state']\n",
    "\n",
    "    # -------------- Training -------------- #\n",
    "    replay_memory = train_options['replay_memory']\n",
    "    iteration = 0\n",
    "    num_of_iter = train_options['num_iter']\n",
    "    while iteration < num_of_iter:\n",
    "\n",
    "        # Proto-action\n",
    "        action_pred = actor_net(state)\n",
    "        reward_pred, bf_gain_pred, action_quant_pred, state_1_pred = CB_Env.get_reward(action_pred)\n",
    "        reward_pred = torch.from_numpy(reward_pred).float().cuda()\n",
    "\n",
    "        # Exploration and Quantization Processing\n",
    "        action_pred_noisy = ounoise.get_action(action_pred,\n",
    "                                               t=train_options['overall_iter'])  # torch.Size([1, action_dim])\n",
    "        mat_dist = torch.abs(action_pred_noisy.reshape(options['num_ant'], 1) - options['ph_table_rep'])\n",
    "        action_quant = options['ph_table_rep'][range(options['num_ant']), torch.argmin(mat_dist, dim=1)].reshape(1, -1)\n",
    "\n",
    "        state_1, reward, bf_gain, terminal = CB_Env.step(action_quant)\n",
    "        reward = torch.from_numpy(reward).float().cuda()\n",
    "        action = action_quant.reshape((1, -1)).float().cuda()\n",
    "        \n",
    "        new_gain = torch.Tensor.cpu(CB_Env.achievement).detach().numpy().reshape((1, 1))\n",
    "        max_previous_gain = max(CB_Env.gain_history)\n",
    "        if new_gain > max_previous_gain:\n",
    "            CB_Env.gain_history.append(float(new_gain))                   \n",
    "        else:\n",
    "            CB_Env.gain_history.append(float(max_previous_gain))\n",
    "            \n",
    "        replay_memory.append((state, action, reward, state_1, terminal))\n",
    "        replay_memory.append((state, action_quant_pred, reward_pred, state_1_pred, terminal))\n",
    "        while len(replay_memory) > train_options['replay_memory_size']:\n",
    "            replay_memory.pop(0)\n",
    "\n",
    "        # -------------- Experience Replay -------------- #\n",
    "        minibatch = random.sample(replay_memory, min(len(replay_memory), train_options['minibatch_size']))\n",
    "\n",
    "        state_batch = torch.cat(tuple(d[0] for d in minibatch))  # torch.Size([*, state_dim])\n",
    "        action_batch = torch.cat(tuple(d[1] for d in minibatch))  # torch.Size([*, action_dim])\n",
    "        reward_batch = torch.cat(tuple(d[2] for d in minibatch))  # torch.Size([*, 1])\n",
    "        state_1_batch = torch.cat(tuple(d[3] for d in minibatch))  # torch.Size([*, state_dim])\n",
    "\n",
    "        state_batch = state_batch.detach()\n",
    "        action_batch = action_batch.detach()\n",
    "        reward_batch = reward_batch.detach()\n",
    "        state_1_batch = state_1_batch.detach()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            state_batch = state_batch.cuda()\n",
    "            action_batch = action_batch.cuda()\n",
    "            reward_batch = reward_batch.cuda()\n",
    "            state_1_batch = state_1_batch.cuda()\n",
    "\n",
    "        # Loss Calculation for Critic Network\n",
    "        next_actions = actor_net_t(state_1_batch)\n",
    "        next_Q = critic_net_t(state_1_batch, next_actions)\n",
    "        Q_prime = reward_batch + train_options['gamma'] * next_Q\n",
    "        Q_pred = critic_net(state_batch, action_batch)\n",
    "        critic_loss = critic_criterion(Q_pred, Q_prime.detach())\n",
    "\n",
    "        # Update Critic Network\n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_optimizer.step()\n",
    "\n",
    "        # Loss Calculation for Actor Network\n",
    "        actor_loss = torch.mean(-critic_net(state_batch, actor_net(state_batch)))\n",
    "\n",
    "        # Update Actor Network\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "\n",
    "        # UPDATE state, epsilon, target network, etc.\n",
    "        state = state_1\n",
    "        iteration += 1\n",
    "        train_options['overall_iter'] += 1  # global counter\n",
    "\n",
    "        # Update: Target Network\n",
    "        if train_options['overall_iter'] % options['target_update'] == 0:\n",
    "            actor_params = actor_net.state_dict()\n",
    "            critic_params = critic_net.state_dict()\n",
    "            actor_t_params = actor_net_t.state_dict()\n",
    "            critic_t_params = critic_net_t.state_dict()\n",
    "\n",
    "            for name in critic_params:\n",
    "                critic_params[name] = train_options['tau'] * critic_params[name].clone() + \\\n",
    "                                      (1 - train_options['tau']) * critic_t_params[name].clone()\n",
    "\n",
    "            critic_net_t.load_state_dict(critic_params)\n",
    "\n",
    "            for name in actor_params:\n",
    "                actor_params[name] = train_options['tau'] * actor_params[name].clone() + \\\n",
    "                                     (1 - train_options['tau']) * actor_t_params[name].clone()\n",
    "\n",
    "            actor_net_t.load_state_dict(actor_params)\n",
    "\n",
    "            # actor_net_t.load_state_dict(actor_net.state_dict())\n",
    "            # critic_net_t.load_state_dict(critic_net.state_dict())\n",
    "\n",
    "    if (train_options['overall_iter']-1)%500==0:\n",
    "        print(\n",
    "            \"Beam: %d, Iter: %d, Q: %.4f, Reward pred: %d, Reward: %d, BF Gain pred: %.2f, BF Gain: %.2f, Critic Loss: %.2f, Policy Loss: %.2f\" % \\\n",
    "            (beam_id, train_options['overall_iter'],\n",
    "             np.max(torch.Tensor.cpu(Q_pred.detach()).numpy().squeeze()),\n",
    "             int(torch.Tensor.cpu(reward_pred).numpy().squeeze()),\n",
    "             int(torch.Tensor.cpu(reward).numpy().squeeze()),\n",
    "             torch.Tensor.cpu(bf_gain_pred.detach()).numpy().squeeze(),\n",
    "             torch.Tensor.cpu(bf_gain.detach()).numpy().squeeze(),\n",
    "             torch.Tensor.cpu(critic_loss.detach()).numpy().squeeze(),\n",
    "             torch.Tensor.cpu(actor_loss.detach()).numpy().squeeze()))\n",
    "\n",
    "    # Training Communication Interface\n",
    "    train_options['replay_memory'] = replay_memory  # used for the next loop\n",
    "    train_options['state'] = state  # used for the next loop\n",
    "    train_options['best_state'] = CB_Env.best_bf_vec  # used for clustering and assignment\n",
    "\n",
    "    return train_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:11:43.360904Z",
     "iopub.status.busy": "2023-11-06T19:11:43.360592Z",
     "iopub.status.idle": "2023-11-06T19:47:10.238202Z",
     "shell.execute_reply": "2023-11-06T19:47:10.237176Z",
     "shell.execute_reply.started": "2023-11-06T19:11:43.360880Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EGC bf gain:  231.17094\n",
      "EGC bf gain:  231.17094\n",
      "EGC bf gain:  231.17094\n",
      "EGC bf gain:  231.17094\n",
      "Initial State Activated.\n",
      "Initial State Activated.\n",
      "Initial State Activated.\n",
      "Initial State Activated.\n",
      "Beam: 0, Iter: 501, Q: 1.4898, Reward pred: -1, Reward: -1, BF Gain pred: 1.11, BF Gain: 1.53, Critic Loss: 0.36, Policy Loss: -2.46\n",
      "Beam: 1, Iter: 501, Q: 1.4414, Reward pred: -1, Reward: -1, BF Gain pred: 2.27, BF Gain: 0.29, Critic Loss: 0.40, Policy Loss: -1.60\n",
      "Beam: 2, Iter: 501, Q: 1.7025, Reward pred: -1, Reward: -1, BF Gain pred: 0.90, BF Gain: 1.97, Critic Loss: 0.22, Policy Loss: -1.23\n",
      "Beam: 3, Iter: 501, Q: 2.1962, Reward pred: -1, Reward: -1, BF Gain pred: 3.32, BF Gain: 0.67, Critic Loss: 0.24, Policy Loss: -2.60\n",
      "Training for 500 iteration for each Beam uses 24.474790334701538 seconds.\n",
      "Beam: 0, Iter: 1001, Q: 2.0777, Reward pred: 1, Reward: -1, BF Gain pred: 21.11, BF Gain: 1.85, Critic Loss: 0.16, Policy Loss: -2.87\n",
      "Beam: 1, Iter: 1001, Q: 1.6840, Reward pred: -1, Reward: 1, BF Gain pred: 0.61, BF Gain: 1.17, Critic Loss: 0.26, Policy Loss: -2.96\n",
      "Beam: 2, Iter: 1001, Q: 1.5164, Reward pred: -1, Reward: -1, BF Gain pred: 6.55, BF Gain: 6.05, Critic Loss: 0.13, Policy Loss: -1.90\n",
      "Beam: 3, Iter: 1001, Q: 1.6359, Reward pred: -1, Reward: 1, BF Gain pred: 14.28, BF Gain: 22.98, Critic Loss: 0.13, Policy Loss: -2.53\n",
      "Training for 500 iteration for each Beam uses 21.272250413894653 seconds.\n",
      "Beam: 0, Iter: 1501, Q: 1.5166, Reward pred: -1, Reward: -1, BF Gain pred: 10.13, BF Gain: 5.85, Critic Loss: 0.11, Policy Loss: -2.80\n",
      "Beam: 1, Iter: 1501, Q: 1.7661, Reward pred: 1, Reward: 1, BF Gain pred: 2.70, BF Gain: 1.05, Critic Loss: 0.17, Policy Loss: -2.60\n",
      "Beam: 2, Iter: 1501, Q: 2.9194, Reward pred: -1, Reward: 1, BF Gain pred: 2.33, BF Gain: 11.06, Critic Loss: 0.15, Policy Loss: -1.95\n",
      "Beam: 3, Iter: 1501, Q: 2.0404, Reward pred: -1, Reward: 1, BF Gain pred: 11.52, BF Gain: 24.09, Critic Loss: 0.06, Policy Loss: -2.45\n",
      "Training for 500 iteration for each Beam uses 21.856759786605835 seconds.\n",
      "Beam: 0, Iter: 2001, Q: 2.8901, Reward pred: 1, Reward: -1, BF Gain pred: 46.40, BF Gain: 8.25, Critic Loss: 0.08, Policy Loss: -2.54\n",
      "Beam: 1, Iter: 2001, Q: 1.7993, Reward pred: -1, Reward: 1, BF Gain pred: 1.78, BF Gain: 14.45, Critic Loss: 0.16, Policy Loss: -2.55\n",
      "Beam: 2, Iter: 2001, Q: 1.5848, Reward pred: 1, Reward: -1, BF Gain pred: 3.76, BF Gain: 1.16, Critic Loss: 0.08, Policy Loss: -1.74\n",
      "Beam: 3, Iter: 2001, Q: 1.4468, Reward pred: 1, Reward: 1, BF Gain pred: 65.10, BF Gain: 48.28, Critic Loss: 0.08, Policy Loss: -1.91\n",
      "Training for 500 iteration for each Beam uses 21.945306301116943 seconds.\n",
      "Beam: 0, Iter: 2501, Q: 1.5858, Reward pred: 1, Reward: -1, BF Gain pred: 19.64, BF Gain: 8.66, Critic Loss: 0.09, Policy Loss: -2.73\n",
      "Beam: 1, Iter: 2501, Q: 2.9480, Reward pred: -1, Reward: -1, BF Gain pred: 1.46, BF Gain: 3.10, Critic Loss: 0.12, Policy Loss: -2.79\n",
      "Beam: 2, Iter: 2501, Q: 1.7096, Reward pred: -1, Reward: -1, BF Gain pred: 4.69, BF Gain: 7.77, Critic Loss: 0.10, Policy Loss: -1.59\n",
      "Beam: 3, Iter: 2501, Q: 1.3713, Reward pred: 1, Reward: 1, BF Gain pred: 82.28, BF Gain: 57.72, Critic Loss: 0.06, Policy Loss: -1.81\n",
      "Training for 500 iteration for each Beam uses 22.048053741455078 seconds.\n",
      "Beam: 0, Iter: 3001, Q: 1.4115, Reward pred: 1, Reward: -1, BF Gain pred: 39.29, BF Gain: 27.66, Critic Loss: 0.05, Policy Loss: -2.59\n",
      "Beam: 1, Iter: 3001, Q: 2.3266, Reward pred: -1, Reward: -1, BF Gain pred: 4.28, BF Gain: 1.85, Critic Loss: 0.11, Policy Loss: -2.63\n",
      "Beam: 2, Iter: 3001, Q: 1.4343, Reward pred: 1, Reward: -1, BF Gain pred: 16.79, BF Gain: 2.35, Critic Loss: 0.09, Policy Loss: -1.58\n",
      "Beam: 3, Iter: 3001, Q: 1.2835, Reward pred: -1, Reward: -1, BF Gain pred: 75.25, BF Gain: 52.18, Critic Loss: 0.07, Policy Loss: -1.62\n",
      "Training for 500 iteration for each Beam uses 21.868003368377686 seconds.\n",
      "Beam: 0, Iter: 3501, Q: 1.6270, Reward pred: -1, Reward: -1, BF Gain pred: 50.36, BF Gain: 54.82, Critic Loss: 0.08, Policy Loss: -2.44\n",
      "Beam: 1, Iter: 3501, Q: 2.0417, Reward pred: 1, Reward: 1, BF Gain pred: 7.54, BF Gain: 5.33, Critic Loss: 0.10, Policy Loss: -2.43\n",
      "Beam: 2, Iter: 3501, Q: 1.4257, Reward pred: -1, Reward: -1, BF Gain pred: 2.27, BF Gain: 2.62, Critic Loss: 0.09, Policy Loss: -1.51\n",
      "Beam: 3, Iter: 3501, Q: 1.3032, Reward pred: 1, Reward: -1, BF Gain pred: 87.26, BF Gain: 47.08, Critic Loss: 0.11, Policy Loss: -1.66\n",
      "Training for 500 iteration for each Beam uses 21.875313758850098 seconds.\n",
      "Beam: 0, Iter: 4001, Q: 1.5306, Reward pred: -1, Reward: -1, BF Gain pred: 60.89, BF Gain: 28.91, Critic Loss: 0.09, Policy Loss: -2.35\n",
      "Beam: 1, Iter: 4001, Q: 1.5761, Reward pred: -1, Reward: -1, BF Gain pred: 7.33, BF Gain: 11.16, Critic Loss: 0.09, Policy Loss: -2.56\n",
      "Beam: 2, Iter: 4001, Q: 1.6165, Reward pred: 1, Reward: 1, BF Gain pred: 8.48, BF Gain: 7.04, Critic Loss: 0.07, Policy Loss: -1.48\n",
      "Beam: 3, Iter: 4001, Q: 1.7804, Reward pred: -1, Reward: -1, BF Gain pred: 100.81, BF Gain: 15.47, Critic Loss: 0.14, Policy Loss: -1.72\n",
      "Training for 500 iteration for each Beam uses 22.21854853630066 seconds.\n",
      "Beam: 0, Iter: 4501, Q: 1.6264, Reward pred: -1, Reward: -1, BF Gain pred: 30.71, BF Gain: 4.89, Critic Loss: 0.15, Policy Loss: -1.95\n",
      "Beam: 1, Iter: 4501, Q: 2.3418, Reward pred: 1, Reward: 1, BF Gain pred: 6.92, BF Gain: 5.83, Critic Loss: 0.09, Policy Loss: -2.30\n",
      "Beam: 2, Iter: 4501, Q: 1.3928, Reward pred: 1, Reward: -1, BF Gain pred: 17.97, BF Gain: 10.44, Critic Loss: 0.07, Policy Loss: -1.51\n",
      "Beam: 3, Iter: 4501, Q: 1.4375, Reward pred: 1, Reward: -1, BF Gain pred: 39.40, BF Gain: 14.38, Critic Loss: 0.08, Policy Loss: -1.64\n",
      "Training for 500 iteration for each Beam uses 23.172138452529907 seconds.\n",
      "Beam: 0, Iter: 5001, Q: 1.7436, Reward pred: -1, Reward: -1, BF Gain pred: 82.79, BF Gain: 9.89, Critic Loss: 0.08, Policy Loss: -1.96\n",
      "Beam: 1, Iter: 5001, Q: 3.2307, Reward pred: -1, Reward: 1, BF Gain pred: 0.60, BF Gain: 6.56, Critic Loss: 0.10, Policy Loss: -2.01\n",
      "Beam: 2, Iter: 5001, Q: 1.4334, Reward pred: 1, Reward: 1, BF Gain pred: 6.73, BF Gain: 15.16, Critic Loss: 0.09, Policy Loss: -1.44\n",
      "Beam: 3, Iter: 5001, Q: 1.7627, Reward pred: -1, Reward: -1, BF Gain pred: 146.24, BF Gain: 51.37, Critic Loss: 0.10, Policy Loss: -1.68\n",
      "Training for 500 iteration for each Beam uses 22.734123468399048 seconds.\n",
      "Beam: 0, Iter: 5501, Q: 1.6858, Reward pred: -1, Reward: -1, BF Gain pred: 107.78, BF Gain: 44.57, Critic Loss: 0.13, Policy Loss: -1.91\n",
      "Beam: 1, Iter: 5501, Q: 1.6584, Reward pred: 1, Reward: 1, BF Gain pred: 6.68, BF Gain: 3.29, Critic Loss: 0.08, Policy Loss: -1.71\n",
      "Beam: 2, Iter: 5501, Q: 1.5321, Reward pred: -1, Reward: -1, BF Gain pred: 71.54, BF Gain: 27.14, Critic Loss: 0.08, Policy Loss: -1.40\n",
      "Beam: 3, Iter: 5501, Q: 1.7434, Reward pred: -1, Reward: -1, BF Gain pred: 145.92, BF Gain: 68.95, Critic Loss: 0.10, Policy Loss: -1.58\n",
      "Training for 500 iteration for each Beam uses 22.83575749397278 seconds.\n",
      "Beam: 0, Iter: 6001, Q: 1.6054, Reward pred: 1, Reward: -1, BF Gain pred: 39.86, BF Gain: 4.26, Critic Loss: 0.17, Policy Loss: -1.54\n",
      "Beam: 1, Iter: 6001, Q: 1.9555, Reward pred: -1, Reward: 1, BF Gain pred: 1.86, BF Gain: 5.24, Critic Loss: 0.07, Policy Loss: -2.03\n",
      "Beam: 2, Iter: 6001, Q: 1.4273, Reward pred: 1, Reward: -1, BF Gain pred: 9.10, BF Gain: 2.28, Critic Loss: 0.10, Policy Loss: -1.36\n",
      "Beam: 3, Iter: 6001, Q: 1.3013, Reward pred: -1, Reward: -1, BF Gain pred: 119.51, BF Gain: 47.50, Critic Loss: 0.20, Policy Loss: -1.36\n",
      "Training for 500 iteration for each Beam uses 23.394086837768555 seconds.\n",
      "Beam: 0, Iter: 6501, Q: 1.7671, Reward pred: 1, Reward: -1, BF Gain pred: 110.25, BF Gain: 49.27, Critic Loss: 0.11, Policy Loss: -1.77\n",
      "Beam: 1, Iter: 6501, Q: 1.5611, Reward pred: -1, Reward: -1, BF Gain pred: 4.41, BF Gain: 7.00, Critic Loss: 0.04, Policy Loss: -2.14\n",
      "Beam: 2, Iter: 6501, Q: 1.5644, Reward pred: 1, Reward: -1, BF Gain pred: 67.59, BF Gain: 5.18, Critic Loss: 0.08, Policy Loss: -1.32\n",
      "Beam: 3, Iter: 6501, Q: 1.9694, Reward pred: -1, Reward: -1, BF Gain pred: 136.94, BF Gain: 12.47, Critic Loss: 0.14, Policy Loss: -1.40\n",
      "Training for 500 iteration for each Beam uses 22.477083683013916 seconds.\n",
      "Beam: 0, Iter: 7001, Q: 1.7245, Reward pred: 1, Reward: -1, BF Gain pred: 112.20, BF Gain: 21.25, Critic Loss: 0.17, Policy Loss: -1.67\n",
      "Beam: 1, Iter: 7001, Q: 2.0332, Reward pred: -1, Reward: -1, BF Gain pred: 16.79, BF Gain: 13.12, Critic Loss: 0.06, Policy Loss: -1.90\n",
      "Beam: 2, Iter: 7001, Q: 1.4388, Reward pred: 1, Reward: -1, BF Gain pred: 18.73, BF Gain: 6.44, Critic Loss: 0.10, Policy Loss: -1.31\n",
      "Beam: 3, Iter: 7001, Q: 1.5343, Reward pred: -1, Reward: -1, BF Gain pred: 89.18, BF Gain: 19.26, Critic Loss: 0.32, Policy Loss: -0.73\n",
      "Training for 500 iteration for each Beam uses 22.71896481513977 seconds.\n",
      "Beam: 0, Iter: 7501, Q: 1.7799, Reward pred: -1, Reward: -1, BF Gain pred: 119.92, BF Gain: 88.83, Critic Loss: 0.08, Policy Loss: -1.78\n",
      "Beam: 1, Iter: 7501, Q: 1.7022, Reward pred: 1, Reward: -1, BF Gain pred: 9.51, BF Gain: 3.26, Critic Loss: 0.23, Policy Loss: -1.71\n",
      "Beam: 2, Iter: 7501, Q: 1.6564, Reward pred: 1, Reward: -1, BF Gain pred: 33.92, BF Gain: 17.62, Critic Loss: 0.22, Policy Loss: -1.00\n",
      "Beam: 3, Iter: 7501, Q: 1.4669, Reward pred: -1, Reward: -1, BF Gain pred: 151.68, BF Gain: 89.60, Critic Loss: 0.36, Policy Loss: -0.72\n",
      "Training for 500 iteration for each Beam uses 23.253072500228882 seconds.\n",
      "Beam: 0, Iter: 8001, Q: 1.3915, Reward pred: 1, Reward: -1, BF Gain pred: 127.13, BF Gain: 21.59, Critic Loss: 0.18, Policy Loss: -0.79\n",
      "Beam: 1, Iter: 8001, Q: 1.5681, Reward pred: -1, Reward: -1, BF Gain pred: 23.03, BF Gain: 22.01, Critic Loss: 0.10, Policy Loss: -2.13\n",
      "Beam: 2, Iter: 8001, Q: 1.3404, Reward pred: 1, Reward: -1, BF Gain pred: 68.12, BF Gain: 23.12, Critic Loss: 0.20, Policy Loss: -0.87\n",
      "Beam: 3, Iter: 8001, Q: 1.4788, Reward pred: 1, Reward: -1, BF Gain pred: 163.77, BF Gain: 57.21, Critic Loss: 0.12, Policy Loss: -1.22\n",
      "Training for 500 iteration for each Beam uses 22.95959758758545 seconds.\n",
      "Beam: 0, Iter: 8501, Q: 1.6730, Reward pred: 1, Reward: -1, BF Gain pred: 102.32, BF Gain: 21.37, Critic Loss: 0.12, Policy Loss: -1.27\n",
      "Beam: 1, Iter: 8501, Q: 1.4911, Reward pred: 1, Reward: -1, BF Gain pred: 39.04, BF Gain: 1.68, Critic Loss: 0.08, Policy Loss: -2.19\n",
      "Beam: 2, Iter: 8501, Q: 1.3142, Reward pred: 1, Reward: -1, BF Gain pred: 68.81, BF Gain: 12.79, Critic Loss: 0.18, Policy Loss: -0.84\n",
      "Beam: 3, Iter: 8501, Q: 1.7733, Reward pred: -1, Reward: -1, BF Gain pred: 170.40, BF Gain: 28.44, Critic Loss: 0.13, Policy Loss: -1.31\n",
      "Training for 500 iteration for each Beam uses 22.67194437980652 seconds.\n",
      "Beam: 0, Iter: 9001, Q: 1.4668, Reward pred: 1, Reward: -1, BF Gain pred: 113.37, BF Gain: 91.30, Critic Loss: 0.14, Policy Loss: -1.67\n",
      "Beam: 1, Iter: 9001, Q: 1.3160, Reward pred: -1, Reward: -1, BF Gain pred: 41.70, BF Gain: 13.30, Critic Loss: 0.07, Policy Loss: -2.37\n",
      "Beam: 2, Iter: 9001, Q: 1.3860, Reward pred: -1, Reward: -1, BF Gain pred: 87.44, BF Gain: 32.59, Critic Loss: 0.20, Policy Loss: -0.75\n",
      "Beam: 3, Iter: 9001, Q: 1.5799, Reward pred: 1, Reward: -1, BF Gain pred: 159.81, BF Gain: 56.49, Critic Loss: 0.13, Policy Loss: -1.06\n",
      "Training for 500 iteration for each Beam uses 23.349357843399048 seconds.\n",
      "Beam: 0, Iter: 9501, Q: 1.6621, Reward pred: -1, Reward: -1, BF Gain pred: 119.06, BF Gain: 25.03, Critic Loss: 0.09, Policy Loss: -1.81\n",
      "Beam: 1, Iter: 9501, Q: 1.6945, Reward pred: 1, Reward: -1, BF Gain pred: 70.70, BF Gain: 13.97, Critic Loss: 0.06, Policy Loss: -2.04\n",
      "Beam: 2, Iter: 9501, Q: 1.3386, Reward pred: -1, Reward: -1, BF Gain pred: 94.24, BF Gain: 20.10, Critic Loss: 0.27, Policy Loss: -0.61\n",
      "Beam: 3, Iter: 9501, Q: 1.4156, Reward pred: -1, Reward: -1, BF Gain pred: 174.66, BF Gain: 59.66, Critic Loss: 0.14, Policy Loss: -1.02\n",
      "Training for 500 iteration for each Beam uses 22.974235773086548 seconds.\n",
      "Beam: 0, Iter: 10001, Q: 1.5495, Reward pred: -1, Reward: -1, BF Gain pred: 127.59, BF Gain: 40.79, Critic Loss: 0.10, Policy Loss: -1.55\n",
      "Beam: 1, Iter: 10001, Q: 1.6148, Reward pred: -1, Reward: -1, BF Gain pred: 41.11, BF Gain: 20.70, Critic Loss: 0.18, Policy Loss: -1.84\n",
      "Beam: 2, Iter: 10001, Q: 1.2541, Reward pred: -1, Reward: -1, BF Gain pred: 63.70, BF Gain: 25.70, Critic Loss: 0.26, Policy Loss: -0.58\n",
      "Beam: 3, Iter: 10001, Q: 1.4786, Reward pred: 1, Reward: -1, BF Gain pred: 205.22, BF Gain: 40.04, Critic Loss: 0.09, Policy Loss: -1.04\n",
      "Training for 500 iteration for each Beam uses 23.0744149684906 seconds.\n",
      "Beam: 0, Iter: 10501, Q: 1.9166, Reward pred: -1, Reward: -1, BF Gain pred: 80.67, BF Gain: 27.72, Critic Loss: 0.07, Policy Loss: -1.60\n",
      "Beam: 1, Iter: 10501, Q: 1.5267, Reward pred: 1, Reward: -1, BF Gain pred: 12.47, BF Gain: 2.84, Critic Loss: 0.45, Policy Loss: -0.75\n",
      "Beam: 2, Iter: 10501, Q: 1.3647, Reward pred: -1, Reward: -1, BF Gain pred: 76.69, BF Gain: 9.91, Critic Loss: 0.24, Policy Loss: -0.83\n",
      "Beam: 3, Iter: 10501, Q: 1.6586, Reward pred: -1, Reward: -1, BF Gain pred: 173.56, BF Gain: 72.98, Critic Loss: 0.07, Policy Loss: -1.05\n",
      "Training for 500 iteration for each Beam uses 23.40378165245056 seconds.\n",
      "Beam: 0, Iter: 11001, Q: 1.3395, Reward pred: 1, Reward: -1, BF Gain pred: 119.48, BF Gain: 76.96, Critic Loss: 0.33, Policy Loss: -0.91\n",
      "Beam: 1, Iter: 11001, Q: 1.8559, Reward pred: 1, Reward: -1, BF Gain pred: 33.43, BF Gain: 24.67, Critic Loss: 0.22, Policy Loss: -0.73\n",
      "Beam: 2, Iter: 11001, Q: 1.5716, Reward pred: -1, Reward: -1, BF Gain pred: 86.93, BF Gain: 15.10, Critic Loss: 0.20, Policy Loss: -0.68\n",
      "Beam: 3, Iter: 11001, Q: 1.4510, Reward pred: 1, Reward: -1, BF Gain pred: 179.79, BF Gain: 107.36, Critic Loss: 0.08, Policy Loss: -0.98\n",
      "Training for 500 iteration for each Beam uses 23.063506364822388 seconds.\n",
      "Beam: 0, Iter: 11501, Q: 1.5905, Reward pred: -1, Reward: -1, BF Gain pred: 87.10, BF Gain: 71.05, Critic Loss: 0.09, Policy Loss: -1.20\n",
      "Beam: 1, Iter: 11501, Q: 1.7480, Reward pred: -1, Reward: -1, BF Gain pred: 46.16, BF Gain: 6.33, Critic Loss: 0.20, Policy Loss: -1.55\n",
      "Beam: 2, Iter: 11501, Q: 1.4537, Reward pred: 1, Reward: -1, BF Gain pred: 60.72, BF Gain: 15.83, Critic Loss: 0.20, Policy Loss: -0.59\n",
      "Beam: 3, Iter: 11501, Q: 1.3357, Reward pred: 1, Reward: -1, BF Gain pred: 213.85, BF Gain: 71.24, Critic Loss: 0.08, Policy Loss: -0.93\n",
      "Training for 500 iteration for each Beam uses 23.38033437728882 seconds.\n",
      "Beam: 0, Iter: 12001, Q: 1.2319, Reward pred: -1, Reward: -1, BF Gain pred: 83.16, BF Gain: 9.43, Critic Loss: 0.17, Policy Loss: -0.85\n",
      "Beam: 1, Iter: 12001, Q: 1.6194, Reward pred: 1, Reward: -1, BF Gain pred: 34.00, BF Gain: 21.87, Critic Loss: 0.19, Policy Loss: -1.31\n",
      "Beam: 2, Iter: 12001, Q: 1.4444, Reward pred: 1, Reward: -1, BF Gain pred: 95.60, BF Gain: 21.53, Critic Loss: 0.19, Policy Loss: -0.70\n",
      "Beam: 3, Iter: 12001, Q: 1.4774, Reward pred: -1, Reward: -1, BF Gain pred: 196.33, BF Gain: 104.99, Critic Loss: 0.11, Policy Loss: -0.85\n",
      "Training for 500 iteration for each Beam uses 23.110881567001343 seconds.\n",
      "Beam: 0, Iter: 12501, Q: 1.7792, Reward pred: -1, Reward: -1, BF Gain pred: 104.09, BF Gain: 33.69, Critic Loss: 0.08, Policy Loss: -1.27\n",
      "Beam: 1, Iter: 12501, Q: 1.6069, Reward pred: 1, Reward: -1, BF Gain pred: 49.15, BF Gain: 19.00, Critic Loss: 0.09, Policy Loss: -2.19\n",
      "Beam: 2, Iter: 12501, Q: 1.4098, Reward pred: -1, Reward: -1, BF Gain pred: 108.43, BF Gain: 36.47, Critic Loss: 0.15, Policy Loss: -0.73\n",
      "Beam: 3, Iter: 12501, Q: 1.5478, Reward pred: -1, Reward: -1, BF Gain pred: 173.70, BF Gain: 35.88, Critic Loss: 0.07, Policy Loss: -1.03\n",
      "Training for 500 iteration for each Beam uses 23.467511653900146 seconds.\n",
      "Beam: 0, Iter: 13001, Q: 1.8959, Reward pred: 1, Reward: -1, BF Gain pred: 124.64, BF Gain: 34.31, Critic Loss: 0.05, Policy Loss: -1.29\n",
      "Beam: 1, Iter: 13001, Q: 1.5883, Reward pred: 1, Reward: -1, BF Gain pred: 62.24, BF Gain: 11.97, Critic Loss: 0.09, Policy Loss: -2.33\n",
      "Beam: 2, Iter: 13001, Q: 1.1869, Reward pred: -1, Reward: -1, BF Gain pred: 101.26, BF Gain: 14.51, Critic Loss: 0.20, Policy Loss: -0.64\n",
      "Beam: 3, Iter: 13001, Q: 1.6227, Reward pred: 1, Reward: -1, BF Gain pred: 207.73, BF Gain: 35.07, Critic Loss: 0.07, Policy Loss: -1.09\n",
      "Training for 500 iteration for each Beam uses 22.85884690284729 seconds.\n",
      "Beam: 0, Iter: 13501, Q: 1.3959, Reward pred: -1, Reward: -1, BF Gain pred: 124.89, BF Gain: 30.31, Critic Loss: 0.07, Policy Loss: -1.17\n",
      "Beam: 1, Iter: 13501, Q: 1.3409, Reward pred: 1, Reward: -1, BF Gain pred: 64.71, BF Gain: 13.21, Critic Loss: 0.07, Policy Loss: -1.93\n",
      "Beam: 2, Iter: 13501, Q: 1.4115, Reward pred: 1, Reward: -1, BF Gain pred: 117.66, BF Gain: 17.10, Critic Loss: 0.14, Policy Loss: -0.70\n",
      "Beam: 3, Iter: 13501, Q: 1.3776, Reward pred: -1, Reward: -1, BF Gain pred: 214.00, BF Gain: 47.98, Critic Loss: 0.17, Policy Loss: -0.77\n",
      "Training for 500 iteration for each Beam uses 23.30410146713257 seconds.\n",
      "Beam: 0, Iter: 14001, Q: 1.4501, Reward pred: -1, Reward: -1, BF Gain pred: 120.00, BF Gain: 20.52, Critic Loss: 0.06, Policy Loss: -1.25\n",
      "Beam: 1, Iter: 14001, Q: 1.5735, Reward pred: 1, Reward: -1, BF Gain pred: 78.86, BF Gain: 22.81, Critic Loss: 0.06, Policy Loss: -1.85\n",
      "Beam: 2, Iter: 14001, Q: 1.6042, Reward pred: -1, Reward: -1, BF Gain pred: 97.31, BF Gain: 32.66, Critic Loss: 0.12, Policy Loss: -0.83\n",
      "Beam: 3, Iter: 14001, Q: 1.5599, Reward pred: -1, Reward: -1, BF Gain pred: 206.99, BF Gain: 97.29, Critic Loss: 0.08, Policy Loss: -0.87\n",
      "Training for 500 iteration for each Beam uses 25.324312925338745 seconds.\n",
      "Beam: 0, Iter: 14501, Q: 1.5587, Reward pred: 1, Reward: -1, BF Gain pred: 125.84, BF Gain: 56.90, Critic Loss: 0.04, Policy Loss: -1.32\n",
      "Beam: 1, Iter: 14501, Q: 1.6327, Reward pred: 1, Reward: -1, BF Gain pred: 53.53, BF Gain: 12.25, Critic Loss: 0.07, Policy Loss: -2.15\n",
      "Beam: 2, Iter: 14501, Q: 1.4296, Reward pred: 1, Reward: -1, BF Gain pred: 110.91, BF Gain: 42.27, Critic Loss: 0.12, Policy Loss: -0.78\n",
      "Beam: 3, Iter: 14501, Q: 1.5287, Reward pred: -1, Reward: -1, BF Gain pred: 213.15, BF Gain: 38.04, Critic Loss: 0.10, Policy Loss: -1.03\n",
      "Training for 500 iteration for each Beam uses 28.3304123878479 seconds.\n",
      "Beam: 0, Iter: 15001, Q: 1.3902, Reward pred: -1, Reward: -1, BF Gain pred: 123.00, BF Gain: 33.89, Critic Loss: 0.05, Policy Loss: -1.35\n",
      "Beam: 1, Iter: 15001, Q: 1.4737, Reward pred: -1, Reward: -1, BF Gain pred: 64.10, BF Gain: 29.95, Critic Loss: 0.05, Policy Loss: -2.15\n",
      "Beam: 2, Iter: 15001, Q: 1.3814, Reward pred: 1, Reward: -1, BF Gain pred: 130.24, BF Gain: 29.77, Critic Loss: 0.10, Policy Loss: -0.90\n",
      "Beam: 3, Iter: 15001, Q: 1.5956, Reward pred: 1, Reward: -1, BF Gain pred: 210.42, BF Gain: 84.83, Critic Loss: 0.09, Policy Loss: -1.09\n",
      "Training for 500 iteration for each Beam uses 25.484688997268677 seconds.\n",
      "Beam: 0, Iter: 15501, Q: 1.5523, Reward pred: -1, Reward: -1, BF Gain pred: 112.50, BF Gain: 50.32, Critic Loss: 0.08, Policy Loss: -1.16\n",
      "Beam: 1, Iter: 15501, Q: 1.5853, Reward pred: -1, Reward: -1, BF Gain pred: 55.95, BF Gain: 26.93, Critic Loss: 0.04, Policy Loss: -2.07\n",
      "Beam: 2, Iter: 15501, Q: 1.3487, Reward pred: -1, Reward: -1, BF Gain pred: 98.58, BF Gain: 53.76, Critic Loss: 0.06, Policy Loss: -1.02\n",
      "Beam: 3, Iter: 15501, Q: 1.7050, Reward pred: -1, Reward: -1, BF Gain pred: 199.32, BF Gain: 73.56, Critic Loss: 0.08, Policy Loss: -1.27\n",
      "Training for 500 iteration for each Beam uses 23.068755626678467 seconds.\n",
      "Beam: 0, Iter: 16001, Q: 1.5928, Reward pred: 1, Reward: -1, BF Gain pred: 134.16, BF Gain: 60.41, Critic Loss: 0.07, Policy Loss: -1.22\n",
      "Beam: 1, Iter: 16001, Q: 1.3811, Reward pred: -1, Reward: -1, BF Gain pred: 80.65, BF Gain: 33.72, Critic Loss: 0.04, Policy Loss: -1.69\n",
      "Beam: 2, Iter: 16001, Q: 1.6114, Reward pred: 1, Reward: -1, BF Gain pred: 139.21, BF Gain: 46.24, Critic Loss: 0.10, Policy Loss: -1.00\n",
      "Beam: 3, Iter: 16001, Q: 1.8538, Reward pred: -1, Reward: -1, BF Gain pred: 179.54, BF Gain: 22.49, Critic Loss: 0.08, Policy Loss: -1.03\n",
      "Training for 500 iteration for each Beam uses 22.97593092918396 seconds.\n",
      "Beam: 0, Iter: 16501, Q: 1.2051, Reward pred: 1, Reward: -1, BF Gain pred: 129.63, BF Gain: 46.92, Critic Loss: 0.08, Policy Loss: -1.07\n",
      "Beam: 1, Iter: 16501, Q: 2.2298, Reward pred: 1, Reward: -1, BF Gain pred: 73.60, BF Gain: 45.26, Critic Loss: 0.04, Policy Loss: -1.56\n",
      "Beam: 2, Iter: 16501, Q: 1.3518, Reward pred: -1, Reward: -1, BF Gain pred: 113.94, BF Gain: 14.36, Critic Loss: 0.08, Policy Loss: -0.90\n",
      "Beam: 3, Iter: 16501, Q: 1.5052, Reward pred: 1, Reward: -1, BF Gain pred: 219.83, BF Gain: 88.45, Critic Loss: 0.07, Policy Loss: -1.15\n",
      "Training for 500 iteration for each Beam uses 23.29143238067627 seconds.\n",
      "Beam: 0, Iter: 17001, Q: 1.4509, Reward pred: -1, Reward: -1, BF Gain pred: 172.45, BF Gain: 99.70, Critic Loss: 0.08, Policy Loss: -1.18\n",
      "Beam: 1, Iter: 17001, Q: 1.4003, Reward pred: 1, Reward: -1, BF Gain pred: 81.74, BF Gain: 14.66, Critic Loss: 0.06, Policy Loss: -1.21\n",
      "Beam: 2, Iter: 17001, Q: 1.6762, Reward pred: -1, Reward: -1, BF Gain pred: 130.03, BF Gain: 40.73, Critic Loss: 0.19, Policy Loss: -0.62\n",
      "Beam: 3, Iter: 17001, Q: 1.6100, Reward pred: 1, Reward: -1, BF Gain pred: 226.42, BF Gain: 136.76, Critic Loss: 0.05, Policy Loss: -1.18\n",
      "Training for 500 iteration for each Beam uses 22.874186992645264 seconds.\n",
      "Beam: 0, Iter: 17501, Q: 1.5593, Reward pred: 1, Reward: -1, BF Gain pred: 155.48, BF Gain: 38.45, Critic Loss: 0.06, Policy Loss: -1.30\n",
      "Beam: 1, Iter: 17501, Q: 1.9529, Reward pred: -1, Reward: -1, BF Gain pred: 68.10, BF Gain: 24.03, Critic Loss: 0.05, Policy Loss: -1.28\n",
      "Beam: 2, Iter: 17501, Q: 1.4199, Reward pred: 1, Reward: -1, BF Gain pred: 134.42, BF Gain: 41.37, Critic Loss: 0.11, Policy Loss: -0.84\n",
      "Beam: 3, Iter: 17501, Q: 1.8365, Reward pred: 1, Reward: -1, BF Gain pred: 227.14, BF Gain: 89.97, Critic Loss: 0.06, Policy Loss: -1.15\n",
      "Training for 500 iteration for each Beam uses 23.083401679992676 seconds.\n",
      "Beam: 0, Iter: 18001, Q: 1.5326, Reward pred: -1, Reward: -1, BF Gain pred: 135.23, BF Gain: 50.70, Critic Loss: 0.04, Policy Loss: -1.35\n",
      "Beam: 1, Iter: 18001, Q: 1.5618, Reward pred: 1, Reward: -1, BF Gain pred: 71.66, BF Gain: 16.55, Critic Loss: 0.04, Policy Loss: -1.45\n",
      "Beam: 2, Iter: 18001, Q: 1.5699, Reward pred: -1, Reward: -1, BF Gain pred: 127.52, BF Gain: 10.84, Critic Loss: 0.09, Policy Loss: -0.82\n",
      "Beam: 3, Iter: 18001, Q: 1.3701, Reward pred: 1, Reward: -1, BF Gain pred: 243.33, BF Gain: 70.26, Critic Loss: 0.20, Policy Loss: -1.09\n",
      "Training for 500 iteration for each Beam uses 23.407624006271362 seconds.\n",
      "Beam: 0, Iter: 18501, Q: 0.8573, Reward pred: 1, Reward: -1, BF Gain pred: 110.88, BF Gain: 42.98, Critic Loss: 0.31, Policy Loss: -0.51\n",
      "Beam: 1, Iter: 18501, Q: 1.5285, Reward pred: 1, Reward: -1, BF Gain pred: 69.32, BF Gain: 28.63, Critic Loss: 0.06, Policy Loss: -1.19\n",
      "Beam: 2, Iter: 18501, Q: 1.5192, Reward pred: -1, Reward: -1, BF Gain pred: 127.54, BF Gain: 45.58, Critic Loss: 0.11, Policy Loss: -0.70\n",
      "Beam: 3, Iter: 18501, Q: 1.3353, Reward pred: -1, Reward: -1, BF Gain pred: 235.17, BF Gain: 49.83, Critic Loss: 0.08, Policy Loss: -0.94\n",
      "Training for 500 iteration for each Beam uses 23.068427085876465 seconds.\n",
      "Beam: 0, Iter: 19001, Q: 1.7002, Reward pred: 1, Reward: -1, BF Gain pred: 134.20, BF Gain: 70.64, Critic Loss: 0.05, Policy Loss: -1.22\n",
      "Beam: 1, Iter: 19001, Q: 2.8612, Reward pred: -1, Reward: -1, BF Gain pred: 64.93, BF Gain: 9.37, Critic Loss: 0.05, Policy Loss: -1.29\n",
      "Beam: 2, Iter: 19001, Q: 1.5453, Reward pred: -1, Reward: -1, BF Gain pred: 129.31, BF Gain: 43.32, Critic Loss: 0.16, Policy Loss: -0.62\n",
      "Beam: 3, Iter: 19001, Q: 1.4378, Reward pred: 1, Reward: -1, BF Gain pred: 236.27, BF Gain: 84.47, Critic Loss: 0.07, Policy Loss: -1.02\n",
      "Training for 500 iteration for each Beam uses 23.35966420173645 seconds.\n",
      "Beam: 0, Iter: 19501, Q: 1.6220, Reward pred: 1, Reward: -1, BF Gain pred: 154.66, BF Gain: 37.66, Critic Loss: 0.06, Policy Loss: -1.38\n",
      "Beam: 1, Iter: 19501, Q: 1.5505, Reward pred: -1, Reward: -1, BF Gain pred: 70.86, BF Gain: 30.52, Critic Loss: 0.04, Policy Loss: -1.30\n",
      "Beam: 2, Iter: 19501, Q: 1.5383, Reward pred: -1, Reward: -1, BF Gain pred: 129.07, BF Gain: 26.91, Critic Loss: 0.13, Policy Loss: -0.74\n",
      "Beam: 3, Iter: 19501, Q: 1.5488, Reward pred: -1, Reward: -1, BF Gain pred: 223.89, BF Gain: 42.66, Critic Loss: 0.07, Policy Loss: -1.00\n",
      "Training for 500 iteration for each Beam uses 23.036473274230957 seconds.\n",
      "Beam: 0, Iter: 20001, Q: 1.4051, Reward pred: -1, Reward: -1, BF Gain pred: 125.17, BF Gain: 33.18, Critic Loss: 0.05, Policy Loss: -1.34\n",
      "Beam: 1, Iter: 20001, Q: 2.2856, Reward pred: 1, Reward: -1, BF Gain pred: 70.21, BF Gain: 14.72, Critic Loss: 0.04, Policy Loss: -1.33\n",
      "Beam: 2, Iter: 20001, Q: 1.5376, Reward pred: -1, Reward: -1, BF Gain pred: 123.31, BF Gain: 30.75, Critic Loss: 0.13, Policy Loss: -0.77\n",
      "Beam: 3, Iter: 20001, Q: 1.3885, Reward pred: 1, Reward: -1, BF Gain pred: 245.54, BF Gain: 41.13, Critic Loss: 0.05, Policy Loss: -1.18\n",
      "Training for 500 iteration for each Beam uses 23.25068974494934 seconds.\n",
      "Beam: 0, Iter: 20501, Q: 1.6118, Reward pred: -1, Reward: -1, BF Gain pred: 124.46, BF Gain: 72.22, Critic Loss: 0.05, Policy Loss: -1.38\n",
      "Beam: 1, Iter: 20501, Q: 1.2697, Reward pred: 1, Reward: -1, BF Gain pred: 78.48, BF Gain: 38.72, Critic Loss: 0.04, Policy Loss: -1.27\n",
      "Beam: 2, Iter: 20501, Q: 1.8205, Reward pred: 1, Reward: -1, BF Gain pred: 133.83, BF Gain: 46.82, Critic Loss: 0.09, Policy Loss: -0.83\n",
      "Beam: 3, Iter: 20501, Q: 1.4868, Reward pred: -1, Reward: -1, BF Gain pred: 213.93, BF Gain: 148.42, Critic Loss: 0.05, Policy Loss: -1.13\n",
      "Training for 500 iteration for each Beam uses 23.283596754074097 seconds.\n",
      "Beam: 0, Iter: 21001, Q: 1.4177, Reward pred: -1, Reward: -1, BF Gain pred: 136.46, BF Gain: 42.70, Critic Loss: 0.05, Policy Loss: -1.47\n",
      "Beam: 1, Iter: 21001, Q: 1.6849, Reward pred: -1, Reward: -1, BF Gain pred: 78.98, BF Gain: 34.91, Critic Loss: 0.05, Policy Loss: -1.28\n",
      "Beam: 2, Iter: 21001, Q: 1.5814, Reward pred: -1, Reward: -1, BF Gain pred: 137.74, BF Gain: 57.02, Critic Loss: 0.11, Policy Loss: -0.74\n",
      "Beam: 3, Iter: 21001, Q: 1.4180, Reward pred: 1, Reward: -1, BF Gain pred: 222.69, BF Gain: 75.45, Critic Loss: 0.07, Policy Loss: -1.01\n",
      "Training for 500 iteration for each Beam uses 23.0280544757843 seconds.\n",
      "Beam: 0, Iter: 21501, Q: 1.3455, Reward pred: -1, Reward: -1, BF Gain pred: 138.10, BF Gain: 30.78, Critic Loss: 0.14, Policy Loss: -0.84\n",
      "Beam: 1, Iter: 21501, Q: 1.7330, Reward pred: 1, Reward: -1, BF Gain pred: 80.51, BF Gain: 11.85, Critic Loss: 0.05, Policy Loss: -1.26\n",
      "Beam: 2, Iter: 21501, Q: 1.4224, Reward pred: -1, Reward: -1, BF Gain pred: 126.56, BF Gain: 52.60, Critic Loss: 0.10, Policy Loss: -0.79\n",
      "Beam: 3, Iter: 21501, Q: 1.6382, Reward pred: 1, Reward: -1, BF Gain pred: 220.93, BF Gain: 88.32, Critic Loss: 0.06, Policy Loss: -1.11\n",
      "Training for 500 iteration for each Beam uses 23.80510139465332 seconds.\n",
      "Beam: 0, Iter: 22001, Q: 1.2821, Reward pred: -1, Reward: -1, BF Gain pred: 130.72, BF Gain: 97.30, Critic Loss: 0.07, Policy Loss: -1.17\n",
      "Beam: 1, Iter: 22001, Q: 1.5324, Reward pred: 1, Reward: -1, BF Gain pred: 57.84, BF Gain: 32.59, Critic Loss: 0.05, Policy Loss: -1.23\n",
      "Beam: 2, Iter: 22001, Q: 1.4509, Reward pred: 1, Reward: -1, BF Gain pred: 122.37, BF Gain: 18.89, Critic Loss: 0.09, Policy Loss: -0.64\n",
      "Beam: 3, Iter: 22001, Q: 1.5118, Reward pred: -1, Reward: -1, BF Gain pred: 238.30, BF Gain: 83.54, Critic Loss: 0.06, Policy Loss: -1.15\n",
      "Training for 500 iteration for each Beam uses 23.490033864974976 seconds.\n",
      "Beam: 0, Iter: 22501, Q: 1.3700, Reward pred: 1, Reward: -1, BF Gain pred: 164.94, BF Gain: 57.67, Critic Loss: 0.05, Policy Loss: -1.21\n",
      "Beam: 1, Iter: 22501, Q: 1.5489, Reward pred: -1, Reward: -1, BF Gain pred: 69.51, BF Gain: 31.09, Critic Loss: 0.04, Policy Loss: -1.24\n",
      "Beam: 2, Iter: 22501, Q: 1.0169, Reward pred: -1, Reward: -1, BF Gain pred: 138.00, BF Gain: 73.73, Critic Loss: 0.17, Policy Loss: -0.48\n",
      "Beam: 3, Iter: 22501, Q: 1.4602, Reward pred: -1, Reward: -1, BF Gain pred: 199.25, BF Gain: 72.77, Critic Loss: 0.07, Policy Loss: -1.01\n",
      "Training for 500 iteration for each Beam uses 23.467622995376587 seconds.\n",
      "Beam: 0, Iter: 23001, Q: 1.5792, Reward pred: -1, Reward: -1, BF Gain pred: 161.43, BF Gain: 42.30, Critic Loss: 0.05, Policy Loss: -1.19\n",
      "Beam: 1, Iter: 23001, Q: 1.3221, Reward pred: 1, Reward: -1, BF Gain pred: 78.06, BF Gain: 26.28, Critic Loss: 0.05, Policy Loss: -1.28\n",
      "Beam: 2, Iter: 23001, Q: 1.3971, Reward pred: 1, Reward: -1, BF Gain pred: 143.89, BF Gain: 52.49, Critic Loss: 0.15, Policy Loss: -0.60\n",
      "Beam: 3, Iter: 23001, Q: 1.5931, Reward pred: -1, Reward: -1, BF Gain pred: 212.07, BF Gain: 92.86, Critic Loss: 0.07, Policy Loss: -1.05\n",
      "Training for 500 iteration for each Beam uses 23.205338716506958 seconds.\n",
      "Beam: 0, Iter: 23501, Q: 1.5981, Reward pred: 1, Reward: -1, BF Gain pred: 163.48, BF Gain: 38.18, Critic Loss: 0.05, Policy Loss: -1.27\n",
      "Beam: 1, Iter: 23501, Q: 1.4073, Reward pred: 1, Reward: -1, BF Gain pred: 75.09, BF Gain: 20.78, Critic Loss: 0.04, Policy Loss: -1.26\n",
      "Beam: 2, Iter: 23501, Q: 1.3859, Reward pred: 1, Reward: -1, BF Gain pred: 138.79, BF Gain: 38.79, Critic Loss: 0.11, Policy Loss: -0.66\n",
      "Beam: 3, Iter: 23501, Q: 1.4637, Reward pred: 1, Reward: -1, BF Gain pred: 222.90, BF Gain: 73.90, Critic Loss: 0.05, Policy Loss: -1.02\n",
      "Training for 500 iteration for each Beam uses 22.840009450912476 seconds.\n",
      "Beam: 0, Iter: 24001, Q: 1.4267, Reward pred: 1, Reward: -1, BF Gain pred: 157.38, BF Gain: 62.11, Critic Loss: 0.05, Policy Loss: -1.21\n",
      "Beam: 1, Iter: 24001, Q: 1.3908, Reward pred: -1, Reward: -1, BF Gain pred: 60.29, BF Gain: 10.43, Critic Loss: 0.05, Policy Loss: -1.17\n",
      "Beam: 2, Iter: 24001, Q: 1.3514, Reward pred: 1, Reward: -1, BF Gain pred: 142.67, BF Gain: 71.05, Critic Loss: 0.07, Policy Loss: -0.76\n",
      "Beam: 3, Iter: 24001, Q: 1.5578, Reward pred: -1, Reward: -1, BF Gain pred: 221.96, BF Gain: 115.54, Critic Loss: 0.07, Policy Loss: -1.12\n",
      "Training for 500 iteration for each Beam uses 23.419461011886597 seconds.\n",
      "Beam: 0, Iter: 24501, Q: 1.5612, Reward pred: 1, Reward: -1, BF Gain pred: 145.35, BF Gain: 68.56, Critic Loss: 0.05, Policy Loss: -1.35\n",
      "Beam: 1, Iter: 24501, Q: 1.8888, Reward pred: 1, Reward: -1, BF Gain pred: 89.03, BF Gain: 19.82, Critic Loss: 0.05, Policy Loss: -1.16\n",
      "Beam: 2, Iter: 24501, Q: 1.5449, Reward pred: -1, Reward: -1, BF Gain pred: 143.71, BF Gain: 60.63, Critic Loss: 0.09, Policy Loss: -0.73\n",
      "Beam: 3, Iter: 24501, Q: 1.4279, Reward pred: -1, Reward: -1, BF Gain pred: 240.00, BF Gain: 76.16, Critic Loss: 0.07, Policy Loss: -1.09\n",
      "Training for 500 iteration for each Beam uses 23.267056703567505 seconds.\n",
      "Beam: 0, Iter: 25001, Q: 1.4498, Reward pred: 1, Reward: -1, BF Gain pred: 142.31, BF Gain: 46.76, Critic Loss: 0.05, Policy Loss: -1.39\n",
      "Beam: 1, Iter: 25001, Q: 1.6230, Reward pred: -1, Reward: -1, BF Gain pred: 72.28, BF Gain: 21.76, Critic Loss: 0.04, Policy Loss: -1.29\n",
      "Beam: 2, Iter: 25001, Q: 1.3072, Reward pred: -1, Reward: -1, BF Gain pred: 134.78, BF Gain: 38.30, Critic Loss: 0.08, Policy Loss: -0.85\n",
      "Beam: 3, Iter: 25001, Q: 1.4182, Reward pred: 1, Reward: -1, BF Gain pred: 245.77, BF Gain: 93.97, Critic Loss: 0.07, Policy Loss: -1.11\n",
      "Training for 500 iteration for each Beam uses 23.20244002342224 seconds.\n",
      "Beam: 0, Iter: 25501, Q: 1.5665, Reward pred: 1, Reward: -1, BF Gain pred: 158.26, BF Gain: 39.10, Critic Loss: 0.06, Policy Loss: -1.36\n",
      "Beam: 1, Iter: 25501, Q: 1.3608, Reward pred: -1, Reward: -1, BF Gain pred: 86.72, BF Gain: 23.92, Critic Loss: 0.05, Policy Loss: -1.32\n",
      "Beam: 2, Iter: 25501, Q: 1.7561, Reward pred: 1, Reward: -1, BF Gain pred: 139.16, BF Gain: 66.74, Critic Loss: 0.12, Policy Loss: -0.79\n",
      "Beam: 3, Iter: 25501, Q: 1.6291, Reward pred: 1, Reward: -1, BF Gain pred: 226.26, BF Gain: 69.12, Critic Loss: 0.08, Policy Loss: -1.06\n",
      "Training for 500 iteration for each Beam uses 23.54176902770996 seconds.\n",
      "Beam: 0, Iter: 26001, Q: 1.8942, Reward pred: -1, Reward: -1, BF Gain pred: 152.12, BF Gain: 40.27, Critic Loss: 0.04, Policy Loss: -1.32\n",
      "Beam: 1, Iter: 26001, Q: 1.5524, Reward pred: 1, Reward: -1, BF Gain pred: 72.93, BF Gain: 42.50, Critic Loss: 0.02, Policy Loss: -1.31\n",
      "Beam: 2, Iter: 26001, Q: 1.4088, Reward pred: -1, Reward: -1, BF Gain pred: 141.08, BF Gain: 82.30, Critic Loss: 0.08, Policy Loss: -0.86\n",
      "Beam: 3, Iter: 26001, Q: 1.4152, Reward pred: -1, Reward: -1, BF Gain pred: 228.70, BF Gain: 120.69, Critic Loss: 0.06, Policy Loss: -1.19\n",
      "Training for 500 iteration for each Beam uses 23.36977982521057 seconds.\n",
      "Beam: 0, Iter: 26501, Q: 1.5016, Reward pred: 1, Reward: -1, BF Gain pred: 149.38, BF Gain: 78.32, Critic Loss: 0.06, Policy Loss: -1.03\n",
      "Beam: 1, Iter: 26501, Q: 1.4964, Reward pred: -1, Reward: -1, BF Gain pred: 88.13, BF Gain: 35.69, Critic Loss: 0.05, Policy Loss: -1.23\n",
      "Beam: 2, Iter: 26501, Q: 2.5723, Reward pred: 1, Reward: -1, BF Gain pred: 143.96, BF Gain: 69.28, Critic Loss: 0.07, Policy Loss: -0.80\n",
      "Beam: 3, Iter: 26501, Q: 1.5717, Reward pred: 1, Reward: -1, BF Gain pred: 241.54, BF Gain: 41.85, Critic Loss: 0.06, Policy Loss: -1.06\n",
      "Training for 500 iteration for each Beam uses 23.221720933914185 seconds.\n",
      "Beam: 0, Iter: 27001, Q: 1.4403, Reward pred: 1, Reward: -1, BF Gain pred: 166.58, BF Gain: 62.54, Critic Loss: 0.05, Policy Loss: -0.97\n",
      "Beam: 1, Iter: 27001, Q: 1.5675, Reward pred: 1, Reward: -1, BF Gain pred: 80.50, BF Gain: 33.87, Critic Loss: 0.04, Policy Loss: -1.23\n",
      "Beam: 2, Iter: 27001, Q: 1.8497, Reward pred: -1, Reward: -1, BF Gain pred: 144.38, BF Gain: 45.41, Critic Loss: 0.11, Policy Loss: -0.70\n",
      "Beam: 3, Iter: 27001, Q: 1.4739, Reward pred: 1, Reward: -1, BF Gain pred: 244.99, BF Gain: 32.19, Critic Loss: 0.06, Policy Loss: -1.08\n",
      "Training for 500 iteration for each Beam uses 23.57211661338806 seconds.\n",
      "Beam: 0, Iter: 27501, Q: 1.6688, Reward pred: 1, Reward: -1, BF Gain pred: 164.95, BF Gain: 69.36, Critic Loss: 0.09, Policy Loss: -0.80\n",
      "Beam: 1, Iter: 27501, Q: 2.0947, Reward pred: -1, Reward: -1, BF Gain pred: 86.22, BF Gain: 28.19, Critic Loss: 0.05, Policy Loss: -1.19\n",
      "Beam: 2, Iter: 27501, Q: 1.3789, Reward pred: -1, Reward: -1, BF Gain pred: 144.46, BF Gain: 58.10, Critic Loss: 0.10, Policy Loss: -0.81\n",
      "Beam: 3, Iter: 27501, Q: 1.4113, Reward pred: -1, Reward: -1, BF Gain pred: 212.22, BF Gain: 82.34, Critic Loss: 0.07, Policy Loss: -1.07\n",
      "Training for 500 iteration for each Beam uses 23.64607071876526 seconds.\n",
      "Beam: 0, Iter: 28001, Q: 1.4218, Reward pred: 1, Reward: -1, BF Gain pred: 164.63, BF Gain: 100.56, Critic Loss: 0.09, Policy Loss: -0.84\n",
      "Beam: 1, Iter: 28001, Q: 1.3204, Reward pred: 1, Reward: -1, BF Gain pred: 85.91, BF Gain: 44.64, Critic Loss: 0.03, Policy Loss: -1.26\n",
      "Beam: 2, Iter: 28001, Q: 1.5641, Reward pred: 1, Reward: -1, BF Gain pred: 137.29, BF Gain: 30.97, Critic Loss: 0.10, Policy Loss: -0.76\n",
      "Beam: 3, Iter: 28001, Q: 1.8035, Reward pred: -1, Reward: -1, BF Gain pred: 244.20, BF Gain: 64.58, Critic Loss: 0.05, Policy Loss: -1.11\n",
      "Training for 500 iteration for each Beam uses 22.947882890701294 seconds.\n",
      "Beam: 0, Iter: 28501, Q: 1.5990, Reward pred: -1, Reward: -1, BF Gain pred: 145.62, BF Gain: 36.49, Critic Loss: 0.06, Policy Loss: -1.01\n",
      "Beam: 1, Iter: 28501, Q: 1.4479, Reward pred: 1, Reward: -1, BF Gain pred: 83.53, BF Gain: 23.70, Critic Loss: 0.04, Policy Loss: -1.20\n",
      "Beam: 2, Iter: 28501, Q: 1.4648, Reward pred: -1, Reward: -1, BF Gain pred: 145.35, BF Gain: 55.41, Critic Loss: 0.09, Policy Loss: -0.84\n",
      "Beam: 3, Iter: 28501, Q: 1.2906, Reward pred: -1, Reward: -1, BF Gain pred: 211.87, BF Gain: 91.96, Critic Loss: 0.06, Policy Loss: -1.14\n",
      "Training for 500 iteration for each Beam uses 23.531452655792236 seconds.\n",
      "Beam: 0, Iter: 29001, Q: 1.5116, Reward pred: -1, Reward: -1, BF Gain pred: 162.45, BF Gain: 63.02, Critic Loss: 0.06, Policy Loss: -1.14\n",
      "Beam: 1, Iter: 29001, Q: 1.2919, Reward pred: 1, Reward: -1, BF Gain pred: 88.37, BF Gain: 34.45, Critic Loss: 0.04, Policy Loss: -1.18\n",
      "Beam: 2, Iter: 29001, Q: 1.7394, Reward pred: -1, Reward: -1, BF Gain pred: 139.60, BF Gain: 42.25, Critic Loss: 0.06, Policy Loss: -0.74\n",
      "Beam: 3, Iter: 29001, Q: 1.4243, Reward pred: 1, Reward: -1, BF Gain pred: 263.66, BF Gain: 107.99, Critic Loss: 0.04, Policy Loss: -1.10\n",
      "Training for 500 iteration for each Beam uses 23.732890844345093 seconds.\n",
      "Beam: 0, Iter: 29501, Q: 1.8882, Reward pred: -1, Reward: -1, BF Gain pred: 152.38, BF Gain: 50.97, Critic Loss: 0.06, Policy Loss: -1.10\n",
      "Beam: 1, Iter: 29501, Q: 1.5287, Reward pred: -1, Reward: -1, BF Gain pred: 81.64, BF Gain: 6.75, Critic Loss: 0.04, Policy Loss: -1.22\n",
      "Beam: 2, Iter: 29501, Q: 1.6943, Reward pred: -1, Reward: -1, BF Gain pred: 142.64, BF Gain: 49.24, Critic Loss: 0.09, Policy Loss: -0.74\n",
      "Beam: 3, Iter: 29501, Q: 1.4856, Reward pred: -1, Reward: -1, BF Gain pred: 215.03, BF Gain: 112.11, Critic Loss: 0.03, Policy Loss: -1.06\n",
      "Training for 500 iteration for each Beam uses 23.27179789543152 seconds.\n",
      "Beam: 0, Iter: 30001, Q: 1.6229, Reward pred: -1, Reward: -1, BF Gain pred: 163.86, BF Gain: 75.47, Critic Loss: 0.07, Policy Loss: -1.26\n",
      "Beam: 1, Iter: 30001, Q: 1.3539, Reward pred: 1, Reward: -1, BF Gain pred: 80.78, BF Gain: 33.10, Critic Loss: 0.04, Policy Loss: -1.22\n",
      "Beam: 2, Iter: 30001, Q: 1.6394, Reward pred: 1, Reward: -1, BF Gain pred: 138.70, BF Gain: 55.50, Critic Loss: 0.10, Policy Loss: -0.83\n",
      "Beam: 3, Iter: 30001, Q: 1.5477, Reward pred: 1, Reward: -1, BF Gain pred: 227.15, BF Gain: 65.06, Critic Loss: 0.05, Policy Loss: -1.06\n",
      "Training for 500 iteration for each Beam uses 23.411839962005615 seconds.\n",
      "Beam: 0, Iter: 30501, Q: 1.3249, Reward pred: -1, Reward: -1, BF Gain pred: 148.43, BF Gain: 62.12, Critic Loss: 0.12, Policy Loss: -0.96\n",
      "Beam: 1, Iter: 30501, Q: 1.6966, Reward pred: 1, Reward: -1, BF Gain pred: 92.09, BF Gain: 40.46, Critic Loss: 0.05, Policy Loss: -1.27\n",
      "Beam: 2, Iter: 30501, Q: 1.6253, Reward pred: 1, Reward: -1, BF Gain pred: 144.51, BF Gain: 61.45, Critic Loss: 0.09, Policy Loss: -0.70\n",
      "Beam: 3, Iter: 30501, Q: 1.9954, Reward pred: -1, Reward: -1, BF Gain pred: 247.34, BF Gain: 120.35, Critic Loss: 0.07, Policy Loss: -1.07\n",
      "Training for 500 iteration for each Beam uses 23.3080472946167 seconds.\n",
      "Beam: 0, Iter: 31001, Q: 1.5253, Reward pred: -1, Reward: -1, BF Gain pred: 168.22, BF Gain: 45.75, Critic Loss: 0.07, Policy Loss: -1.18\n",
      "Beam: 1, Iter: 31001, Q: 1.6356, Reward pred: 1, Reward: -1, BF Gain pred: 88.94, BF Gain: 14.46, Critic Loss: 0.04, Policy Loss: -1.32\n",
      "Beam: 2, Iter: 31001, Q: 1.5367, Reward pred: -1, Reward: -1, BF Gain pred: 135.45, BF Gain: 44.15, Critic Loss: 0.09, Policy Loss: -0.84\n",
      "Beam: 3, Iter: 31001, Q: 1.7099, Reward pred: -1, Reward: -1, BF Gain pred: 217.19, BF Gain: 83.58, Critic Loss: 0.05, Policy Loss: -1.06\n",
      "Training for 500 iteration for each Beam uses 23.403635501861572 seconds.\n",
      "Beam: 0, Iter: 31501, Q: 1.6634, Reward pred: -1, Reward: -1, BF Gain pred: 163.11, BF Gain: 68.48, Critic Loss: 0.08, Policy Loss: -1.23\n",
      "Beam: 1, Iter: 31501, Q: 1.4518, Reward pred: 1, Reward: -1, BF Gain pred: 95.64, BF Gain: 26.69, Critic Loss: 0.05, Policy Loss: -1.34\n",
      "Beam: 2, Iter: 31501, Q: 1.3912, Reward pred: -1, Reward: -1, BF Gain pred: 127.55, BF Gain: 43.91, Critic Loss: 0.08, Policy Loss: -0.86\n",
      "Beam: 3, Iter: 31501, Q: 1.4360, Reward pred: -1, Reward: -1, BF Gain pred: 211.95, BF Gain: 97.73, Critic Loss: 0.04, Policy Loss: -1.16\n",
      "Training for 500 iteration for each Beam uses 23.4165940284729 seconds.\n",
      "Beam: 0, Iter: 32001, Q: 1.6993, Reward pred: -1, Reward: -1, BF Gain pred: 147.07, BF Gain: 44.65, Critic Loss: 0.17, Policy Loss: -1.14\n",
      "Beam: 1, Iter: 32001, Q: 1.7470, Reward pred: -1, Reward: -1, BF Gain pred: 80.59, BF Gain: 30.40, Critic Loss: 0.04, Policy Loss: -1.26\n",
      "Beam: 2, Iter: 32001, Q: 1.3622, Reward pred: -1, Reward: -1, BF Gain pred: 144.01, BF Gain: 66.30, Critic Loss: 0.09, Policy Loss: -0.88\n",
      "Beam: 3, Iter: 32001, Q: 1.5138, Reward pred: 1, Reward: -1, BF Gain pred: 218.22, BF Gain: 99.27, Critic Loss: 0.05, Policy Loss: -1.16\n",
      "Training for 500 iteration for each Beam uses 23.66344904899597 seconds.\n",
      "Beam: 0, Iter: 32501, Q: 1.3987, Reward pred: -1, Reward: -1, BF Gain pred: 168.99, BF Gain: 61.27, Critic Loss: 0.08, Policy Loss: -1.18\n",
      "Beam: 1, Iter: 32501, Q: 1.4954, Reward pred: 1, Reward: -1, BF Gain pred: 95.89, BF Gain: 36.30, Critic Loss: 0.04, Policy Loss: -1.39\n",
      "Beam: 2, Iter: 32501, Q: 1.3975, Reward pred: 1, Reward: -1, BF Gain pred: 142.70, BF Gain: 46.10, Critic Loss: 0.09, Policy Loss: -0.89\n",
      "Beam: 3, Iter: 32501, Q: 1.2892, Reward pred: 1, Reward: -1, BF Gain pred: 215.20, BF Gain: 72.30, Critic Loss: 0.05, Policy Loss: -1.13\n",
      "Training for 500 iteration for each Beam uses 23.443299293518066 seconds.\n",
      "Beam: 0, Iter: 33001, Q: 1.8811, Reward pred: -1, Reward: -1, BF Gain pred: 163.42, BF Gain: 63.83, Critic Loss: 0.07, Policy Loss: -1.23\n",
      "Beam: 1, Iter: 33001, Q: 1.4454, Reward pred: 1, Reward: -1, BF Gain pred: 73.65, BF Gain: 31.93, Critic Loss: 0.03, Policy Loss: -1.32\n",
      "Beam: 2, Iter: 33001, Q: 1.4383, Reward pred: -1, Reward: -1, BF Gain pred: 127.02, BF Gain: 56.97, Critic Loss: 0.08, Policy Loss: -0.84\n",
      "Beam: 3, Iter: 33001, Q: 1.4547, Reward pred: -1, Reward: -1, BF Gain pred: 228.99, BF Gain: 60.76, Critic Loss: 0.06, Policy Loss: -1.14\n",
      "Training for 500 iteration for each Beam uses 23.19862151145935 seconds.\n",
      "Beam: 0, Iter: 33501, Q: 1.4348, Reward pred: 1, Reward: -1, BF Gain pred: 179.93, BF Gain: 69.16, Critic Loss: 0.08, Policy Loss: -1.33\n",
      "Beam: 1, Iter: 33501, Q: 1.5227, Reward pred: -1, Reward: -1, BF Gain pred: 77.04, BF Gain: 32.41, Critic Loss: 0.04, Policy Loss: -1.32\n",
      "Beam: 2, Iter: 33501, Q: 1.4774, Reward pred: -1, Reward: -1, BF Gain pred: 136.53, BF Gain: 75.55, Critic Loss: 0.08, Policy Loss: -0.87\n",
      "Beam: 3, Iter: 33501, Q: 1.5835, Reward pred: -1, Reward: -1, BF Gain pred: 230.90, BF Gain: 129.04, Critic Loss: 0.06, Policy Loss: -1.12\n",
      "Training for 500 iteration for each Beam uses 23.650201320648193 seconds.\n",
      "Beam: 0, Iter: 34001, Q: 1.6698, Reward pred: -1, Reward: -1, BF Gain pred: 166.18, BF Gain: 103.25, Critic Loss: 0.06, Policy Loss: -1.33\n",
      "Beam: 1, Iter: 34001, Q: 1.5363, Reward pred: 1, Reward: -1, BF Gain pred: 81.85, BF Gain: 33.55, Critic Loss: 0.05, Policy Loss: -1.28\n",
      "Beam: 2, Iter: 34001, Q: 1.4502, Reward pred: 1, Reward: -1, BF Gain pred: 136.52, BF Gain: 27.23, Critic Loss: 0.09, Policy Loss: -0.87\n",
      "Beam: 3, Iter: 34001, Q: 1.5482, Reward pred: -1, Reward: -1, BF Gain pred: 226.36, BF Gain: 115.94, Critic Loss: 0.05, Policy Loss: -1.15\n",
      "Training for 500 iteration for each Beam uses 23.362465620040894 seconds.\n",
      "Beam: 0, Iter: 34501, Q: 1.8162, Reward pred: 1, Reward: -1, BF Gain pred: 145.52, BF Gain: 70.32, Critic Loss: 0.06, Policy Loss: -1.34\n",
      "Beam: 1, Iter: 34501, Q: 1.4928, Reward pred: -1, Reward: -1, BF Gain pred: 80.10, BF Gain: 35.01, Critic Loss: 0.04, Policy Loss: -1.25\n",
      "Beam: 2, Iter: 34501, Q: 0.6907, Reward pred: 1, Reward: -1, BF Gain pred: 125.15, BF Gain: 84.95, Critic Loss: 0.50, Policy Loss: -0.10\n",
      "Beam: 3, Iter: 34501, Q: 1.4276, Reward pred: 1, Reward: -1, BF Gain pred: 233.76, BF Gain: 152.03, Critic Loss: 0.05, Policy Loss: -1.17\n",
      "Training for 500 iteration for each Beam uses 23.164896726608276 seconds.\n",
      "Beam: 0, Iter: 35001, Q: 1.3711, Reward pred: -1, Reward: -1, BF Gain pred: 177.68, BF Gain: 63.29, Critic Loss: 0.06, Policy Loss: -1.33\n",
      "Beam: 1, Iter: 35001, Q: 1.4333, Reward pred: -1, Reward: -1, BF Gain pred: 88.52, BF Gain: 43.01, Critic Loss: 0.05, Policy Loss: -1.25\n",
      "Beam: 2, Iter: 35001, Q: 0.9617, Reward pred: -1, Reward: -1, BF Gain pred: 134.84, BF Gain: 62.93, Critic Loss: 0.28, Policy Loss: -0.26\n",
      "Beam: 3, Iter: 35001, Q: 1.5556, Reward pred: 1, Reward: -1, BF Gain pred: 260.23, BF Gain: 92.98, Critic Loss: 0.03, Policy Loss: -1.25\n",
      "Training for 500 iteration for each Beam uses 23.352023363113403 seconds.\n",
      "Beam: 0, Iter: 35501, Q: 1.4830, Reward pred: -1, Reward: -1, BF Gain pred: 165.34, BF Gain: 80.52, Critic Loss: 0.07, Policy Loss: -1.19\n",
      "Beam: 1, Iter: 35501, Q: 1.3679, Reward pred: -1, Reward: -1, BF Gain pred: 84.18, BF Gain: 45.72, Critic Loss: 0.03, Policy Loss: -1.38\n",
      "Beam: 2, Iter: 35501, Q: 1.2960, Reward pred: 1, Reward: -1, BF Gain pred: 141.51, BF Gain: 91.61, Critic Loss: 0.22, Policy Loss: -0.64\n",
      "Beam: 3, Iter: 35501, Q: 1.4674, Reward pred: -1, Reward: -1, BF Gain pred: 234.42, BF Gain: 85.68, Critic Loss: 0.04, Policy Loss: -1.23\n",
      "Training for 500 iteration for each Beam uses 23.58215856552124 seconds.\n",
      "Beam: 0, Iter: 36001, Q: 1.5226, Reward pred: 1, Reward: -1, BF Gain pred: 172.44, BF Gain: 84.18, Critic Loss: 0.06, Policy Loss: -1.19\n",
      "Beam: 1, Iter: 36001, Q: 1.5029, Reward pred: -1, Reward: -1, BF Gain pred: 84.96, BF Gain: 42.60, Critic Loss: 0.05, Policy Loss: -1.37\n",
      "Beam: 2, Iter: 36001, Q: 1.4999, Reward pred: -1, Reward: -1, BF Gain pred: 145.66, BF Gain: 71.68, Critic Loss: 0.19, Policy Loss: -0.71\n",
      "Beam: 3, Iter: 36001, Q: 1.5576, Reward pred: 1, Reward: -1, BF Gain pred: 261.72, BF Gain: 66.21, Critic Loss: 0.04, Policy Loss: -1.33\n",
      "Training for 500 iteration for each Beam uses 23.693533897399902 seconds.\n",
      "Beam: 0, Iter: 36501, Q: 1.7490, Reward pred: -1, Reward: -1, BF Gain pred: 145.65, BF Gain: 49.27, Critic Loss: 0.06, Policy Loss: -1.20\n",
      "Beam: 1, Iter: 36501, Q: 1.3435, Reward pred: -1, Reward: -1, BF Gain pred: 77.39, BF Gain: 27.37, Critic Loss: 0.04, Policy Loss: -1.35\n",
      "Beam: 2, Iter: 36501, Q: 1.3916, Reward pred: 1, Reward: -1, BF Gain pred: 150.10, BF Gain: 108.84, Critic Loss: 0.17, Policy Loss: -0.81\n",
      "Beam: 3, Iter: 36501, Q: 1.6050, Reward pred: -1, Reward: -1, BF Gain pred: 256.81, BF Gain: 132.53, Critic Loss: 0.04, Policy Loss: -1.25\n",
      "Training for 500 iteration for each Beam uses 23.40776777267456 seconds.\n",
      "Beam: 0, Iter: 37001, Q: 1.4810, Reward pred: -1, Reward: -1, BF Gain pred: 156.41, BF Gain: 73.11, Critic Loss: 0.05, Policy Loss: -1.26\n",
      "Beam: 1, Iter: 37001, Q: 1.6690, Reward pred: 1, Reward: -1, BF Gain pred: 93.46, BF Gain: 40.79, Critic Loss: 0.04, Policy Loss: -1.33\n",
      "Beam: 2, Iter: 37001, Q: 1.7063, Reward pred: 1, Reward: -1, BF Gain pred: 145.34, BF Gain: 57.19, Critic Loss: 0.14, Policy Loss: -0.78\n",
      "Beam: 3, Iter: 37001, Q: 1.7870, Reward pred: -1, Reward: -1, BF Gain pred: 211.66, BF Gain: 91.96, Critic Loss: 0.05, Policy Loss: -1.21\n",
      "Training for 500 iteration for each Beam uses 23.40566349029541 seconds.\n",
      "Beam: 0, Iter: 37501, Q: 1.5447, Reward pred: -1, Reward: -1, BF Gain pred: 160.86, BF Gain: 83.03, Critic Loss: 0.07, Policy Loss: -1.29\n",
      "Beam: 1, Iter: 37501, Q: 1.8382, Reward pred: -1, Reward: -1, BF Gain pred: 93.69, BF Gain: 38.40, Critic Loss: 0.04, Policy Loss: -1.36\n",
      "Beam: 2, Iter: 37501, Q: 1.9836, Reward pred: 1, Reward: -1, BF Gain pred: 143.87, BF Gain: 67.46, Critic Loss: 0.12, Policy Loss: -0.80\n",
      "Beam: 3, Iter: 37501, Q: 1.9853, Reward pred: 1, Reward: -1, BF Gain pred: 230.39, BF Gain: 94.34, Critic Loss: 0.05, Policy Loss: -1.26\n",
      "Training for 500 iteration for each Beam uses 23.63654088973999 seconds.\n",
      "Beam: 0, Iter: 38001, Q: 1.5976, Reward pred: -1, Reward: -1, BF Gain pred: 175.54, BF Gain: 133.37, Critic Loss: 0.06, Policy Loss: -1.36\n",
      "Beam: 1, Iter: 38001, Q: 2.6744, Reward pred: 1, Reward: -1, BF Gain pred: 90.42, BF Gain: 43.88, Critic Loss: 0.05, Policy Loss: -1.31\n",
      "Beam: 2, Iter: 38001, Q: 1.4444, Reward pred: -1, Reward: -1, BF Gain pred: 146.55, BF Gain: 73.04, Critic Loss: 0.14, Policy Loss: -0.79\n",
      "Beam: 3, Iter: 38001, Q: 1.4789, Reward pred: -1, Reward: -1, BF Gain pred: 241.70, BF Gain: 121.39, Critic Loss: 0.05, Policy Loss: -1.20\n",
      "Training for 500 iteration for each Beam uses 24.142359733581543 seconds.\n",
      "Beam: 0, Iter: 38501, Q: 1.7152, Reward pred: -1, Reward: -1, BF Gain pred: 154.01, BF Gain: 65.75, Critic Loss: 0.06, Policy Loss: -1.17\n",
      "Beam: 1, Iter: 38501, Q: 1.3286, Reward pred: 1, Reward: -1, BF Gain pred: 95.65, BF Gain: 58.36, Critic Loss: 0.03, Policy Loss: -1.28\n",
      "Beam: 2, Iter: 38501, Q: 1.5189, Reward pred: -1, Reward: -1, BF Gain pred: 138.09, BF Gain: 68.04, Critic Loss: 0.11, Policy Loss: -0.70\n",
      "Beam: 3, Iter: 38501, Q: 1.4480, Reward pred: -1, Reward: -1, BF Gain pred: 216.06, BF Gain: 129.04, Critic Loss: 0.05, Policy Loss: -1.18\n",
      "Training for 500 iteration for each Beam uses 23.693439245224 seconds.\n",
      "Beam: 0, Iter: 39001, Q: 1.4492, Reward pred: 1, Reward: -1, BF Gain pred: 149.22, BF Gain: 82.84, Critic Loss: 0.05, Policy Loss: -1.42\n",
      "Beam: 1, Iter: 39001, Q: 1.4753, Reward pred: 1, Reward: -1, BF Gain pred: 99.71, BF Gain: 69.37, Critic Loss: 0.04, Policy Loss: -1.29\n",
      "Beam: 2, Iter: 39001, Q: 1.3598, Reward pred: -1, Reward: -1, BF Gain pred: 147.49, BF Gain: 100.75, Critic Loss: 0.28, Policy Loss: -0.40\n",
      "Beam: 3, Iter: 39001, Q: 1.3794, Reward pred: 1, Reward: -1, BF Gain pred: 256.63, BF Gain: 155.76, Critic Loss: 0.04, Policy Loss: -1.16\n",
      "Training for 500 iteration for each Beam uses 24.169461727142334 seconds.\n",
      "Beam: 0, Iter: 39501, Q: 1.3312, Reward pred: 1, Reward: -1, BF Gain pred: 173.93, BF Gain: 80.08, Critic Loss: 0.07, Policy Loss: -1.59\n",
      "Beam: 1, Iter: 39501, Q: 1.5530, Reward pred: -1, Reward: -1, BF Gain pred: 78.69, BF Gain: 29.00, Critic Loss: 0.05, Policy Loss: -1.26\n",
      "Beam: 2, Iter: 39501, Q: 1.7682, Reward pred: -1, Reward: -1, BF Gain pred: 144.04, BF Gain: 78.69, Critic Loss: 0.17, Policy Loss: -0.67\n",
      "Beam: 3, Iter: 39501, Q: 1.4548, Reward pred: 1, Reward: -1, BF Gain pred: 240.43, BF Gain: 118.78, Critic Loss: 0.06, Policy Loss: -1.15\n",
      "Training for 500 iteration for each Beam uses 23.17652177810669 seconds.\n",
      "Beam: 0, Iter: 40001, Q: 1.9062, Reward pred: -1, Reward: -1, BF Gain pred: 158.42, BF Gain: 71.39, Critic Loss: 0.07, Policy Loss: -1.64\n",
      "Beam: 1, Iter: 40001, Q: 1.5503, Reward pred: 1, Reward: -1, BF Gain pred: 95.73, BF Gain: 45.54, Critic Loss: 0.06, Policy Loss: -1.28\n",
      "Beam: 2, Iter: 40001, Q: 1.5040, Reward pred: 1, Reward: -1, BF Gain pred: 144.10, BF Gain: 78.93, Critic Loss: 0.15, Policy Loss: -0.71\n",
      "Beam: 3, Iter: 40001, Q: 1.3766, Reward pred: -1, Reward: -1, BF Gain pred: 249.00, BF Gain: 109.52, Critic Loss: 0.06, Policy Loss: -1.17\n",
      "Training for 500 iteration for each Beam uses 23.515411853790283 seconds.\n",
      "Beam: 0, Iter: 40501, Q: 1.6620, Reward pred: -1, Reward: -1, BF Gain pred: 158.14, BF Gain: 106.43, Critic Loss: 0.08, Policy Loss: -1.59\n",
      "Beam: 1, Iter: 40501, Q: 1.7383, Reward pred: -1, Reward: -1, BF Gain pred: 81.51, BF Gain: 18.51, Critic Loss: 0.04, Policy Loss: -1.25\n",
      "Beam: 2, Iter: 40501, Q: 1.6858, Reward pred: 1, Reward: -1, BF Gain pred: 144.53, BF Gain: 70.94, Critic Loss: 0.11, Policy Loss: -0.88\n",
      "Beam: 3, Iter: 40501, Q: 1.6386, Reward pred: 1, Reward: -1, BF Gain pred: 253.90, BF Gain: 151.85, Critic Loss: 0.06, Policy Loss: -1.14\n",
      "Training for 500 iteration for each Beam uses 23.301150798797607 seconds.\n",
      "Beam: 0, Iter: 41001, Q: 1.4997, Reward pred: 1, Reward: -1, BF Gain pred: 153.75, BF Gain: 105.82, Critic Loss: 0.09, Policy Loss: -1.48\n",
      "Beam: 1, Iter: 41001, Q: 1.5608, Reward pred: 1, Reward: -1, BF Gain pred: 80.75, BF Gain: 23.29, Critic Loss: 0.04, Policy Loss: -1.37\n",
      "Beam: 2, Iter: 41001, Q: 1.2814, Reward pred: -1, Reward: -1, BF Gain pred: 139.32, BF Gain: 39.55, Critic Loss: 0.27, Policy Loss: -0.63\n",
      "Beam: 3, Iter: 41001, Q: 1.5295, Reward pred: -1, Reward: -1, BF Gain pred: 225.74, BF Gain: 79.42, Critic Loss: 0.06, Policy Loss: -1.09\n",
      "Training for 500 iteration for each Beam uses 23.70478320121765 seconds.\n",
      "Beam: 0, Iter: 41501, Q: 1.8894, Reward pred: -1, Reward: -1, BF Gain pred: 165.16, BF Gain: 100.84, Critic Loss: 0.07, Policy Loss: -1.48\n",
      "Beam: 1, Iter: 41501, Q: 1.6835, Reward pred: -1, Reward: -1, BF Gain pred: 91.21, BF Gain: 38.23, Critic Loss: 0.07, Policy Loss: -1.23\n",
      "Beam: 2, Iter: 41501, Q: 0.5029, Reward pred: -1, Reward: -1, BF Gain pred: 141.54, BF Gain: 115.27, Critic Loss: 0.43, Policy Loss: -0.07\n",
      "Beam: 3, Iter: 41501, Q: 1.5554, Reward pred: 1, Reward: -1, BF Gain pred: 228.41, BF Gain: 107.70, Critic Loss: 0.05, Policy Loss: -1.21\n",
      "Training for 500 iteration for each Beam uses 23.87782311439514 seconds.\n",
      "Beam: 0, Iter: 42001, Q: 2.5109, Reward pred: 1, Reward: -1, BF Gain pred: 165.27, BF Gain: 93.06, Critic Loss: 0.06, Policy Loss: -1.52\n",
      "Beam: 1, Iter: 42001, Q: 1.5451, Reward pred: -1, Reward: -1, BF Gain pred: 89.46, BF Gain: 48.31, Critic Loss: 0.05, Policy Loss: -1.25\n",
      "Beam: 2, Iter: 42001, Q: 1.6668, Reward pred: -1, Reward: -1, BF Gain pred: 143.92, BF Gain: 57.56, Critic Loss: 0.16, Policy Loss: -0.72\n",
      "Beam: 3, Iter: 42001, Q: 1.4956, Reward pred: -1, Reward: -1, BF Gain pred: 258.20, BF Gain: 98.94, Critic Loss: 0.05, Policy Loss: -1.21\n",
      "Training for 500 iteration for each Beam uses 23.54256582260132 seconds.\n",
      "Beam: 0, Iter: 42501, Q: 1.6467, Reward pred: -1, Reward: -1, BF Gain pred: 169.47, BF Gain: 101.40, Critic Loss: 0.07, Policy Loss: -1.55\n",
      "Beam: 1, Iter: 42501, Q: 1.3541, Reward pred: 1, Reward: -1, BF Gain pred: 91.71, BF Gain: 50.62, Critic Loss: 0.04, Policy Loss: -1.30\n",
      "Beam: 2, Iter: 42501, Q: 1.4516, Reward pred: 1, Reward: -1, BF Gain pred: 150.23, BF Gain: 84.80, Critic Loss: 0.11, Policy Loss: -0.85\n",
      "Beam: 3, Iter: 42501, Q: 1.4983, Reward pred: 1, Reward: -1, BF Gain pred: 253.74, BF Gain: 159.54, Critic Loss: 0.06, Policy Loss: -1.29\n",
      "Training for 500 iteration for each Beam uses 25.032278776168823 seconds.\n",
      "Beam: 0, Iter: 43001, Q: 2.6299, Reward pred: 1, Reward: -1, BF Gain pred: 169.45, BF Gain: 53.55, Critic Loss: 0.09, Policy Loss: -1.56\n",
      "Beam: 1, Iter: 43001, Q: 1.6093, Reward pred: -1, Reward: -1, BF Gain pred: 88.14, BF Gain: 44.02, Critic Loss: 0.04, Policy Loss: -1.32\n",
      "Beam: 2, Iter: 43001, Q: 1.7496, Reward pred: -1, Reward: -1, BF Gain pred: 139.75, BF Gain: 81.80, Critic Loss: 0.12, Policy Loss: -0.87\n",
      "Beam: 3, Iter: 43001, Q: 1.3396, Reward pred: 1, Reward: -1, BF Gain pred: 241.10, BF Gain: 102.31, Critic Loss: 0.06, Policy Loss: -1.30\n",
      "Training for 500 iteration for each Beam uses 23.876725435256958 seconds.\n",
      "Beam: 0, Iter: 43501, Q: 1.9428, Reward pred: -1, Reward: -1, BF Gain pred: 148.98, BF Gain: 60.20, Critic Loss: 0.09, Policy Loss: -1.43\n",
      "Beam: 1, Iter: 43501, Q: 1.4373, Reward pred: -1, Reward: -1, BF Gain pred: 87.42, BF Gain: 60.04, Critic Loss: 0.05, Policy Loss: -1.32\n",
      "Beam: 2, Iter: 43501, Q: 1.4123, Reward pred: 1, Reward: -1, BF Gain pred: 141.91, BF Gain: 115.75, Critic Loss: 0.10, Policy Loss: -0.88\n",
      "Beam: 3, Iter: 43501, Q: 1.7893, Reward pred: 1, Reward: -1, BF Gain pred: 254.00, BF Gain: 118.04, Critic Loss: 0.05, Policy Loss: -1.17\n",
      "Training for 500 iteration for each Beam uses 23.87631607055664 seconds.\n",
      "Beam: 0, Iter: 44001, Q: 1.8546, Reward pred: -1, Reward: -1, BF Gain pred: 163.55, BF Gain: 70.17, Critic Loss: 0.07, Policy Loss: -1.30\n",
      "Beam: 1, Iter: 44001, Q: 1.3500, Reward pred: -1, Reward: -1, BF Gain pred: 94.70, BF Gain: 39.74, Critic Loss: 0.05, Policy Loss: -1.35\n",
      "Beam: 2, Iter: 44001, Q: 1.7501, Reward pred: 1, Reward: -1, BF Gain pred: 148.00, BF Gain: 91.13, Critic Loss: 0.11, Policy Loss: -0.86\n",
      "Beam: 3, Iter: 44001, Q: 1.9629, Reward pred: 1, Reward: -1, BF Gain pred: 255.71, BF Gain: 95.08, Critic Loss: 0.07, Policy Loss: -1.28\n",
      "Training for 500 iteration for each Beam uses 23.568434715270996 seconds.\n",
      "Beam: 0, Iter: 44501, Q: 1.6667, Reward pred: -1, Reward: -1, BF Gain pred: 171.84, BF Gain: 116.17, Critic Loss: 0.06, Policy Loss: -1.49\n",
      "Beam: 1, Iter: 44501, Q: 1.5735, Reward pred: 1, Reward: -1, BF Gain pred: 89.68, BF Gain: 46.57, Critic Loss: 0.03, Policy Loss: -1.38\n",
      "Beam: 2, Iter: 44501, Q: 1.7692, Reward pred: -1, Reward: -1, BF Gain pred: 146.51, BF Gain: 54.07, Critic Loss: 0.11, Policy Loss: -0.90\n",
      "Beam: 3, Iter: 44501, Q: 1.6275, Reward pred: -1, Reward: -1, BF Gain pred: 239.93, BF Gain: 106.98, Critic Loss: 0.06, Policy Loss: -1.29\n",
      "Training for 500 iteration for each Beam uses 24.19533085823059 seconds.\n",
      "Beam: 0, Iter: 45001, Q: 1.3906, Reward pred: 1, Reward: -1, BF Gain pred: 187.35, BF Gain: 81.44, Critic Loss: 0.06, Policy Loss: -1.56\n",
      "Beam: 1, Iter: 45001, Q: 1.3983, Reward pred: -1, Reward: -1, BF Gain pred: 94.45, BF Gain: 42.33, Critic Loss: 0.06, Policy Loss: -1.28\n",
      "Beam: 2, Iter: 45001, Q: 1.9321, Reward pred: -1, Reward: -1, BF Gain pred: 140.92, BF Gain: 86.70, Critic Loss: 0.11, Policy Loss: -0.86\n",
      "Beam: 3, Iter: 45001, Q: 2.7862, Reward pred: -1, Reward: -1, BF Gain pred: 230.13, BF Gain: 138.16, Critic Loss: 0.07, Policy Loss: -1.23\n",
      "Training for 500 iteration for each Beam uses 23.80977988243103 seconds.\n",
      "Beam: 0, Iter: 45501, Q: 1.6175, Reward pred: -1, Reward: -1, BF Gain pred: 157.15, BF Gain: 63.89, Critic Loss: 0.06, Policy Loss: -1.41\n",
      "Beam: 1, Iter: 45501, Q: 1.7438, Reward pred: -1, Reward: -1, BF Gain pred: 92.34, BF Gain: 37.20, Critic Loss: 0.05, Policy Loss: -1.26\n",
      "Beam: 2, Iter: 45501, Q: 1.6382, Reward pred: -1, Reward: -1, BF Gain pred: 144.63, BF Gain: 80.49, Critic Loss: 0.09, Policy Loss: -0.85\n",
      "Beam: 3, Iter: 45501, Q: 1.6767, Reward pred: 1, Reward: -1, BF Gain pred: 250.78, BF Gain: 194.86, Critic Loss: 0.06, Policy Loss: -1.32\n",
      "Training for 500 iteration for each Beam uses 23.78928565979004 seconds.\n",
      "Beam: 0, Iter: 46001, Q: 1.8268, Reward pred: -1, Reward: -1, BF Gain pred: 172.08, BF Gain: 70.85, Critic Loss: 0.06, Policy Loss: -1.42\n",
      "Beam: 1, Iter: 46001, Q: 1.4854, Reward pred: 1, Reward: -1, BF Gain pred: 91.02, BF Gain: 45.49, Critic Loss: 0.05, Policy Loss: -1.44\n",
      "Beam: 2, Iter: 46001, Q: 1.7176, Reward pred: 1, Reward: -1, BF Gain pred: 148.33, BF Gain: 122.96, Critic Loss: 0.10, Policy Loss: -0.90\n",
      "Beam: 3, Iter: 46001, Q: 1.4618, Reward pred: -1, Reward: -1, BF Gain pred: 236.11, BF Gain: 93.36, Critic Loss: 0.06, Policy Loss: -1.35\n",
      "Training for 500 iteration for each Beam uses 23.755971670150757 seconds.\n",
      "Beam: 0, Iter: 46501, Q: 2.9266, Reward pred: -1, Reward: -1, BF Gain pred: 161.82, BF Gain: 43.68, Critic Loss: 0.07, Policy Loss: -1.39\n",
      "Beam: 1, Iter: 46501, Q: 1.6139, Reward pred: -1, Reward: -1, BF Gain pred: 84.94, BF Gain: 35.75, Critic Loss: 0.05, Policy Loss: -1.40\n",
      "Beam: 2, Iter: 46501, Q: 1.8345, Reward pred: -1, Reward: -1, BF Gain pred: 145.30, BF Gain: 64.35, Critic Loss: 0.11, Policy Loss: -0.85\n",
      "Beam: 3, Iter: 46501, Q: 1.5486, Reward pred: -1, Reward: -1, BF Gain pred: 228.30, BF Gain: 151.43, Critic Loss: 0.06, Policy Loss: -1.37\n",
      "Training for 500 iteration for each Beam uses 26.7176251411438 seconds.\n",
      "Beam: 0, Iter: 47001, Q: 1.4251, Reward pred: -1, Reward: -1, BF Gain pred: 165.30, BF Gain: 130.45, Critic Loss: 0.07, Policy Loss: -1.41\n",
      "Beam: 1, Iter: 47001, Q: 1.6423, Reward pred: -1, Reward: -1, BF Gain pred: 82.46, BF Gain: 42.74, Critic Loss: 0.05, Policy Loss: -1.39\n",
      "Beam: 2, Iter: 47001, Q: 1.8506, Reward pred: -1, Reward: -1, BF Gain pred: 143.83, BF Gain: 69.23, Critic Loss: 0.15, Policy Loss: -0.93\n",
      "Beam: 3, Iter: 47001, Q: 1.6546, Reward pred: 1, Reward: -1, BF Gain pred: 257.32, BF Gain: 100.86, Critic Loss: 0.05, Policy Loss: -1.30\n",
      "Training for 500 iteration for each Beam uses 23.53556776046753 seconds.\n",
      "Beam: 0, Iter: 47501, Q: 1.5174, Reward pred: 1, Reward: -1, BF Gain pred: 177.24, BF Gain: 86.76, Critic Loss: 0.07, Policy Loss: -1.36\n",
      "Beam: 1, Iter: 47501, Q: 1.5659, Reward pred: 1, Reward: -1, BF Gain pred: 91.21, BF Gain: 49.93, Critic Loss: 0.04, Policy Loss: -1.46\n",
      "Beam: 2, Iter: 47501, Q: 2.2579, Reward pred: 1, Reward: -1, BF Gain pred: 147.97, BF Gain: 59.45, Critic Loss: 0.13, Policy Loss: -0.96\n",
      "Beam: 3, Iter: 47501, Q: 1.4359, Reward pred: 1, Reward: -1, BF Gain pred: 239.95, BF Gain: 104.76, Critic Loss: 0.06, Policy Loss: -1.27\n",
      "Training for 500 iteration for each Beam uses 23.758671283721924 seconds.\n",
      "Beam: 0, Iter: 48001, Q: 2.1404, Reward pred: 1, Reward: -1, BF Gain pred: 173.25, BF Gain: 101.49, Critic Loss: 0.08, Policy Loss: -1.34\n",
      "Beam: 1, Iter: 48001, Q: 1.7970, Reward pred: 1, Reward: -1, BF Gain pred: 93.00, BF Gain: 39.39, Critic Loss: 0.03, Policy Loss: -1.42\n",
      "Beam: 2, Iter: 48001, Q: 1.7335, Reward pred: 1, Reward: -1, BF Gain pred: 147.88, BF Gain: 110.49, Critic Loss: 0.10, Policy Loss: -1.02\n",
      "Beam: 3, Iter: 48001, Q: 1.6549, Reward pred: 1, Reward: -1, BF Gain pred: 268.89, BF Gain: 162.40, Critic Loss: 0.06, Policy Loss: -1.36\n",
      "Training for 500 iteration for each Beam uses 23.669762134552002 seconds.\n",
      "Beam: 0, Iter: 48501, Q: 1.6996, Reward pred: -1, Reward: -1, BF Gain pred: 184.83, BF Gain: 101.27, Critic Loss: 0.06, Policy Loss: -1.35\n",
      "Beam: 1, Iter: 48501, Q: 1.4814, Reward pred: -1, Reward: -1, BF Gain pred: 85.08, BF Gain: 45.94, Critic Loss: 0.06, Policy Loss: -1.31\n",
      "Beam: 2, Iter: 48501, Q: 1.8531, Reward pred: 1, Reward: -1, BF Gain pred: 149.18, BF Gain: 72.70, Critic Loss: 0.12, Policy Loss: -1.12\n",
      "Beam: 3, Iter: 48501, Q: 1.7006, Reward pred: -1, Reward: -1, BF Gain pred: 261.55, BF Gain: 136.32, Critic Loss: 0.06, Policy Loss: -1.21\n",
      "Training for 500 iteration for each Beam uses 24.792067766189575 seconds.\n",
      "Beam: 0, Iter: 49001, Q: 1.5583, Reward pred: -1, Reward: -1, BF Gain pred: 164.13, BF Gain: 103.33, Critic Loss: 0.07, Policy Loss: -1.25\n",
      "Beam: 1, Iter: 49001, Q: 1.5823, Reward pred: -1, Reward: -1, BF Gain pred: 95.23, BF Gain: 46.15, Critic Loss: 0.05, Policy Loss: -1.26\n",
      "Beam: 2, Iter: 49001, Q: 1.9258, Reward pred: -1, Reward: -1, BF Gain pred: 139.09, BF Gain: 68.33, Critic Loss: 0.12, Policy Loss: -1.13\n",
      "Beam: 3, Iter: 49001, Q: 1.5179, Reward pred: 1, Reward: -1, BF Gain pred: 253.79, BF Gain: 134.06, Critic Loss: 0.06, Policy Loss: -1.30\n",
      "Training for 500 iteration for each Beam uses 24.405298948287964 seconds.\n",
      "Beam: 0, Iter: 49501, Q: 2.0648, Reward pred: -1, Reward: -1, BF Gain pred: 157.70, BF Gain: 79.51, Critic Loss: 0.06, Policy Loss: -1.49\n",
      "Beam: 1, Iter: 49501, Q: 1.3989, Reward pred: 1, Reward: -1, BF Gain pred: 98.03, BF Gain: 61.20, Critic Loss: 0.05, Policy Loss: -1.32\n",
      "Beam: 2, Iter: 49501, Q: 1.9420, Reward pred: 1, Reward: -1, BF Gain pred: 146.76, BF Gain: 104.96, Critic Loss: 0.12, Policy Loss: -1.08\n",
      "Beam: 3, Iter: 49501, Q: 1.4737, Reward pred: -1, Reward: -1, BF Gain pred: 252.88, BF Gain: 156.64, Critic Loss: 0.07, Policy Loss: -1.31\n",
      "Training for 500 iteration for each Beam uses 23.950351238250732 seconds.\n",
      "Beam: 0, Iter: 50001, Q: 1.5036, Reward pred: -1, Reward: -1, BF Gain pred: 181.64, BF Gain: 99.00, Critic Loss: 0.06, Policy Loss: -1.43\n",
      "Beam: 1, Iter: 50001, Q: 1.7136, Reward pred: 1, Reward: -1, BF Gain pred: 96.00, BF Gain: 52.12, Critic Loss: 0.04, Policy Loss: -1.28\n",
      "Beam: 2, Iter: 50001, Q: 1.5680, Reward pred: -1, Reward: -1, BF Gain pred: 146.68, BF Gain: 68.94, Critic Loss: 0.15, Policy Loss: -1.06\n",
      "Beam: 3, Iter: 50001, Q: 1.7703, Reward pred: -1, Reward: -1, BF Gain pred: 237.44, BF Gain: 158.61, Critic Loss: 0.06, Policy Loss: -1.31\n",
      "Training for 500 iteration for each Beam uses 23.955139636993408 seconds.\n",
      "Beam: 0, Iter: 50501, Q: 1.4524, Reward pred: 1, Reward: -1, BF Gain pred: 177.92, BF Gain: 119.58, Critic Loss: 0.08, Policy Loss: -1.43\n",
      "Beam: 1, Iter: 50501, Q: 1.5215, Reward pred: 1, Reward: -1, BF Gain pred: 87.27, BF Gain: 49.99, Critic Loss: 0.05, Policy Loss: -1.41\n",
      "Beam: 2, Iter: 50501, Q: 1.8885, Reward pred: 1, Reward: -1, BF Gain pred: 144.01, BF Gain: 90.35, Critic Loss: 0.13, Policy Loss: -1.05\n",
      "Beam: 3, Iter: 50501, Q: 1.3703, Reward pred: 1, Reward: -1, BF Gain pred: 254.55, BF Gain: 127.46, Critic Loss: 0.07, Policy Loss: -1.20\n",
      "Training for 500 iteration for each Beam uses 23.25452423095703 seconds.\n",
      "Beam: 0, Iter: 51001, Q: 1.5891, Reward pred: -1, Reward: -1, BF Gain pred: 169.12, BF Gain: 109.74, Critic Loss: 0.09, Policy Loss: -1.49\n",
      "Beam: 1, Iter: 51001, Q: 1.7355, Reward pred: -1, Reward: -1, BF Gain pred: 79.55, BF Gain: 28.80, Critic Loss: 0.06, Policy Loss: -1.32\n",
      "Beam: 2, Iter: 51001, Q: 1.8223, Reward pred: -1, Reward: -1, BF Gain pred: 144.44, BF Gain: 113.05, Critic Loss: 0.15, Policy Loss: -1.03\n",
      "Beam: 3, Iter: 51001, Q: 1.4606, Reward pred: -1, Reward: -1, BF Gain pred: 246.99, BF Gain: 148.64, Critic Loss: 0.06, Policy Loss: -1.22\n",
      "Training for 500 iteration for each Beam uses 23.75723695755005 seconds.\n",
      "Beam: 0, Iter: 51501, Q: 1.5773, Reward pred: 1, Reward: -1, BF Gain pred: 158.16, BF Gain: 109.99, Critic Loss: 0.08, Policy Loss: -1.33\n",
      "Beam: 1, Iter: 51501, Q: 1.3924, Reward pred: 1, Reward: -1, BF Gain pred: 100.93, BF Gain: 52.33, Critic Loss: 0.06, Policy Loss: -1.34\n",
      "Beam: 2, Iter: 51501, Q: 1.5727, Reward pred: -1, Reward: -1, BF Gain pred: 148.23, BF Gain: 61.60, Critic Loss: 0.12, Policy Loss: -0.96\n",
      "Beam: 3, Iter: 51501, Q: 1.5401, Reward pred: 1, Reward: -1, BF Gain pred: 259.06, BF Gain: 134.34, Critic Loss: 0.07, Policy Loss: -1.29\n",
      "Training for 500 iteration for each Beam uses 23.720038652420044 seconds.\n",
      "Beam: 0, Iter: 52001, Q: 1.7762, Reward pred: 1, Reward: -1, BF Gain pred: 194.75, BF Gain: 161.93, Critic Loss: 0.07, Policy Loss: -1.41\n",
      "Beam: 1, Iter: 52001, Q: 1.5413, Reward pred: 1, Reward: -1, BF Gain pred: 101.01, BF Gain: 49.90, Critic Loss: 0.06, Policy Loss: -1.32\n",
      "Beam: 2, Iter: 52001, Q: 1.4274, Reward pred: -1, Reward: -1, BF Gain pred: 143.77, BF Gain: 64.77, Critic Loss: 0.21, Policy Loss: -0.72\n",
      "Beam: 3, Iter: 52001, Q: 1.6613, Reward pred: -1, Reward: -1, BF Gain pred: 239.18, BF Gain: 114.07, Critic Loss: 0.07, Policy Loss: -1.34\n",
      "Training for 500 iteration for each Beam uses 24.01065969467163 seconds.\n",
      "Beam: 0, Iter: 52501, Q: 1.4386, Reward pred: 1, Reward: -1, BF Gain pred: 179.06, BF Gain: 101.26, Critic Loss: 0.08, Policy Loss: -1.39\n",
      "Beam: 1, Iter: 52501, Q: 1.5868, Reward pred: 1, Reward: -1, BF Gain pred: 93.46, BF Gain: 61.02, Critic Loss: 0.06, Policy Loss: -1.36\n",
      "Beam: 2, Iter: 52501, Q: 1.4320, Reward pred: -1, Reward: -1, BF Gain pred: 143.80, BF Gain: 96.78, Critic Loss: 0.18, Policy Loss: -0.78\n",
      "Beam: 3, Iter: 52501, Q: 1.9711, Reward pred: 1, Reward: -1, BF Gain pred: 265.07, BF Gain: 166.96, Critic Loss: 0.07, Policy Loss: -1.32\n",
      "Training for 500 iteration for each Beam uses 27.52781629562378 seconds.\n",
      "Beam: 0, Iter: 53001, Q: 1.4389, Reward pred: 1, Reward: -1, BF Gain pred: 168.15, BF Gain: 81.11, Critic Loss: 0.07, Policy Loss: -1.35\n",
      "Beam: 1, Iter: 53001, Q: 1.9136, Reward pred: 1, Reward: -1, BF Gain pred: 100.74, BF Gain: 59.40, Critic Loss: 0.06, Policy Loss: -1.37\n",
      "Beam: 2, Iter: 53001, Q: 1.5025, Reward pred: -1, Reward: -1, BF Gain pred: 148.02, BF Gain: 94.29, Critic Loss: 0.16, Policy Loss: -0.81\n",
      "Beam: 3, Iter: 53001, Q: 1.4774, Reward pred: 1, Reward: -1, BF Gain pred: 262.56, BF Gain: 161.84, Critic Loss: 0.09, Policy Loss: -1.33\n",
      "Training for 500 iteration for each Beam uses 25.82155704498291 seconds.\n",
      "Beam: 0, Iter: 53501, Q: 1.5755, Reward pred: 1, Reward: -1, BF Gain pred: 171.41, BF Gain: 130.03, Critic Loss: 0.08, Policy Loss: -1.33\n",
      "Beam: 1, Iter: 53501, Q: 1.4629, Reward pred: 1, Reward: -1, BF Gain pred: 100.57, BF Gain: 59.16, Critic Loss: 0.05, Policy Loss: -1.35\n",
      "Beam: 2, Iter: 53501, Q: 1.5996, Reward pred: 1, Reward: -1, BF Gain pred: 147.94, BF Gain: 117.04, Critic Loss: 0.15, Policy Loss: -0.83\n",
      "Beam: 3, Iter: 53501, Q: 1.3663, Reward pred: 1, Reward: -1, BF Gain pred: 251.88, BF Gain: 167.38, Critic Loss: 0.06, Policy Loss: -1.37\n",
      "Training for 500 iteration for each Beam uses 24.33223295211792 seconds.\n",
      "Beam: 0, Iter: 54001, Q: 2.0125, Reward pred: 1, Reward: -1, BF Gain pred: 191.85, BF Gain: 98.71, Critic Loss: 0.07, Policy Loss: -1.40\n",
      "Beam: 1, Iter: 54001, Q: 1.5974, Reward pred: -1, Reward: -1, BF Gain pred: 93.93, BF Gain: 50.50, Critic Loss: 0.06, Policy Loss: -1.34\n",
      "Beam: 2, Iter: 54001, Q: 1.5862, Reward pred: 1, Reward: -1, BF Gain pred: 148.40, BF Gain: 127.05, Critic Loss: 0.14, Policy Loss: -0.71\n",
      "Beam: 3, Iter: 54001, Q: 1.5515, Reward pred: -1, Reward: -1, BF Gain pred: 243.29, BF Gain: 140.95, Critic Loss: 0.06, Policy Loss: -1.32\n",
      "Training for 500 iteration for each Beam uses 23.944704055786133 seconds.\n",
      "Beam: 0, Iter: 54501, Q: 1.6422, Reward pred: 1, Reward: -1, BF Gain pred: 181.49, BF Gain: 121.58, Critic Loss: 0.08, Policy Loss: -1.35\n",
      "Beam: 1, Iter: 54501, Q: 1.8799, Reward pred: -1, Reward: -1, BF Gain pred: 93.76, BF Gain: 49.34, Critic Loss: 0.06, Policy Loss: -1.27\n",
      "Beam: 2, Iter: 54501, Q: 1.4345, Reward pred: 1, Reward: -1, BF Gain pred: 148.69, BF Gain: 93.93, Critic Loss: 0.15, Policy Loss: -0.77\n",
      "Beam: 3, Iter: 54501, Q: 1.5260, Reward pred: -1, Reward: -1, BF Gain pred: 231.13, BF Gain: 171.21, Critic Loss: 0.07, Policy Loss: -1.28\n",
      "Training for 500 iteration for each Beam uses 24.00500726699829 seconds.\n",
      "Beam: 0, Iter: 55001, Q: 1.3794, Reward pred: 1, Reward: -1, BF Gain pred: 186.59, BF Gain: 136.90, Critic Loss: 0.07, Policy Loss: -1.46\n",
      "Beam: 1, Iter: 55001, Q: 1.5947, Reward pred: -1, Reward: -1, BF Gain pred: 97.80, BF Gain: 58.26, Critic Loss: 0.06, Policy Loss: -1.39\n",
      "Beam: 2, Iter: 55001, Q: 2.0323, Reward pred: -1, Reward: -1, BF Gain pred: 149.83, BF Gain: 106.03, Critic Loss: 0.19, Policy Loss: -0.82\n",
      "Beam: 3, Iter: 55001, Q: 1.4215, Reward pred: -1, Reward: -1, BF Gain pred: 235.90, BF Gain: 124.13, Critic Loss: 0.07, Policy Loss: -1.35\n",
      "Training for 500 iteration for each Beam uses 24.190552234649658 seconds.\n",
      "Beam: 0, Iter: 55501, Q: 1.4961, Reward pred: 1, Reward: -1, BF Gain pred: 180.99, BF Gain: 123.17, Critic Loss: 0.08, Policy Loss: -1.27\n",
      "Beam: 1, Iter: 55501, Q: 1.7222, Reward pred: -1, Reward: -1, BF Gain pred: 98.37, BF Gain: 54.42, Critic Loss: 0.07, Policy Loss: -1.39\n",
      "Beam: 2, Iter: 55501, Q: 1.4400, Reward pred: -1, Reward: -1, BF Gain pred: 145.96, BF Gain: 91.90, Critic Loss: 0.21, Policy Loss: -0.72\n",
      "Beam: 3, Iter: 55501, Q: 1.5688, Reward pred: 1, Reward: -1, BF Gain pred: 227.21, BF Gain: 114.23, Critic Loss: 0.08, Policy Loss: -1.35\n",
      "Training for 500 iteration for each Beam uses 24.464498043060303 seconds.\n",
      "Beam: 0, Iter: 56001, Q: 1.4070, Reward pred: 1, Reward: -1, BF Gain pred: 184.37, BF Gain: 96.34, Critic Loss: 0.07, Policy Loss: -1.39\n",
      "Beam: 1, Iter: 56001, Q: 1.6778, Reward pred: 1, Reward: -1, BF Gain pred: 94.18, BF Gain: 59.30, Critic Loss: 0.07, Policy Loss: -1.38\n",
      "Beam: 2, Iter: 56001, Q: 1.8610, Reward pred: 1, Reward: -1, BF Gain pred: 149.74, BF Gain: 104.51, Critic Loss: 0.18, Policy Loss: -0.95\n",
      "Beam: 3, Iter: 56001, Q: 1.6913, Reward pred: 1, Reward: -1, BF Gain pred: 253.27, BF Gain: 184.91, Critic Loss: 0.08, Policy Loss: -1.30\n",
      "Training for 500 iteration for each Beam uses 23.640395879745483 seconds.\n",
      "Beam: 0, Iter: 56501, Q: 1.7081, Reward pred: -1, Reward: -1, BF Gain pred: 166.32, BF Gain: 110.42, Critic Loss: 0.10, Policy Loss: -1.45\n",
      "Beam: 1, Iter: 56501, Q: 1.5867, Reward pred: -1, Reward: -1, BF Gain pred: 99.04, BF Gain: 81.69, Critic Loss: 0.05, Policy Loss: -1.42\n",
      "Beam: 2, Iter: 56501, Q: 1.6631, Reward pred: -1, Reward: -1, BF Gain pred: 145.23, BF Gain: 100.47, Critic Loss: 0.14, Policy Loss: -1.06\n",
      "Beam: 3, Iter: 56501, Q: 1.7774, Reward pred: 1, Reward: -1, BF Gain pred: 244.29, BF Gain: 121.64, Critic Loss: 0.08, Policy Loss: -1.33\n",
      "Training for 500 iteration for each Beam uses 24.335612773895264 seconds.\n",
      "Beam: 0, Iter: 57001, Q: 1.6493, Reward pred: 1, Reward: -1, BF Gain pred: 166.15, BF Gain: 129.82, Critic Loss: 0.08, Policy Loss: -1.55\n",
      "Beam: 1, Iter: 57001, Q: 1.7199, Reward pred: -1, Reward: -1, BF Gain pred: 100.52, BF Gain: 56.92, Critic Loss: 0.06, Policy Loss: -1.43\n",
      "Beam: 2, Iter: 57001, Q: 1.8935, Reward pred: 1, Reward: -1, BF Gain pred: 147.43, BF Gain: 131.86, Critic Loss: 0.15, Policy Loss: -1.05\n",
      "Beam: 3, Iter: 57001, Q: 1.6108, Reward pred: 1, Reward: -1, BF Gain pred: 236.99, BF Gain: 145.92, Critic Loss: 0.09, Policy Loss: -1.26\n",
      "Training for 500 iteration for each Beam uses 28.305278539657593 seconds.\n",
      "Beam: 0, Iter: 57501, Q: 1.6488, Reward pred: -1, Reward: -1, BF Gain pred: 179.72, BF Gain: 153.58, Critic Loss: 0.09, Policy Loss: -1.56\n",
      "Beam: 1, Iter: 57501, Q: 1.8234, Reward pred: -1, Reward: -1, BF Gain pred: 97.34, BF Gain: 59.49, Critic Loss: 0.07, Policy Loss: -1.50\n",
      "Beam: 2, Iter: 57501, Q: 1.6968, Reward pred: -1, Reward: -1, BF Gain pred: 151.27, BF Gain: 97.38, Critic Loss: 0.18, Policy Loss: -1.06\n",
      "Beam: 3, Iter: 57501, Q: 1.9573, Reward pred: -1, Reward: -1, BF Gain pred: 241.51, BF Gain: 192.58, Critic Loss: 0.07, Policy Loss: -1.39\n",
      "Training for 500 iteration for each Beam uses 28.293450832366943 seconds.\n",
      "Beam: 0, Iter: 58001, Q: 1.7455, Reward pred: 1, Reward: -1, BF Gain pred: 176.17, BF Gain: 137.63, Critic Loss: 0.10, Policy Loss: -1.40\n",
      "Beam: 1, Iter: 58001, Q: 1.5035, Reward pred: -1, Reward: -1, BF Gain pred: 91.52, BF Gain: 54.68, Critic Loss: 0.05, Policy Loss: -1.47\n",
      "Beam: 2, Iter: 58001, Q: 1.3144, Reward pred: 1, Reward: -1, BF Gain pred: 145.79, BF Gain: 85.31, Critic Loss: 0.16, Policy Loss: -1.07\n",
      "Beam: 3, Iter: 58001, Q: 1.5872, Reward pred: -1, Reward: -1, BF Gain pred: 259.88, BF Gain: 156.91, Critic Loss: 0.09, Policy Loss: -1.74\n",
      "Training for 500 iteration for each Beam uses 23.68966245651245 seconds.\n",
      "Beam: 0, Iter: 58501, Q: 1.5673, Reward pred: 1, Reward: -1, BF Gain pred: 176.73, BF Gain: 116.26, Critic Loss: 0.09, Policy Loss: -1.34\n",
      "Beam: 1, Iter: 58501, Q: 1.7297, Reward pred: 1, Reward: -1, BF Gain pred: 95.70, BF Gain: 57.97, Critic Loss: 0.07, Policy Loss: -1.37\n",
      "Beam: 2, Iter: 58501, Q: 0.7460, Reward pred: -1, Reward: -1, BF Gain pred: 148.77, BF Gain: 99.14, Critic Loss: 0.46, Policy Loss: -0.24\n",
      "Beam: 3, Iter: 58501, Q: 1.4681, Reward pred: 1, Reward: -1, BF Gain pred: 247.94, BF Gain: 173.32, Critic Loss: 0.10, Policy Loss: -1.63\n",
      "Training for 500 iteration for each Beam uses 24.486807584762573 seconds.\n",
      "Beam: 0, Iter: 59001, Q: 1.6183, Reward pred: -1, Reward: -1, BF Gain pred: 171.79, BF Gain: 129.58, Critic Loss: 0.09, Policy Loss: -1.50\n",
      "Beam: 1, Iter: 59001, Q: 1.8409, Reward pred: 1, Reward: -1, BF Gain pred: 106.10, BF Gain: 44.29, Critic Loss: 0.08, Policy Loss: -1.36\n",
      "Beam: 2, Iter: 59001, Q: 1.4632, Reward pred: -1, Reward: -1, BF Gain pred: 146.04, BF Gain: 107.28, Critic Loss: 0.32, Policy Loss: -0.73\n",
      "Beam: 3, Iter: 59001, Q: 1.4639, Reward pred: -1, Reward: -1, BF Gain pred: 259.43, BF Gain: 184.87, Critic Loss: 0.10, Policy Loss: -1.40\n",
      "Training for 500 iteration for each Beam uses 25.06212854385376 seconds.\n",
      "Beam: 0, Iter: 59501, Q: 1.8237, Reward pred: 1, Reward: -1, BF Gain pred: 169.43, BF Gain: 126.03, Critic Loss: 0.09, Policy Loss: -1.73\n",
      "Beam: 1, Iter: 59501, Q: 1.6244, Reward pred: -1, Reward: -1, BF Gain pred: 100.56, BF Gain: 66.82, Critic Loss: 0.08, Policy Loss: -1.32\n",
      "Beam: 2, Iter: 59501, Q: 1.2326, Reward pred: -1, Reward: -1, BF Gain pred: 148.63, BF Gain: 112.40, Critic Loss: 0.27, Policy Loss: -0.86\n",
      "Beam: 3, Iter: 59501, Q: 1.5826, Reward pred: 1, Reward: -1, BF Gain pred: 263.55, BF Gain: 167.57, Critic Loss: 0.08, Policy Loss: -1.45\n",
      "Training for 500 iteration for each Beam uses 24.451366424560547 seconds.\n",
      "Beam: 0, Iter: 60001, Q: 1.6666, Reward pred: -1, Reward: -1, BF Gain pred: 147.90, BF Gain: 102.12, Critic Loss: 0.08, Policy Loss: -1.76\n",
      "Beam: 1, Iter: 60001, Q: 1.7741, Reward pred: -1, Reward: -1, BF Gain pred: 97.69, BF Gain: 59.79, Critic Loss: 0.08, Policy Loss: -1.41\n",
      "Beam: 2, Iter: 60001, Q: 1.3467, Reward pred: 1, Reward: -1, BF Gain pred: 151.04, BF Gain: 110.70, Critic Loss: 0.27, Policy Loss: -0.88\n",
      "Beam: 3, Iter: 60001, Q: 1.8450, Reward pred: -1, Reward: -1, BF Gain pred: 233.41, BF Gain: 106.30, Critic Loss: 0.12, Policy Loss: -1.41\n",
      "Training for 500 iteration for each Beam uses 24.249669790267944 seconds.\n",
      "Beam: 0, Iter: 60501, Q: 1.6666, Reward pred: -1, Reward: -1, BF Gain pred: 178.40, BF Gain: 95.19, Critic Loss: 0.08, Policy Loss: -1.56\n",
      "Beam: 1, Iter: 60501, Q: 1.6054, Reward pred: -1, Reward: -1, BF Gain pred: 98.18, BF Gain: 80.85, Critic Loss: 0.07, Policy Loss: -1.50\n",
      "Beam: 2, Iter: 60501, Q: 1.6202, Reward pred: -1, Reward: -1, BF Gain pred: 148.19, BF Gain: 99.25, Critic Loss: 0.28, Policy Loss: -0.48\n",
      "Beam: 3, Iter: 60501, Q: 1.4155, Reward pred: 1, Reward: -1, BF Gain pred: 262.71, BF Gain: 181.72, Critic Loss: 0.09, Policy Loss: -1.46\n",
      "Training for 500 iteration for each Beam uses 24.162439584732056 seconds.\n",
      "Beam: 0, Iter: 61001, Q: 1.7340, Reward pred: -1, Reward: -1, BF Gain pred: 160.07, BF Gain: 117.50, Critic Loss: 0.08, Policy Loss: -1.46\n",
      "Beam: 1, Iter: 61001, Q: 2.3904, Reward pred: -1, Reward: -1, BF Gain pred: 92.29, BF Gain: 74.98, Critic Loss: 0.08, Policy Loss: -1.67\n",
      "Beam: 2, Iter: 61001, Q: 1.5348, Reward pred: 1, Reward: -1, BF Gain pred: 149.03, BF Gain: 146.32, Critic Loss: 0.25, Policy Loss: -0.66\n",
      "Beam: 3, Iter: 61001, Q: 1.5343, Reward pred: -1, Reward: -1, BF Gain pred: 261.10, BF Gain: 168.49, Critic Loss: 0.12, Policy Loss: -1.22\n",
      "Training for 500 iteration for each Beam uses 24.062047719955444 seconds.\n",
      "Beam: 0, Iter: 61501, Q: 1.4843, Reward pred: -1, Reward: -1, BF Gain pred: 174.50, BF Gain: 116.01, Critic Loss: 0.10, Policy Loss: -1.57\n",
      "Beam: 1, Iter: 61501, Q: 2.1231, Reward pred: -1, Reward: -1, BF Gain pred: 99.50, BF Gain: 66.05, Critic Loss: 0.11, Policy Loss: -1.56\n",
      "Beam: 2, Iter: 61501, Q: 1.4174, Reward pred: -1, Reward: -1, BF Gain pred: 150.24, BF Gain: 97.06, Critic Loss: 0.26, Policy Loss: -0.81\n",
      "Beam: 3, Iter: 61501, Q: 1.7059, Reward pred: 1, Reward: -1, BF Gain pred: 275.11, BF Gain: 194.16, Critic Loss: 0.12, Policy Loss: -1.20\n",
      "Training for 500 iteration for each Beam uses 24.303639888763428 seconds.\n",
      "Beam: 0, Iter: 62001, Q: 2.3185, Reward pred: -1, Reward: -1, BF Gain pred: 179.08, BF Gain: 117.90, Critic Loss: 0.10, Policy Loss: -1.66\n",
      "Beam: 1, Iter: 62001, Q: 1.5413, Reward pred: -1, Reward: -1, BF Gain pred: 102.43, BF Gain: 55.77, Critic Loss: 0.07, Policy Loss: -1.63\n",
      "Beam: 2, Iter: 62001, Q: 1.4802, Reward pred: -1, Reward: -1, BF Gain pred: 147.90, BF Gain: 129.26, Critic Loss: 0.24, Policy Loss: -0.97\n",
      "Beam: 3, Iter: 62001, Q: 1.9999, Reward pred: -1, Reward: -1, BF Gain pred: 246.09, BF Gain: 190.81, Critic Loss: 0.13, Policy Loss: -1.32\n",
      "Training for 500 iteration for each Beam uses 24.15101647377014 seconds.\n",
      "Beam: 0, Iter: 62501, Q: 1.5668, Reward pred: -1, Reward: -1, BF Gain pred: 177.74, BF Gain: 139.78, Critic Loss: 0.10, Policy Loss: -1.51\n",
      "Beam: 1, Iter: 62501, Q: 1.6480, Reward pred: -1, Reward: -1, BF Gain pred: 98.71, BF Gain: 67.74, Critic Loss: 0.09, Policy Loss: -1.45\n",
      "Beam: 2, Iter: 62501, Q: 1.6651, Reward pred: 1, Reward: -1, BF Gain pred: 149.29, BF Gain: 126.91, Critic Loss: 0.23, Policy Loss: -0.99\n",
      "Beam: 3, Iter: 62501, Q: 1.7033, Reward pred: 1, Reward: -1, BF Gain pred: 271.03, BF Gain: 191.52, Critic Loss: 0.13, Policy Loss: -1.26\n",
      "Training for 500 iteration for each Beam uses 24.560500860214233 seconds.\n",
      "Beam: 0, Iter: 63001, Q: 1.7894, Reward pred: 1, Reward: -1, BF Gain pred: 188.31, BF Gain: 123.21, Critic Loss: 0.10, Policy Loss: -1.56\n",
      "Beam: 1, Iter: 63001, Q: 1.5295, Reward pred: -1, Reward: -1, BF Gain pred: 93.82, BF Gain: 53.19, Critic Loss: 0.09, Policy Loss: -1.37\n",
      "Beam: 2, Iter: 63001, Q: 1.3225, Reward pred: -1, Reward: -1, BF Gain pred: 145.65, BF Gain: 127.93, Critic Loss: 0.26, Policy Loss: -0.65\n",
      "Beam: 3, Iter: 63001, Q: 1.5753, Reward pred: 1, Reward: -1, BF Gain pred: 282.95, BF Gain: 236.77, Critic Loss: 0.09, Policy Loss: -1.21\n",
      "Training for 500 iteration for each Beam uses 24.078614234924316 seconds.\n",
      "Beam: 0, Iter: 63501, Q: 2.0486, Reward pred: -1, Reward: -1, BF Gain pred: 187.66, BF Gain: 136.63, Critic Loss: 0.12, Policy Loss: -1.57\n",
      "Beam: 1, Iter: 63501, Q: 1.5656, Reward pred: -1, Reward: -1, BF Gain pred: 103.76, BF Gain: 68.15, Critic Loss: 0.08, Policy Loss: -1.43\n",
      "Beam: 2, Iter: 63501, Q: 1.7786, Reward pred: -1, Reward: -1, BF Gain pred: 147.24, BF Gain: 91.04, Critic Loss: 0.26, Policy Loss: -0.80\n",
      "Beam: 3, Iter: 63501, Q: 2.1474, Reward pred: 1, Reward: -1, BF Gain pred: 264.68, BF Gain: 168.68, Critic Loss: 0.11, Policy Loss: -1.30\n",
      "Training for 500 iteration for each Beam uses 24.473340034484863 seconds.\n",
      "Beam: 0, Iter: 64001, Q: 1.5965, Reward pred: 1, Reward: -1, BF Gain pred: 196.20, BF Gain: 136.06, Critic Loss: 0.11, Policy Loss: -1.53\n",
      "Beam: 1, Iter: 64001, Q: 1.6199, Reward pred: 1, Reward: -1, BF Gain pred: 101.62, BF Gain: 65.06, Critic Loss: 0.12, Policy Loss: -1.29\n",
      "Beam: 2, Iter: 64001, Q: 1.6153, Reward pred: 1, Reward: -1, BF Gain pred: 149.91, BF Gain: 137.29, Critic Loss: 0.22, Policy Loss: -0.97\n",
      "Beam: 3, Iter: 64001, Q: 1.8200, Reward pred: -1, Reward: -1, BF Gain pred: 250.15, BF Gain: 176.20, Critic Loss: 0.11, Policy Loss: -1.34\n",
      "Training for 500 iteration for each Beam uses 24.19217610359192 seconds.\n",
      "Beam: 0, Iter: 64501, Q: 1.8927, Reward pred: 1, Reward: -1, BF Gain pred: 196.28, BF Gain: 146.02, Critic Loss: 0.11, Policy Loss: -1.33\n",
      "Beam: 1, Iter: 64501, Q: 1.7859, Reward pred: -1, Reward: -1, BF Gain pred: 100.85, BF Gain: 78.23, Critic Loss: 0.11, Policy Loss: -1.27\n",
      "Beam: 2, Iter: 64501, Q: 1.4261, Reward pred: 1, Reward: -1, BF Gain pred: 145.43, BF Gain: 109.99, Critic Loss: 0.26, Policy Loss: -1.00\n",
      "Beam: 3, Iter: 64501, Q: 1.6715, Reward pred: 1, Reward: -1, BF Gain pred: 278.31, BF Gain: 191.07, Critic Loss: 0.09, Policy Loss: -1.31\n",
      "Training for 500 iteration for each Beam uses 24.4612238407135 seconds.\n",
      "Beam: 0, Iter: 65001, Q: 2.0642, Reward pred: 1, Reward: -1, BF Gain pred: 153.70, BF Gain: 147.36, Critic Loss: 0.11, Policy Loss: -1.43\n",
      "Beam: 1, Iter: 65001, Q: 1.7782, Reward pred: 1, Reward: -1, BF Gain pred: 108.21, BF Gain: 85.99, Critic Loss: 0.11, Policy Loss: -1.31\n",
      "Beam: 2, Iter: 65001, Q: 1.6458, Reward pred: -1, Reward: -1, BF Gain pred: 152.09, BF Gain: 107.19, Critic Loss: 0.31, Policy Loss: -0.78\n",
      "Beam: 3, Iter: 65001, Q: 1.5582, Reward pred: -1, Reward: -1, BF Gain pred: 262.20, BF Gain: 212.31, Critic Loss: 0.11, Policy Loss: -1.26\n",
      "Training for 500 iteration for each Beam uses 24.61116337776184 seconds.\n",
      "Beam: 0, Iter: 65501, Q: 1.4573, Reward pred: 1, Reward: -1, BF Gain pred: 181.31, BF Gain: 146.15, Critic Loss: 0.09, Policy Loss: -1.59\n",
      "Beam: 1, Iter: 65501, Q: 1.7968, Reward pred: -1, Reward: -1, BF Gain pred: 101.12, BF Gain: 57.68, Critic Loss: 0.10, Policy Loss: -1.36\n",
      "Beam: 2, Iter: 65501, Q: 1.8448, Reward pred: -1, Reward: -1, BF Gain pred: 147.87, BF Gain: 123.63, Critic Loss: 0.28, Policy Loss: -0.96\n",
      "Beam: 3, Iter: 65501, Q: 1.6103, Reward pred: -1, Reward: -1, BF Gain pred: 256.43, BF Gain: 205.46, Critic Loss: 0.12, Policy Loss: -1.34\n",
      "Training for 500 iteration for each Beam uses 24.520185947418213 seconds.\n",
      "Beam: 0, Iter: 66001, Q: 1.9689, Reward pred: -1, Reward: -1, BF Gain pred: 177.62, BF Gain: 125.65, Critic Loss: 0.10, Policy Loss: -1.65\n",
      "Beam: 1, Iter: 66001, Q: 2.0282, Reward pred: 1, Reward: -1, BF Gain pred: 103.40, BF Gain: 75.73, Critic Loss: 0.12, Policy Loss: -1.24\n",
      "Beam: 2, Iter: 66001, Q: 1.9291, Reward pred: -1, Reward: -1, BF Gain pred: 147.76, BF Gain: 128.68, Critic Loss: 0.31, Policy Loss: -0.93\n",
      "Beam: 3, Iter: 66001, Q: 1.5292, Reward pred: 1, Reward: -1, BF Gain pred: 249.71, BF Gain: 149.04, Critic Loss: 0.13, Policy Loss: -1.24\n",
      "Training for 500 iteration for each Beam uses 24.125853538513184 seconds.\n",
      "Beam: 0, Iter: 66501, Q: 1.4703, Reward pred: -1, Reward: -1, BF Gain pred: 169.81, BF Gain: 114.67, Critic Loss: 0.12, Policy Loss: -1.70\n",
      "Beam: 1, Iter: 66501, Q: 1.8147, Reward pred: -1, Reward: -1, BF Gain pred: 100.65, BF Gain: 76.18, Critic Loss: 0.12, Policy Loss: -1.29\n",
      "Beam: 2, Iter: 66501, Q: 1.7336, Reward pred: -1, Reward: -1, BF Gain pred: 148.96, BF Gain: 114.46, Critic Loss: 0.26, Policy Loss: -1.07\n",
      "Beam: 3, Iter: 66501, Q: 1.5728, Reward pred: 1, Reward: -1, BF Gain pred: 279.74, BF Gain: 196.83, Critic Loss: 0.13, Policy Loss: -1.18\n",
      "Training for 500 iteration for each Beam uses 23.874870777130127 seconds.\n",
      "Beam: 0, Iter: 67001, Q: 1.6596, Reward pred: 1, Reward: -1, BF Gain pred: 171.78, BF Gain: 133.79, Critic Loss: 0.12, Policy Loss: -1.54\n",
      "Beam: 1, Iter: 67001, Q: 1.5589, Reward pred: -1, Reward: -1, BF Gain pred: 100.63, BF Gain: 71.82, Critic Loss: 0.13, Policy Loss: -1.18\n",
      "Beam: 2, Iter: 67001, Q: 2.1718, Reward pred: 1, Reward: -1, BF Gain pred: 154.81, BF Gain: 96.89, Critic Loss: 0.27, Policy Loss: -1.11\n",
      "Beam: 3, Iter: 67001, Q: 1.7316, Reward pred: -1, Reward: -1, BF Gain pred: 267.94, BF Gain: 206.38, Critic Loss: 0.14, Policy Loss: -1.23\n",
      "Training for 500 iteration for each Beam uses 23.901859760284424 seconds.\n",
      "Beam: 0, Iter: 67501, Q: 2.0933, Reward pred: 1, Reward: -1, BF Gain pred: 174.40, BF Gain: 140.47, Critic Loss: 0.12, Policy Loss: -1.85\n",
      "Beam: 1, Iter: 67501, Q: 1.7302, Reward pred: -1, Reward: -1, BF Gain pred: 101.51, BF Gain: 75.92, Critic Loss: 0.12, Policy Loss: -1.43\n",
      "Beam: 2, Iter: 67501, Q: 1.8738, Reward pred: -1, Reward: -1, BF Gain pred: 148.68, BF Gain: 107.82, Critic Loss: 0.25, Policy Loss: -1.04\n",
      "Beam: 3, Iter: 67501, Q: 1.6773, Reward pred: 1, Reward: -1, BF Gain pred: 280.05, BF Gain: 177.83, Critic Loss: 0.16, Policy Loss: -1.21\n",
      "Training for 500 iteration for each Beam uses 24.670576333999634 seconds.\n",
      "Beam: 0, Iter: 68001, Q: 1.5496, Reward pred: -1, Reward: -1, BF Gain pred: 180.77, BF Gain: 133.13, Critic Loss: 0.12, Policy Loss: -1.76\n",
      "Beam: 1, Iter: 68001, Q: 1.8251, Reward pred: 1, Reward: -1, BF Gain pred: 105.09, BF Gain: 82.58, Critic Loss: 0.13, Policy Loss: -1.25\n",
      "Beam: 2, Iter: 68001, Q: 1.9429, Reward pred: 1, Reward: -1, BF Gain pred: 152.00, BF Gain: 111.63, Critic Loss: 0.27, Policy Loss: -1.06\n",
      "Beam: 3, Iter: 68001, Q: 1.7000, Reward pred: 1, Reward: -1, BF Gain pred: 269.76, BF Gain: 199.93, Critic Loss: 0.14, Policy Loss: -1.21\n",
      "Training for 500 iteration for each Beam uses 24.552724838256836 seconds.\n",
      "Beam: 0, Iter: 68501, Q: 1.6612, Reward pred: 1, Reward: -1, BF Gain pred: 175.74, BF Gain: 162.59, Critic Loss: 0.13, Policy Loss: -1.88\n",
      "Beam: 1, Iter: 68501, Q: 1.8044, Reward pred: 1, Reward: -1, BF Gain pred: 103.69, BF Gain: 68.39, Critic Loss: 0.14, Policy Loss: -1.26\n",
      "Beam: 2, Iter: 68501, Q: 1.8153, Reward pred: -1, Reward: -1, BF Gain pred: 154.63, BF Gain: 139.87, Critic Loss: 0.25, Policy Loss: -1.09\n",
      "Beam: 3, Iter: 68501, Q: 1.5443, Reward pred: 1, Reward: -1, BF Gain pred: 263.23, BF Gain: 194.32, Critic Loss: 0.14, Policy Loss: -1.26\n",
      "Training for 500 iteration for each Beam uses 24.503381490707397 seconds.\n",
      "Beam: 0, Iter: 69001, Q: 1.9148, Reward pred: -1, Reward: -1, BF Gain pred: 176.20, BF Gain: 136.58, Critic Loss: 0.15, Policy Loss: -1.81\n",
      "Beam: 1, Iter: 69001, Q: 1.7177, Reward pred: 1, Reward: -1, BF Gain pred: 104.93, BF Gain: 80.01, Critic Loss: 0.13, Policy Loss: -1.24\n",
      "Beam: 2, Iter: 69001, Q: 1.7564, Reward pred: -1, Reward: -1, BF Gain pred: 151.84, BF Gain: 124.25, Critic Loss: 0.24, Policy Loss: -1.06\n",
      "Beam: 3, Iter: 69001, Q: 1.5510, Reward pred: 1, Reward: -1, BF Gain pred: 278.14, BF Gain: 234.24, Critic Loss: 0.15, Policy Loss: -1.09\n",
      "Training for 500 iteration for each Beam uses 24.547975301742554 seconds.\n",
      "Beam: 0, Iter: 69501, Q: 1.6169, Reward pred: -1, Reward: -1, BF Gain pred: 178.23, BF Gain: 141.95, Critic Loss: 0.16, Policy Loss: -1.63\n",
      "Beam: 1, Iter: 69501, Q: 1.6809, Reward pred: -1, Reward: -1, BF Gain pred: 103.37, BF Gain: 73.88, Critic Loss: 0.16, Policy Loss: -1.37\n",
      "Beam: 2, Iter: 69501, Q: 1.3634, Reward pred: -1, Reward: -1, BF Gain pred: 155.25, BF Gain: 144.72, Critic Loss: 0.23, Policy Loss: -1.10\n",
      "Beam: 3, Iter: 69501, Q: 1.7145, Reward pred: -1, Reward: -1, BF Gain pred: 273.10, BF Gain: 207.51, Critic Loss: 0.14, Policy Loss: -1.36\n",
      "Training for 500 iteration for each Beam uses 24.61017370223999 seconds.\n",
      "Beam: 0, Iter: 70001, Q: 1.9962, Reward pred: 1, Reward: -1, BF Gain pred: 179.94, BF Gain: 148.89, Critic Loss: 0.13, Policy Loss: -1.88\n",
      "Beam: 1, Iter: 70001, Q: 1.7059, Reward pred: -1, Reward: -1, BF Gain pred: 103.22, BF Gain: 79.49, Critic Loss: 0.15, Policy Loss: -1.31\n",
      "Beam: 2, Iter: 70001, Q: 1.4234, Reward pred: -1, Reward: -1, BF Gain pred: 153.00, BF Gain: 130.93, Critic Loss: 0.25, Policy Loss: -1.09\n",
      "Beam: 3, Iter: 70001, Q: 1.5438, Reward pred: -1, Reward: -1, BF Gain pred: 255.65, BF Gain: 203.26, Critic Loss: 0.17, Policy Loss: -1.18\n",
      "Training for 500 iteration for each Beam uses 25.689699172973633 seconds.\n",
      "Beam: 0, Iter: 70501, Q: 1.6488, Reward pred: 1, Reward: -1, BF Gain pred: 183.83, BF Gain: 131.60, Critic Loss: 0.18, Policy Loss: -1.76\n",
      "Beam: 1, Iter: 70501, Q: 1.7796, Reward pred: -1, Reward: -1, BF Gain pred: 103.68, BF Gain: 85.47, Critic Loss: 0.15, Policy Loss: -1.23\n",
      "Beam: 2, Iter: 70501, Q: 1.4883, Reward pred: 1, Reward: -1, BF Gain pred: 149.66, BF Gain: 132.84, Critic Loss: 0.26, Policy Loss: -1.11\n",
      "Beam: 3, Iter: 70501, Q: 1.7011, Reward pred: -1, Reward: -1, BF Gain pred: 274.93, BF Gain: 216.70, Critic Loss: 0.15, Policy Loss: -1.18\n",
      "Training for 500 iteration for each Beam uses 24.84380054473877 seconds.\n",
      "Beam: 0, Iter: 71001, Q: 1.6904, Reward pred: -1, Reward: -1, BF Gain pred: 97.42, BF Gain: 78.83, Critic Loss: 0.20, Policy Loss: -2.88\n",
      "Beam: 1, Iter: 71001, Q: 1.8822, Reward pred: 1, Reward: -1, BF Gain pred: 100.07, BF Gain: 72.71, Critic Loss: 0.19, Policy Loss: -1.11\n",
      "Beam: 2, Iter: 71001, Q: 1.6119, Reward pred: 1, Reward: -1, BF Gain pred: 154.00, BF Gain: 127.64, Critic Loss: 0.26, Policy Loss: -0.97\n",
      "Beam: 3, Iter: 71001, Q: 1.6271, Reward pred: 1, Reward: -1, BF Gain pred: 272.47, BF Gain: 211.12, Critic Loss: 0.17, Policy Loss: -1.08\n",
      "Training for 500 iteration for each Beam uses 24.556744813919067 seconds.\n",
      "Beam: 0, Iter: 71501, Q: 1.4976, Reward pred: 1, Reward: -1, BF Gain pred: 142.24, BF Gain: 119.02, Critic Loss: 0.19, Policy Loss: -2.08\n",
      "Beam: 1, Iter: 71501, Q: 1.4855, Reward pred: -1, Reward: -1, BF Gain pred: 106.04, BF Gain: 80.59, Critic Loss: 0.18, Policy Loss: -1.09\n",
      "Beam: 2, Iter: 71501, Q: 1.9561, Reward pred: -1, Reward: -1, BF Gain pred: 151.89, BF Gain: 139.94, Critic Loss: 0.28, Policy Loss: -1.03\n",
      "Beam: 3, Iter: 71501, Q: 1.7948, Reward pred: -1, Reward: -1, BF Gain pred: 276.36, BF Gain: 213.81, Critic Loss: 0.16, Policy Loss: -1.20\n",
      "Training for 500 iteration for each Beam uses 24.533273458480835 seconds.\n",
      "Beam: 0, Iter: 72001, Q: 1.5865, Reward pred: 1, Reward: -1, BF Gain pred: 177.74, BF Gain: 159.47, Critic Loss: 0.21, Policy Loss: -1.54\n",
      "Beam: 1, Iter: 72001, Q: 1.5387, Reward pred: -1, Reward: -1, BF Gain pred: 103.64, BF Gain: 83.17, Critic Loss: 0.17, Policy Loss: -1.21\n",
      "Beam: 2, Iter: 72001, Q: 1.2312, Reward pred: 1, Reward: -1, BF Gain pred: 156.13, BF Gain: 122.84, Critic Loss: 0.25, Policy Loss: -1.04\n",
      "Beam: 3, Iter: 72001, Q: 2.1019, Reward pred: 1, Reward: -1, BF Gain pred: 275.04, BF Gain: 210.24, Critic Loss: 0.14, Policy Loss: -1.10\n",
      "Training for 500 iteration for each Beam uses 26.25021481513977 seconds.\n",
      "Beam: 0, Iter: 72501, Q: 1.8205, Reward pred: -1, Reward: -1, BF Gain pred: 188.67, BF Gain: 137.64, Critic Loss: 0.25, Policy Loss: -1.77\n",
      "Beam: 1, Iter: 72501, Q: 1.8287, Reward pred: -1, Reward: -1, BF Gain pred: 104.76, BF Gain: 71.25, Critic Loss: 0.15, Policy Loss: -1.11\n",
      "Beam: 2, Iter: 72501, Q: 1.7336, Reward pred: -1, Reward: -1, BF Gain pred: 149.72, BF Gain: 137.25, Critic Loss: 0.25, Policy Loss: -1.13\n",
      "Beam: 3, Iter: 72501, Q: 1.7693, Reward pred: -1, Reward: -1, BF Gain pred: 278.42, BF Gain: 225.09, Critic Loss: 0.16, Policy Loss: -1.07\n",
      "Training for 500 iteration for each Beam uses 24.25930428504944 seconds.\n",
      "Beam: 0, Iter: 73001, Q: 1.9599, Reward pred: -1, Reward: -1, BF Gain pred: 187.35, BF Gain: 162.98, Critic Loss: 0.24, Policy Loss: -1.87\n",
      "Beam: 1, Iter: 73001, Q: 1.9434, Reward pred: 1, Reward: -1, BF Gain pred: 109.26, BF Gain: 80.25, Critic Loss: 0.17, Policy Loss: -1.27\n",
      "Beam: 2, Iter: 73001, Q: 1.7286, Reward pred: 1, Reward: -1, BF Gain pred: 151.70, BF Gain: 144.24, Critic Loss: 0.27, Policy Loss: -0.93\n",
      "Beam: 3, Iter: 73001, Q: 1.8322, Reward pred: 1, Reward: -1, BF Gain pred: 271.71, BF Gain: 225.59, Critic Loss: 0.17, Policy Loss: -1.23\n",
      "Training for 500 iteration for each Beam uses 24.581365823745728 seconds.\n",
      "Beam: 0, Iter: 73501, Q: 1.3409, Reward pred: 1, Reward: -1, BF Gain pred: 169.31, BF Gain: 145.45, Critic Loss: 0.24, Policy Loss: -1.88\n",
      "Beam: 1, Iter: 73501, Q: 1.7732, Reward pred: -1, Reward: -1, BF Gain pred: 101.52, BF Gain: 86.25, Critic Loss: 0.18, Policy Loss: -1.17\n",
      "Beam: 2, Iter: 73501, Q: 1.5022, Reward pred: 1, Reward: -1, BF Gain pred: 153.47, BF Gain: 132.51, Critic Loss: 0.31, Policy Loss: -1.07\n",
      "Beam: 3, Iter: 73501, Q: 1.5883, Reward pred: -1, Reward: -1, BF Gain pred: 268.87, BF Gain: 198.94, Critic Loss: 0.19, Policy Loss: -1.04\n",
      "Training for 500 iteration for each Beam uses 25.409862995147705 seconds.\n",
      "Beam: 0, Iter: 74001, Q: 1.8088, Reward pred: 1, Reward: -1, BF Gain pred: 190.09, BF Gain: 137.09, Critic Loss: 0.23, Policy Loss: -1.82\n",
      "Beam: 1, Iter: 74001, Q: 1.6112, Reward pred: 1, Reward: -1, BF Gain pred: 107.10, BF Gain: 82.63, Critic Loss: 0.17, Policy Loss: -1.29\n",
      "Beam: 2, Iter: 74001, Q: 1.5654, Reward pred: -1, Reward: -1, BF Gain pred: 154.33, BF Gain: 143.73, Critic Loss: 0.29, Policy Loss: -1.08\n",
      "Beam: 3, Iter: 74001, Q: 1.7591, Reward pred: -1, Reward: -1, BF Gain pred: 270.00, BF Gain: 231.79, Critic Loss: 0.16, Policy Loss: -1.21\n",
      "Training for 500 iteration for each Beam uses 24.98317837715149 seconds.\n",
      "Beam: 0, Iter: 74501, Q: 1.6769, Reward pred: -1, Reward: -1, BF Gain pred: 170.32, BF Gain: 117.65, Critic Loss: 0.23, Policy Loss: -1.93\n",
      "Beam: 1, Iter: 74501, Q: 1.5120, Reward pred: 1, Reward: -1, BF Gain pred: 106.68, BF Gain: 99.90, Critic Loss: 0.21, Policy Loss: -1.23\n",
      "Beam: 2, Iter: 74501, Q: 1.6080, Reward pred: 1, Reward: -1, BF Gain pred: 152.09, BF Gain: 134.69, Critic Loss: 0.29, Policy Loss: -1.08\n",
      "Beam: 3, Iter: 74501, Q: 2.3644, Reward pred: -1, Reward: -1, BF Gain pred: 264.92, BF Gain: 187.91, Critic Loss: 0.17, Policy Loss: -1.41\n",
      "Training for 500 iteration for each Beam uses 24.40264654159546 seconds.\n",
      "Beam: 0, Iter: 75001, Q: 1.3372, Reward pred: 1, Reward: -1, BF Gain pred: 196.65, BF Gain: 153.38, Critic Loss: 0.25, Policy Loss: -1.86\n",
      "Beam: 1, Iter: 75001, Q: 2.0515, Reward pred: 1, Reward: -1, BF Gain pred: 103.77, BF Gain: 92.84, Critic Loss: 0.17, Policy Loss: -1.25\n",
      "Beam: 2, Iter: 75001, Q: 1.3600, Reward pred: -1, Reward: -1, BF Gain pred: 151.38, BF Gain: 124.83, Critic Loss: 0.29, Policy Loss: -1.16\n",
      "Beam: 3, Iter: 75001, Q: 1.7475, Reward pred: 1, Reward: -1, BF Gain pred: 270.68, BF Gain: 219.49, Critic Loss: 0.17, Policy Loss: -1.41\n",
      "Training for 500 iteration for each Beam uses 25.567402839660645 seconds.\n",
      "Beam: 0, Iter: 75501, Q: 1.5818, Reward pred: -1, Reward: -1, BF Gain pred: 185.34, BF Gain: 146.53, Critic Loss: 0.27, Policy Loss: -1.51\n",
      "Beam: 1, Iter: 75501, Q: 1.7969, Reward pred: 1, Reward: -1, BF Gain pred: 105.64, BF Gain: 90.91, Critic Loss: 0.20, Policy Loss: -1.30\n",
      "Beam: 2, Iter: 75501, Q: 1.2210, Reward pred: 1, Reward: -1, BF Gain pred: 151.84, BF Gain: 126.92, Critic Loss: 0.32, Policy Loss: -1.01\n",
      "Beam: 3, Iter: 75501, Q: 1.5957, Reward pred: 1, Reward: -1, BF Gain pred: 263.33, BF Gain: 212.08, Critic Loss: 0.18, Policy Loss: -1.35\n",
      "Training for 500 iteration for each Beam uses 27.715723037719727 seconds.\n",
      "Beam: 0, Iter: 76001, Q: 1.5553, Reward pred: -1, Reward: -1, BF Gain pred: 181.42, BF Gain: 138.64, Critic Loss: 0.29, Policy Loss: -1.30\n",
      "Beam: 1, Iter: 76001, Q: 1.8624, Reward pred: 1, Reward: -1, BF Gain pred: 105.85, BF Gain: 93.05, Critic Loss: 0.19, Policy Loss: -1.22\n",
      "Beam: 2, Iter: 76001, Q: 1.7301, Reward pred: -1, Reward: -1, BF Gain pred: 148.09, BF Gain: 107.80, Critic Loss: 0.32, Policy Loss: -1.09\n",
      "Beam: 3, Iter: 76001, Q: 2.0171, Reward pred: -1, Reward: -1, BF Gain pred: 276.39, BF Gain: 235.45, Critic Loss: 0.19, Policy Loss: -1.42\n",
      "Training for 500 iteration for each Beam uses 24.8787043094635 seconds.\n",
      "Beam: 0, Iter: 76501, Q: 1.5691, Reward pred: 1, Reward: -1, BF Gain pred: 187.93, BF Gain: 153.80, Critic Loss: 0.26, Policy Loss: -1.52\n",
      "Beam: 1, Iter: 76501, Q: 1.4153, Reward pred: -1, Reward: -1, BF Gain pred: 105.12, BF Gain: 96.77, Critic Loss: 0.20, Policy Loss: -1.16\n",
      "Beam: 2, Iter: 76501, Q: 1.3473, Reward pred: -1, Reward: -1, BF Gain pred: 152.64, BF Gain: 127.76, Critic Loss: 0.27, Policy Loss: -1.10\n",
      "Beam: 3, Iter: 76501, Q: 1.5951, Reward pred: 1, Reward: -1, BF Gain pred: 265.58, BF Gain: 195.70, Critic Loss: 0.21, Policy Loss: -1.36\n",
      "Training for 500 iteration for each Beam uses 24.63469696044922 seconds.\n",
      "Beam: 0, Iter: 77001, Q: 1.8588, Reward pred: 1, Reward: -1, BF Gain pred: 188.27, BF Gain: 164.96, Critic Loss: 0.26, Policy Loss: -1.56\n",
      "Beam: 1, Iter: 77001, Q: 1.8777, Reward pred: 1, Reward: -1, BF Gain pred: 105.53, BF Gain: 101.04, Critic Loss: 0.20, Policy Loss: -1.08\n",
      "Beam: 2, Iter: 77001, Q: 1.9217, Reward pred: 1, Reward: -1, BF Gain pred: 154.78, BF Gain: 144.53, Critic Loss: 0.31, Policy Loss: -1.08\n",
      "Beam: 3, Iter: 77001, Q: 1.3966, Reward pred: -1, Reward: -1, BF Gain pred: 246.35, BF Gain: 226.15, Critic Loss: 0.22, Policy Loss: -1.46\n",
      "Training for 500 iteration for each Beam uses 24.622694730758667 seconds.\n",
      "Beam: 0, Iter: 77501, Q: 1.4871, Reward pred: 1, Reward: -1, BF Gain pred: 186.48, BF Gain: 154.41, Critic Loss: 0.29, Policy Loss: -1.24\n",
      "Beam: 1, Iter: 77501, Q: 1.8823, Reward pred: -1, Reward: -1, BF Gain pred: 100.88, BF Gain: 83.36, Critic Loss: 0.25, Policy Loss: -1.17\n",
      "Beam: 2, Iter: 77501, Q: 1.6757, Reward pred: 1, Reward: -1, BF Gain pred: 151.89, BF Gain: 138.99, Critic Loss: 0.29, Policy Loss: -1.09\n",
      "Beam: 3, Iter: 77501, Q: 1.5169, Reward pred: -1, Reward: -1, BF Gain pred: 266.62, BF Gain: 215.61, Critic Loss: 0.22, Policy Loss: -1.40\n",
      "Training for 500 iteration for each Beam uses 24.52981424331665 seconds.\n",
      "Beam: 0, Iter: 78001, Q: 1.4072, Reward pred: -1, Reward: -1, BF Gain pred: 185.98, BF Gain: 158.96, Critic Loss: 0.27, Policy Loss: -1.32\n",
      "Beam: 1, Iter: 78001, Q: 1.4582, Reward pred: -1, Reward: -1, BF Gain pred: 105.72, BF Gain: 89.58, Critic Loss: 0.18, Policy Loss: -1.26\n",
      "Beam: 2, Iter: 78001, Q: 1.7246, Reward pred: -1, Reward: -1, BF Gain pred: 157.04, BF Gain: 129.35, Critic Loss: 0.29, Policy Loss: -1.08\n",
      "Beam: 3, Iter: 78001, Q: 1.9943, Reward pred: -1, Reward: -1, BF Gain pred: 262.96, BF Gain: 248.36, Critic Loss: 0.19, Policy Loss: -1.59\n",
      "Training for 500 iteration for each Beam uses 24.686119079589844 seconds.\n",
      "Beam: 0, Iter: 78501, Q: 1.5981, Reward pred: 1, Reward: -1, BF Gain pred: 188.10, BF Gain: 165.56, Critic Loss: 0.24, Policy Loss: -1.47\n",
      "Beam: 1, Iter: 78501, Q: 1.6741, Reward pred: 1, Reward: -1, BF Gain pred: 101.53, BF Gain: 87.21, Critic Loss: 0.19, Policy Loss: -1.31\n",
      "Beam: 2, Iter: 78501, Q: 1.6722, Reward pred: -1, Reward: -1, BF Gain pred: 154.98, BF Gain: 128.85, Critic Loss: 0.30, Policy Loss: -1.12\n",
      "Beam: 3, Iter: 78501, Q: 1.8299, Reward pred: -1, Reward: -1, BF Gain pred: 251.80, BF Gain: 233.24, Critic Loss: 0.23, Policy Loss: -1.47\n",
      "Training for 500 iteration for each Beam uses 25.311556339263916 seconds.\n",
      "Beam: 0, Iter: 79001, Q: 1.4823, Reward pred: -1, Reward: -1, BF Gain pred: 195.11, BF Gain: 140.46, Critic Loss: 0.27, Policy Loss: -1.52\n",
      "Beam: 1, Iter: 79001, Q: 1.6866, Reward pred: -1, Reward: -1, BF Gain pred: 81.21, BF Gain: 76.84, Critic Loss: 0.19, Policy Loss: -1.32\n",
      "Beam: 2, Iter: 79001, Q: 2.1992, Reward pred: 1, Reward: -1, BF Gain pred: 154.63, BF Gain: 142.83, Critic Loss: 0.35, Policy Loss: -1.07\n",
      "Beam: 3, Iter: 79001, Q: 1.5365, Reward pred: 1, Reward: -1, BF Gain pred: 280.62, BF Gain: 252.33, Critic Loss: 0.23, Policy Loss: -1.33\n",
      "Training for 500 iteration for each Beam uses 27.622560024261475 seconds.\n",
      "Beam: 0, Iter: 79501, Q: 1.8882, Reward pred: -1, Reward: -1, BF Gain pred: 197.66, BF Gain: 179.45, Critic Loss: 0.27, Policy Loss: -1.30\n",
      "Beam: 1, Iter: 79501, Q: 1.4330, Reward pred: 1, Reward: -1, BF Gain pred: 96.83, BF Gain: 85.25, Critic Loss: 0.22, Policy Loss: -1.18\n",
      "Beam: 2, Iter: 79501, Q: 1.4758, Reward pred: -1, Reward: -1, BF Gain pred: 154.11, BF Gain: 142.87, Critic Loss: 0.34, Policy Loss: -0.91\n",
      "Beam: 3, Iter: 79501, Q: 1.7165, Reward pred: 1, Reward: -1, BF Gain pred: 264.99, BF Gain: 229.41, Critic Loss: 0.27, Policy Loss: -1.36\n",
      "Training for 500 iteration for each Beam uses 24.628889560699463 seconds.\n",
      "Beam: 0, Iter: 80001, Q: 1.6467, Reward pred: 1, Reward: -1, BF Gain pred: 196.44, BF Gain: 168.51, Critic Loss: 0.27, Policy Loss: -1.30\n",
      "Beam: 1, Iter: 80001, Q: 1.6686, Reward pred: -1, Reward: -1, BF Gain pred: 105.16, BF Gain: 89.42, Critic Loss: 0.19, Policy Loss: -1.37\n",
      "Beam: 2, Iter: 80001, Q: 1.5501, Reward pred: 1, Reward: -1, BF Gain pred: 156.52, BF Gain: 129.13, Critic Loss: 0.31, Policy Loss: -0.91\n",
      "Beam: 3, Iter: 80001, Q: 1.7507, Reward pred: -1, Reward: -1, BF Gain pred: 255.59, BF Gain: 247.27, Critic Loss: 0.26, Policy Loss: -1.39\n",
      "Training for 500 iteration for each Beam uses 24.67634105682373 seconds.\n",
      "Beam: 0, Iter: 80501, Q: 1.3863, Reward pred: 1, Reward: -1, BF Gain pred: 173.71, BF Gain: 170.57, Critic Loss: 0.32, Policy Loss: -1.25\n",
      "Beam: 1, Iter: 80501, Q: 1.4249, Reward pred: -1, Reward: -1, BF Gain pred: 108.78, BF Gain: 102.90, Critic Loss: 0.22, Policy Loss: -1.38\n",
      "Beam: 2, Iter: 80501, Q: 1.3812, Reward pred: -1, Reward: 1, BF Gain pred: 156.52, BF Gain: 156.95, Critic Loss: 0.32, Policy Loss: -0.96\n",
      "Beam: 3, Iter: 80501, Q: 1.5918, Reward pred: -1, Reward: -1, BF Gain pred: 269.51, BF Gain: 248.74, Critic Loss: 0.26, Policy Loss: -1.47\n",
      "Training for 500 iteration for each Beam uses 25.16736149787903 seconds.\n",
      "Beam: 0, Iter: 81001, Q: 1.4547, Reward pred: 1, Reward: -1, BF Gain pred: 187.57, BF Gain: 171.51, Critic Loss: 0.30, Policy Loss: -1.30\n",
      "Beam: 1, Iter: 81001, Q: 1.7132, Reward pred: -1, Reward: -1, BF Gain pred: 105.10, BF Gain: 96.23, Critic Loss: 0.23, Policy Loss: -1.29\n",
      "Beam: 2, Iter: 81001, Q: 1.7129, Reward pred: -1, Reward: -1, BF Gain pred: 154.68, BF Gain: 129.71, Critic Loss: 0.34, Policy Loss: -0.94\n",
      "Beam: 3, Iter: 81001, Q: 1.9384, Reward pred: -1, Reward: -1, BF Gain pred: 255.15, BF Gain: 240.51, Critic Loss: 0.29, Policy Loss: -1.23\n",
      "Training for 500 iteration for each Beam uses 24.377996683120728 seconds.\n",
      "Beam: 0, Iter: 81501, Q: 1.5681, Reward pred: -1, Reward: -1, BF Gain pred: 198.14, BF Gain: 184.36, Critic Loss: 0.28, Policy Loss: -1.36\n",
      "Beam: 1, Iter: 81501, Q: 1.9742, Reward pred: -1, Reward: -1, BF Gain pred: 103.85, BF Gain: 91.85, Critic Loss: 0.25, Policy Loss: -1.35\n",
      "Beam: 2, Iter: 81501, Q: 1.3504, Reward pred: -1, Reward: 1, BF Gain pred: 154.92, BF Gain: 156.15, Critic Loss: 0.37, Policy Loss: -0.96\n",
      "Beam: 3, Iter: 81501, Q: 1.8769, Reward pred: 1, Reward: -1, BF Gain pred: 233.96, BF Gain: 193.19, Critic Loss: 0.23, Policy Loss: -1.33\n",
      "Training for 500 iteration for each Beam uses 24.722293376922607 seconds.\n",
      "Beam: 0, Iter: 82001, Q: 1.1590, Reward pred: 1, Reward: -1, BF Gain pred: 183.18, BF Gain: 173.53, Critic Loss: 0.32, Policy Loss: -1.34\n",
      "Beam: 1, Iter: 82001, Q: 1.5399, Reward pred: -1, Reward: -1, BF Gain pred: 96.64, BF Gain: 80.58, Critic Loss: 0.22, Policy Loss: -1.26\n",
      "Beam: 2, Iter: 82001, Q: 1.4552, Reward pred: -1, Reward: -1, BF Gain pred: 155.95, BF Gain: 152.03, Critic Loss: 0.38, Policy Loss: -0.84\n",
      "Beam: 3, Iter: 82001, Q: 1.6244, Reward pred: -1, Reward: -1, BF Gain pred: 216.44, BF Gain: 196.23, Critic Loss: 0.30, Policy Loss: -1.47\n",
      "Training for 500 iteration for each Beam uses 24.92417025566101 seconds.\n",
      "Beam: 0, Iter: 82501, Q: 1.3541, Reward pred: 1, Reward: -1, BF Gain pred: 195.00, BF Gain: 166.56, Critic Loss: 0.29, Policy Loss: -1.41\n",
      "Beam: 1, Iter: 82501, Q: 1.4468, Reward pred: 1, Reward: -1, BF Gain pred: 101.35, BF Gain: 92.53, Critic Loss: 0.23, Policy Loss: -1.38\n",
      "Beam: 2, Iter: 82501, Q: 1.5031, Reward pred: 1, Reward: -1, BF Gain pred: 154.47, BF Gain: 138.76, Critic Loss: 0.41, Policy Loss: -0.79\n",
      "Beam: 3, Iter: 82501, Q: 1.6456, Reward pred: -1, Reward: -1, BF Gain pred: 230.28, BF Gain: 220.99, Critic Loss: 0.30, Policy Loss: -1.56\n",
      "Training for 500 iteration for each Beam uses 24.784846305847168 seconds.\n",
      "Beam: 0, Iter: 83001, Q: 1.3181, Reward pred: -1, Reward: -1, BF Gain pred: 196.70, BF Gain: 181.95, Critic Loss: 0.30, Policy Loss: -1.49\n",
      "Beam: 1, Iter: 83001, Q: 1.6615, Reward pred: 1, Reward: -1, BF Gain pred: 104.58, BF Gain: 90.16, Critic Loss: 0.24, Policy Loss: -1.29\n",
      "Beam: 2, Iter: 83001, Q: 1.7524, Reward pred: 1, Reward: -1, BF Gain pred: 152.71, BF Gain: 148.11, Critic Loss: 0.36, Policy Loss: -1.00\n",
      "Beam: 3, Iter: 83001, Q: 1.9549, Reward pred: 1, Reward: -1, BF Gain pred: 220.11, BF Gain: 163.74, Critic Loss: 0.30, Policy Loss: -1.66\n",
      "Training for 500 iteration for each Beam uses 24.836238861083984 seconds.\n",
      "Beam: 0, Iter: 83501, Q: 1.6423, Reward pred: -1, Reward: -1, BF Gain pred: 186.63, BF Gain: 175.45, Critic Loss: 0.28, Policy Loss: -1.58\n",
      "Beam: 1, Iter: 83501, Q: 1.5390, Reward pred: -1, Reward: -1, BF Gain pred: 103.57, BF Gain: 90.55, Critic Loss: 0.25, Policy Loss: -1.32\n",
      "Beam: 2, Iter: 83501, Q: 1.6542, Reward pred: -1, Reward: -1, BF Gain pred: 139.67, BF Gain: 136.61, Critic Loss: 0.41, Policy Loss: -1.68\n",
      "Beam: 3, Iter: 83501, Q: 1.7216, Reward pred: 1, Reward: -1, BF Gain pred: 266.23, BF Gain: 247.75, Critic Loss: 0.33, Policy Loss: -1.03\n",
      "Training for 500 iteration for each Beam uses 24.9918954372406 seconds.\n",
      "Beam: 0, Iter: 84001, Q: 1.7876, Reward pred: -1, Reward: -1, BF Gain pred: 176.25, BF Gain: 168.23, Critic Loss: 0.30, Policy Loss: -1.56\n",
      "Beam: 1, Iter: 84001, Q: 1.5539, Reward pred: -1, Reward: -1, BF Gain pred: 106.76, BF Gain: 93.17, Critic Loss: 0.25, Policy Loss: -1.11\n",
      "Beam: 2, Iter: 84001, Q: 1.1972, Reward pred: -1, Reward: 1, BF Gain pred: 57.47, BF Gain: 60.62, Critic Loss: 0.45, Policy Loss: -3.04\n",
      "Beam: 3, Iter: 84001, Q: 1.4189, Reward pred: 1, Reward: -1, BF Gain pred: 267.76, BF Gain: 238.47, Critic Loss: 0.35, Policy Loss: -1.31\n",
      "Training for 500 iteration for each Beam uses 25.0686993598938 seconds.\n",
      "Beam: 0, Iter: 84501, Q: 1.7047, Reward pred: -1, Reward: -1, BF Gain pred: 198.32, BF Gain: 182.01, Critic Loss: 0.31, Policy Loss: -1.34\n",
      "Beam: 1, Iter: 84501, Q: 1.5237, Reward pred: 1, Reward: -1, BF Gain pred: 105.32, BF Gain: 96.30, Critic Loss: 0.28, Policy Loss: -1.17\n",
      "Beam: 2, Iter: 84501, Q: 2.3036, Reward pred: 1, Reward: -1, BF Gain pred: 148.70, BF Gain: 130.04, Critic Loss: 0.44, Policy Loss: -3.55\n",
      "Beam: 3, Iter: 84501, Q: 1.4730, Reward pred: 1, Reward: -1, BF Gain pred: 275.27, BF Gain: 248.42, Critic Loss: 0.33, Policy Loss: -1.31\n",
      "Training for 500 iteration for each Beam uses 25.25281834602356 seconds.\n",
      "Beam: 0, Iter: 85001, Q: 1.6521, Reward pred: 1, Reward: -1, BF Gain pred: 195.14, BF Gain: 184.43, Critic Loss: 0.36, Policy Loss: -1.25\n",
      "Beam: 1, Iter: 85001, Q: 1.7126, Reward pred: 1, Reward: -1, BF Gain pred: 106.60, BF Gain: 97.18, Critic Loss: 0.23, Policy Loss: -1.11\n",
      "Beam: 2, Iter: 85001, Q: 1.4259, Reward pred: 1, Reward: -1, BF Gain pred: 149.75, BF Gain: 130.66, Critic Loss: 0.44, Policy Loss: -3.78\n",
      "Beam: 3, Iter: 85001, Q: 1.7038, Reward pred: 1, Reward: -1, BF Gain pred: 272.76, BF Gain: 253.70, Critic Loss: 0.37, Policy Loss: -1.28\n",
      "Training for 500 iteration for each Beam uses 24.76722288131714 seconds.\n",
      "Beam: 0, Iter: 85501, Q: 1.7409, Reward pred: 1, Reward: -1, BF Gain pred: 200.74, BF Gain: 186.24, Critic Loss: 0.32, Policy Loss: -1.32\n",
      "Beam: 1, Iter: 85501, Q: 1.5270, Reward pred: 1, Reward: -1, BF Gain pred: 106.51, BF Gain: 99.41, Critic Loss: 0.25, Policy Loss: -1.08\n",
      "Beam: 2, Iter: 85501, Q: 1.7845, Reward pred: 1, Reward: -1, BF Gain pred: 152.63, BF Gain: 142.06, Critic Loss: 0.43, Policy Loss: -4.55\n",
      "Beam: 3, Iter: 85501, Q: 1.7846, Reward pred: -1, Reward: -1, BF Gain pred: 271.60, BF Gain: 267.93, Critic Loss: 0.34, Policy Loss: -1.11\n",
      "Training for 500 iteration for each Beam uses 25.17044186592102 seconds.\n",
      "Beam: 0, Iter: 86001, Q: 1.5975, Reward pred: -1, Reward: -1, BF Gain pred: 184.94, BF Gain: 179.07, Critic Loss: 0.35, Policy Loss: -1.15\n",
      "Beam: 1, Iter: 86001, Q: 1.3474, Reward pred: 1, Reward: -1, BF Gain pred: 107.50, BF Gain: 96.64, Critic Loss: 0.30, Policy Loss: -1.03\n",
      "Beam: 2, Iter: 86001, Q: 1.5378, Reward pred: 1, Reward: -1, BF Gain pred: 157.55, BF Gain: 146.23, Critic Loss: 0.44, Policy Loss: -4.49\n",
      "Beam: 3, Iter: 86001, Q: 1.2994, Reward pred: -1, Reward: -1, BF Gain pred: 270.69, BF Gain: 266.89, Critic Loss: 0.37, Policy Loss: -1.17\n",
      "Training for 500 iteration for each Beam uses 25.088393449783325 seconds.\n",
      "Beam: 0, Iter: 86501, Q: 1.4660, Reward pred: -1, Reward: -1, BF Gain pred: 192.00, BF Gain: 177.50, Critic Loss: 0.33, Policy Loss: -1.32\n",
      "Beam: 1, Iter: 86501, Q: 1.5343, Reward pred: 1, Reward: -1, BF Gain pred: 102.28, BF Gain: 96.28, Critic Loss: 0.32, Policy Loss: -1.06\n",
      "Beam: 2, Iter: 86501, Q: 1.7216, Reward pred: -1, Reward: -1, BF Gain pred: 155.91, BF Gain: 147.96, Critic Loss: 0.44, Policy Loss: -3.94\n",
      "Beam: 3, Iter: 86501, Q: 1.8470, Reward pred: 1, Reward: -1, BF Gain pred: 258.89, BF Gain: 244.84, Critic Loss: 0.36, Policy Loss: -1.12\n",
      "Training for 500 iteration for each Beam uses 24.825667142868042 seconds.\n",
      "Beam: 0, Iter: 87001, Q: 1.5789, Reward pred: -1, Reward: -1, BF Gain pred: 189.63, BF Gain: 190.39, Critic Loss: 0.32, Policy Loss: -1.27\n",
      "Beam: 1, Iter: 87001, Q: 1.4173, Reward pred: 1, Reward: -1, BF Gain pred: 106.76, BF Gain: 101.86, Critic Loss: 0.30, Policy Loss: -1.11\n",
      "Beam: 2, Iter: 87001, Q: 1.9044, Reward pred: 1, Reward: 1, BF Gain pred: 148.49, BF Gain: 150.71, Critic Loss: 0.45, Policy Loss: -2.63\n",
      "Beam: 3, Iter: 87001, Q: 1.3550, Reward pred: 1, Reward: -1, BF Gain pred: 188.69, BF Gain: 164.65, Critic Loss: 0.38, Policy Loss: -1.42\n",
      "Training for 500 iteration for each Beam uses 24.933289289474487 seconds.\n",
      "Beam: 0, Iter: 87501, Q: 1.2277, Reward pred: -1, Reward: -1, BF Gain pred: 184.13, BF Gain: 176.21, Critic Loss: 0.36, Policy Loss: -1.30\n",
      "Beam: 1, Iter: 87501, Q: 1.7643, Reward pred: -1, Reward: -1, BF Gain pred: 104.02, BF Gain: 101.61, Critic Loss: 0.30, Policy Loss: -1.06\n",
      "Beam: 2, Iter: 87501, Q: 1.7600, Reward pred: 1, Reward: -1, BF Gain pred: 149.70, BF Gain: 139.34, Critic Loss: 0.47, Policy Loss: -1.70\n",
      "Beam: 3, Iter: 87501, Q: 1.3222, Reward pred: -1, Reward: -1, BF Gain pred: 266.99, BF Gain: 238.57, Critic Loss: 0.38, Policy Loss: -1.34\n",
      "Training for 500 iteration for each Beam uses 24.903098106384277 seconds.\n",
      "Beam: 0, Iter: 88001, Q: 1.4311, Reward pred: 1, Reward: -1, BF Gain pred: 181.20, BF Gain: 158.43, Critic Loss: 0.37, Policy Loss: -1.35\n",
      "Beam: 1, Iter: 88001, Q: 1.0594, Reward pred: 1, Reward: -1, BF Gain pred: 108.27, BF Gain: 107.89, Critic Loss: 0.35, Policy Loss: -0.77\n",
      "Beam: 2, Iter: 88001, Q: 1.3833, Reward pred: -1, Reward: 1, BF Gain pred: 160.90, BF Gain: 162.24, Critic Loss: 0.49, Policy Loss: -1.37\n",
      "Beam: 3, Iter: 88001, Q: 1.3344, Reward pred: 1, Reward: -1, BF Gain pred: 276.60, BF Gain: 261.14, Critic Loss: 0.43, Policy Loss: -1.08\n",
      "Training for 500 iteration for each Beam uses 24.831419467926025 seconds.\n",
      "Beam: 0, Iter: 88501, Q: 1.3309, Reward pred: -1, Reward: -1, BF Gain pred: 182.35, BF Gain: 170.84, Critic Loss: 0.36, Policy Loss: -1.52\n",
      "Beam: 1, Iter: 88501, Q: 1.4920, Reward pred: 1, Reward: -1, BF Gain pred: 106.02, BF Gain: 101.48, Critic Loss: 0.33, Policy Loss: -0.90\n",
      "Beam: 2, Iter: 88501, Q: 1.3921, Reward pred: -1, Reward: -1, BF Gain pred: 160.74, BF Gain: 153.24, Critic Loss: 0.46, Policy Loss: -1.19\n",
      "Beam: 3, Iter: 88501, Q: 1.1219, Reward pred: -1, Reward: -1, BF Gain pred: 275.39, BF Gain: 260.56, Critic Loss: 0.41, Policy Loss: -1.32\n",
      "Training for 500 iteration for each Beam uses 25.06221628189087 seconds.\n",
      "Beam: 0, Iter: 89001, Q: 1.1077, Reward pred: -1, Reward: -1, BF Gain pred: 193.71, BF Gain: 187.22, Critic Loss: 0.44, Policy Loss: -1.13\n",
      "Beam: 1, Iter: 89001, Q: 1.4029, Reward pred: 1, Reward: 1, BF Gain pred: 109.98, BF Gain: 110.87, Critic Loss: 0.33, Policy Loss: -1.01\n",
      "Beam: 2, Iter: 89001, Q: 1.0514, Reward pred: 1, Reward: -1, BF Gain pred: 162.46, BF Gain: 156.54, Critic Loss: 0.44, Policy Loss: -1.30\n",
      "Beam: 3, Iter: 89001, Q: 1.7974, Reward pred: -1, Reward: -1, BF Gain pred: 277.80, BF Gain: 255.46, Critic Loss: 0.41, Policy Loss: -1.34\n",
      "Training for 500 iteration for each Beam uses 26.66296362876892 seconds.\n",
      "Beam: 0, Iter: 89501, Q: 1.5825, Reward pred: -1, Reward: -1, BF Gain pred: 196.20, BF Gain: 186.04, Critic Loss: 0.44, Policy Loss: -1.15\n",
      "Beam: 1, Iter: 89501, Q: 1.8278, Reward pred: -1, Reward: -1, BF Gain pred: 108.10, BF Gain: 104.32, Critic Loss: 0.32, Policy Loss: -1.18\n",
      "Beam: 2, Iter: 89501, Q: 0.6077, Reward pred: -1, Reward: -1, BF Gain pred: 158.81, BF Gain: 151.43, Critic Loss: 0.53, Policy Loss: 0.02\n",
      "Beam: 3, Iter: 89501, Q: 1.7771, Reward pred: 1, Reward: -1, BF Gain pred: 273.34, BF Gain: 266.04, Critic Loss: 0.42, Policy Loss: -1.47\n",
      "Training for 500 iteration for each Beam uses 25.646512985229492 seconds.\n",
      "Beam: 0, Iter: 90001, Q: 1.1589, Reward pred: 1, Reward: 1, BF Gain pred: 140.79, BF Gain: 140.70, Critic Loss: 0.42, Policy Loss: -1.40\n",
      "Beam: 1, Iter: 90001, Q: 1.7957, Reward pred: -1, Reward: -1, BF Gain pred: 105.60, BF Gain: 103.59, Critic Loss: 0.34, Policy Loss: -1.17\n",
      "Beam: 2, Iter: 90001, Q: 1.9200, Reward pred: -1, Reward: -1, BF Gain pred: 160.95, BF Gain: 157.64, Critic Loss: 0.44, Policy Loss: -1.00\n",
      "Beam: 3, Iter: 90001, Q: 1.7530, Reward pred: 1, Reward: -1, BF Gain pred: 278.60, BF Gain: 268.14, Critic Loss: 0.44, Policy Loss: -1.49\n",
      "Training for 500 iteration for each Beam uses 25.71795153617859 seconds.\n",
      "Beam: 0, Iter: 90501, Q: 1.3452, Reward pred: 1, Reward: -1, BF Gain pred: 181.21, BF Gain: 172.69, Critic Loss: 0.39, Policy Loss: -1.50\n",
      "Beam: 1, Iter: 90501, Q: 1.3555, Reward pred: -1, Reward: -1, BF Gain pred: 95.68, BF Gain: 85.93, Critic Loss: 0.38, Policy Loss: -1.09\n",
      "Beam: 2, Iter: 90501, Q: 1.3129, Reward pred: -1, Reward: -1, BF Gain pred: 164.25, BF Gain: 157.82, Critic Loss: 0.41, Policy Loss: -1.05\n",
      "Beam: 3, Iter: 90501, Q: 1.3844, Reward pred: -1, Reward: -1, BF Gain pred: 238.85, BF Gain: 216.39, Critic Loss: 0.41, Policy Loss: -1.56\n",
      "Training for 500 iteration for each Beam uses 25.293078660964966 seconds.\n",
      "Beam: 0, Iter: 91001, Q: 1.6001, Reward pred: -1, Reward: -1, BF Gain pred: 193.08, BF Gain: 188.28, Critic Loss: 0.41, Policy Loss: -1.64\n",
      "Beam: 1, Iter: 91001, Q: 1.3838, Reward pred: 1, Reward: -1, BF Gain pred: 106.15, BF Gain: 103.68, Critic Loss: 0.38, Policy Loss: -1.04\n",
      "Beam: 2, Iter: 91001, Q: 1.8502, Reward pred: 1, Reward: -1, BF Gain pred: 166.77, BF Gain: 158.20, Critic Loss: 0.42, Policy Loss: -1.05\n",
      "Beam: 3, Iter: 91001, Q: 1.9302, Reward pred: -1, Reward: -1, BF Gain pred: 268.39, BF Gain: 261.57, Critic Loss: 0.44, Policy Loss: -1.34\n",
      "Training for 500 iteration for each Beam uses 25.074642181396484 seconds.\n",
      "Beam: 0, Iter: 91501, Q: 1.6410, Reward pred: 1, Reward: -1, BF Gain pred: 191.72, BF Gain: 179.90, Critic Loss: 0.43, Policy Loss: -1.79\n",
      "Beam: 1, Iter: 91501, Q: 1.3158, Reward pred: -1, Reward: -1, BF Gain pred: 102.79, BF Gain: 102.41, Critic Loss: 0.40, Policy Loss: -1.24\n",
      "Beam: 2, Iter: 91501, Q: 0.1186, Reward pred: -1, Reward: -1, BF Gain pred: 162.12, BF Gain: 152.30, Critic Loss: 0.50, Policy Loss: -0.12\n",
      "Beam: 3, Iter: 91501, Q: 2.1183, Reward pred: 1, Reward: -1, BF Gain pred: 282.18, BF Gain: 261.84, Critic Loss: 0.43, Policy Loss: -1.03\n",
      "Training for 500 iteration for each Beam uses 25.52074432373047 seconds.\n",
      "Beam: 0, Iter: 92001, Q: 1.3680, Reward pred: 1, Reward: -1, BF Gain pred: 194.63, BF Gain: 190.90, Critic Loss: 0.43, Policy Loss: -1.74\n",
      "Beam: 1, Iter: 92001, Q: 1.7869, Reward pred: -1, Reward: -1, BF Gain pred: 106.78, BF Gain: 102.76, Critic Loss: 0.42, Policy Loss: -1.11\n",
      "Beam: 2, Iter: 92001, Q: 0.6248, Reward pred: -1, Reward: -1, BF Gain pred: 164.85, BF Gain: 153.71, Critic Loss: 0.45, Policy Loss: -0.50\n",
      "Beam: 3, Iter: 92001, Q: 1.8357, Reward pred: -1, Reward: -1, BF Gain pred: 271.76, BF Gain: 260.12, Critic Loss: 0.43, Policy Loss: -1.01\n",
      "Training for 500 iteration for each Beam uses 25.17438268661499 seconds.\n",
      "Beam: 0, Iter: 92501, Q: 1.3886, Reward pred: 1, Reward: -1, BF Gain pred: 192.01, BF Gain: 181.62, Critic Loss: 0.40, Policy Loss: -2.20\n",
      "Beam: 1, Iter: 92501, Q: 1.6782, Reward pred: 1, Reward: 1, BF Gain pred: 107.17, BF Gain: 107.59, Critic Loss: 0.43, Policy Loss: -1.04\n",
      "Beam: 2, Iter: 92501, Q: 0.7712, Reward pred: 1, Reward: -1, BF Gain pred: 161.80, BF Gain: 154.91, Critic Loss: 0.43, Policy Loss: -0.57\n",
      "Beam: 3, Iter: 92501, Q: 1.4437, Reward pred: -1, Reward: 1, BF Gain pred: 253.26, BF Gain: 255.69, Critic Loss: 0.43, Policy Loss: -0.92\n",
      "Training for 500 iteration for each Beam uses 29.476441144943237 seconds.\n",
      "Beam: 0, Iter: 93001, Q: 1.8042, Reward pred: 1, Reward: 1, BF Gain pred: 179.00, BF Gain: 175.21, Critic Loss: 0.43, Policy Loss: -2.34\n",
      "Beam: 1, Iter: 93001, Q: 0.6665, Reward pred: -1, Reward: -1, BF Gain pred: 107.44, BF Gain: 105.66, Critic Loss: 0.57, Policy Loss: -0.63\n",
      "Beam: 2, Iter: 93001, Q: 0.7668, Reward pred: 1, Reward: -1, BF Gain pred: 165.10, BF Gain: 158.62, Critic Loss: 0.46, Policy Loss: -0.44\n",
      "Beam: 3, Iter: 93001, Q: 1.6309, Reward pred: -1, Reward: -1, BF Gain pred: 274.09, BF Gain: 261.85, Critic Loss: 0.46, Policy Loss: -0.99\n",
      "Training for 500 iteration for each Beam uses 27.358508825302124 seconds.\n",
      "Beam: 0, Iter: 93501, Q: 0.4558, Reward pred: -1, Reward: -1, BF Gain pred: 192.29, BF Gain: 181.10, Critic Loss: 0.52, Policy Loss: -1.28\n",
      "Beam: 1, Iter: 93501, Q: 1.5871, Reward pred: 1, Reward: -1, BF Gain pred: 110.48, BF Gain: 105.80, Critic Loss: 0.43, Policy Loss: -0.92\n",
      "Beam: 2, Iter: 93501, Q: 0.5427, Reward pred: -1, Reward: -1, BF Gain pred: 166.01, BF Gain: 156.93, Critic Loss: 0.43, Policy Loss: -0.63\n",
      "Beam: 3, Iter: 93501, Q: 1.7969, Reward pred: 1, Reward: -1, BF Gain pred: 267.02, BF Gain: 261.68, Critic Loss: 0.43, Policy Loss: -1.03\n",
      "Training for 500 iteration for each Beam uses 25.64230704307556 seconds.\n",
      "Beam: 0, Iter: 94001, Q: 0.5698, Reward pred: 1, Reward: -1, BF Gain pred: 189.02, BF Gain: 183.30, Critic Loss: 0.54, Policy Loss: -1.36\n",
      "Beam: 1, Iter: 94001, Q: 1.8950, Reward pred: -1, Reward: -1, BF Gain pred: 110.57, BF Gain: 106.22, Critic Loss: 0.44, Policy Loss: -0.97\n",
      "Beam: 2, Iter: 94001, Q: 0.2567, Reward pred: 1, Reward: -1, BF Gain pred: 162.38, BF Gain: 157.07, Critic Loss: 0.46, Policy Loss: -0.42\n",
      "Beam: 3, Iter: 94001, Q: 1.3333, Reward pred: -1, Reward: -1, BF Gain pred: 280.35, BF Gain: 278.35, Critic Loss: 0.46, Policy Loss: -0.87\n",
      "Training for 500 iteration for each Beam uses 25.25291085243225 seconds.\n",
      "Beam: 0, Iter: 94501, Q: 1.0755, Reward pred: -1, Reward: -1, BF Gain pred: 188.97, BF Gain: 188.34, Critic Loss: 0.57, Policy Loss: -1.57\n",
      "Beam: 1, Iter: 94501, Q: 2.5863, Reward pred: -1, Reward: -1, BF Gain pred: 111.00, BF Gain: 108.95, Critic Loss: 0.45, Policy Loss: -0.88\n",
      "Beam: 2, Iter: 94501, Q: 0.5714, Reward pred: 1, Reward: -1, BF Gain pred: 163.20, BF Gain: 156.23, Critic Loss: 0.42, Policy Loss: -0.24\n",
      "Beam: 3, Iter: 94501, Q: 1.7455, Reward pred: -1, Reward: -1, BF Gain pred: 170.02, BF Gain: 158.76, Critic Loss: 0.47, Policy Loss: -0.91\n",
      "Training for 500 iteration for each Beam uses 25.44435715675354 seconds.\n",
      "Beam: 0, Iter: 95001, Q: 0.9817, Reward pred: -1, Reward: -1, BF Gain pred: 187.73, BF Gain: 183.17, Critic Loss: 0.52, Policy Loss: -1.28\n",
      "Beam: 1, Iter: 95001, Q: 1.5502, Reward pred: 1, Reward: -1, BF Gain pred: 108.06, BF Gain: 105.02, Critic Loss: 0.42, Policy Loss: -0.92\n",
      "Beam: 2, Iter: 95001, Q: 0.4315, Reward pred: -1, Reward: -1, BF Gain pred: 164.02, BF Gain: 160.80, Critic Loss: 0.46, Policy Loss: -0.28\n",
      "Beam: 3, Iter: 95001, Q: 1.8875, Reward pred: -1, Reward: 1, BF Gain pred: 282.23, BF Gain: 283.24, Critic Loss: 0.51, Policy Loss: -1.34\n",
      "Training for 500 iteration for each Beam uses 25.376548051834106 seconds.\n",
      "Beam: 0, Iter: 95501, Q: 0.7240, Reward pred: -1, Reward: -1, BF Gain pred: 182.11, BF Gain: 178.27, Critic Loss: 0.58, Policy Loss: -1.10\n",
      "Beam: 1, Iter: 95501, Q: 0.9507, Reward pred: -1, Reward: -1, BF Gain pred: 97.99, BF Gain: 99.80, Critic Loss: 0.49, Policy Loss: -0.99\n",
      "Beam: 2, Iter: 95501, Q: 0.3468, Reward pred: -1, Reward: -1, BF Gain pred: 166.37, BF Gain: 162.76, Critic Loss: 0.42, Policy Loss: -0.30\n",
      "Beam: 3, Iter: 95501, Q: 1.5286, Reward pred: -1, Reward: -1, BF Gain pred: 286.16, BF Gain: 274.59, Critic Loss: 0.51, Policy Loss: -0.95\n",
      "Training for 500 iteration for each Beam uses 25.040154695510864 seconds.\n",
      "Beam: 0, Iter: 96001, Q: 0.6252, Reward pred: -1, Reward: -1, BF Gain pred: 1.39, BF Gain: 1.39, Critic Loss: 0.75, Policy Loss: -1.26\n",
      "Beam: 1, Iter: 96001, Q: 0.7866, Reward pred: -1, Reward: -1, BF Gain pred: 109.46, BF Gain: 108.10, Critic Loss: 0.53, Policy Loss: -0.39\n",
      "Beam: 2, Iter: 96001, Q: 0.2162, Reward pred: -1, Reward: -1, BF Gain pred: 161.07, BF Gain: 160.75, Critic Loss: 0.45, Policy Loss: -0.07\n",
      "Beam: 3, Iter: 96001, Q: 1.8167, Reward pred: 1, Reward: -1, BF Gain pred: 262.93, BF Gain: 249.52, Critic Loss: 0.53, Policy Loss: -1.17\n",
      "Training for 500 iteration for each Beam uses 25.853749752044678 seconds.\n",
      "Beam: 0, Iter: 96501, Q: 0.6619, Reward pred: 1, Reward: -1, BF Gain pred: 24.21, BF Gain: 22.04, Critic Loss: 0.69, Policy Loss: -1.35\n",
      "Beam: 1, Iter: 96501, Q: 0.6314, Reward pred: -1, Reward: -1, BF Gain pred: 105.00, BF Gain: 103.68, Critic Loss: 0.52, Policy Loss: -0.61\n",
      "Beam: 2, Iter: 96501, Q: 0.2798, Reward pred: 1, Reward: -1, BF Gain pred: 164.29, BF Gain: 162.47, Critic Loss: 0.46, Policy Loss: -0.14\n",
      "Beam: 3, Iter: 96501, Q: 2.2441, Reward pred: 1, Reward: -1, BF Gain pred: 164.87, BF Gain: 150.11, Critic Loss: 0.50, Policy Loss: -1.53\n",
      "Training for 500 iteration for each Beam uses 25.067952156066895 seconds.\n",
      "Beam: 0, Iter: 97001, Q: 0.5745, Reward pred: 1, Reward: -1, BF Gain pred: 61.02, BF Gain: 54.57, Critic Loss: 0.71, Policy Loss: -1.20\n",
      "Beam: 1, Iter: 97001, Q: -0.3701, Reward pred: -1, Reward: -1, BF Gain pred: 1.18, BF Gain: 1.23, Critic Loss: 0.69, Policy Loss: 0.42\n",
      "Beam: 2, Iter: 97001, Q: 0.4268, Reward pred: -1, Reward: -1, BF Gain pred: 122.62, BF Gain: 116.16, Critic Loss: 0.51, Policy Loss: -0.12\n",
      "Beam: 3, Iter: 97001, Q: 1.4712, Reward pred: 1, Reward: 1, BF Gain pred: 219.29, BF Gain: 216.98, Critic Loss: 0.55, Policy Loss: -2.37\n",
      "Training for 500 iteration for each Beam uses 25.534319162368774 seconds.\n",
      "Beam: 0, Iter: 97501, Q: 0.8303, Reward pred: -1, Reward: 1, BF Gain pred: 20.35, BF Gain: 20.99, Critic Loss: 0.68, Policy Loss: -1.97\n",
      "Beam: 1, Iter: 97501, Q: 0.5357, Reward pred: -1, Reward: -1, BF Gain pred: 3.17, BF Gain: 2.50, Critic Loss: 0.67, Policy Loss: -1.27\n",
      "Beam: 2, Iter: 97501, Q: 0.4648, Reward pred: 1, Reward: -1, BF Gain pred: 162.40, BF Gain: 162.15, Critic Loss: 0.59, Policy Loss: -0.18\n",
      "Beam: 3, Iter: 97501, Q: 1.9641, Reward pred: 1, Reward: 1, BF Gain pred: 272.81, BF Gain: 267.33, Critic Loss: 0.52, Policy Loss: -2.24\n",
      "Training for 500 iteration for each Beam uses 25.89634919166565 seconds.\n",
      "Beam: 0, Iter: 98001, Q: 0.9417, Reward pred: -1, Reward: 1, BF Gain pred: 73.73, BF Gain: 74.44, Critic Loss: 0.74, Policy Loss: -2.79\n",
      "Beam: 1, Iter: 98001, Q: 0.3996, Reward pred: 1, Reward: -1, BF Gain pred: 5.59, BF Gain: 4.83, Critic Loss: 0.70, Policy Loss: -1.47\n",
      "Beam: 2, Iter: 98001, Q: 0.5510, Reward pred: 1, Reward: -1, BF Gain pred: 164.05, BF Gain: 161.74, Critic Loss: 0.54, Policy Loss: -0.11\n",
      "Beam: 3, Iter: 98001, Q: 1.9424, Reward pred: 1, Reward: -1, BF Gain pred: 279.62, BF Gain: 268.01, Critic Loss: 0.52, Policy Loss: -2.35\n",
      "Training for 500 iteration for each Beam uses 25.6201593875885 seconds.\n",
      "Beam: 0, Iter: 98501, Q: 1.2528, Reward pred: 1, Reward: -1, BF Gain pred: 84.94, BF Gain: 80.92, Critic Loss: 0.72, Policy Loss: -3.18\n",
      "Beam: 1, Iter: 98501, Q: 0.7871, Reward pred: -1, Reward: -1, BF Gain pred: 18.44, BF Gain: 17.65, Critic Loss: 0.68, Policy Loss: -1.98\n",
      "Beam: 2, Iter: 98501, Q: 1.0037, Reward pred: 1, Reward: 1, BF Gain pred: 128.63, BF Gain: 130.20, Critic Loss: 0.57, Policy Loss: -0.30\n",
      "Beam: 3, Iter: 98501, Q: 1.8322, Reward pred: 1, Reward: -1, BF Gain pred: 272.83, BF Gain: 271.54, Critic Loss: 0.58, Policy Loss: -2.35\n",
      "Training for 500 iteration for each Beam uses 26.001442670822144 seconds.\n",
      "Beam: 0, Iter: 99001, Q: 1.2849, Reward pred: 1, Reward: -1, BF Gain pred: 94.39, BF Gain: 91.91, Critic Loss: 0.79, Policy Loss: -3.50\n",
      "Beam: 1, Iter: 99001, Q: 0.7959, Reward pred: -1, Reward: -1, BF Gain pred: 20.63, BF Gain: 20.63, Critic Loss: 0.72, Policy Loss: -1.67\n",
      "Beam: 2, Iter: 99001, Q: 0.6680, Reward pred: -1, Reward: -1, BF Gain pred: 112.57, BF Gain: 110.49, Critic Loss: 0.57, Policy Loss: -0.30\n",
      "Beam: 3, Iter: 99001, Q: 2.2027, Reward pred: 1, Reward: 1, BF Gain pred: 276.86, BF Gain: 283.40, Critic Loss: 0.53, Policy Loss: -2.23\n",
      "Training for 500 iteration for each Beam uses 25.775926113128662 seconds.\n",
      "Beam: 0, Iter: 99501, Q: 1.4315, Reward pred: -1, Reward: -1, BF Gain pred: 93.57, BF Gain: 90.31, Critic Loss: 0.77, Policy Loss: -3.74\n",
      "Beam: 1, Iter: 99501, Q: 0.7964, Reward pred: -1, Reward: -1, BF Gain pred: 11.65, BF Gain: 11.63, Critic Loss: 0.70, Policy Loss: -1.87\n",
      "Beam: 2, Iter: 99501, Q: 0.5692, Reward pred: -1, Reward: -1, BF Gain pred: 79.84, BF Gain: 69.24, Critic Loss: 0.62, Policy Loss: -0.73\n",
      "Beam: 3, Iter: 99501, Q: 2.2445, Reward pred: 1, Reward: 1, BF Gain pred: 268.18, BF Gain: 263.56, Critic Loss: 0.53, Policy Loss: -2.21\n",
      "Training for 500 iteration for each Beam uses 25.648329973220825 seconds.\n",
      "Beam: 0, Iter: 100001, Q: 1.2547, Reward pred: -1, Reward: -1, BF Gain pred: 35.52, BF Gain: 35.33, Critic Loss: 0.81, Policy Loss: -4.40\n",
      "Beam: 1, Iter: 100001, Q: 1.1489, Reward pred: 1, Reward: 1, BF Gain pred: 21.74, BF Gain: 21.44, Critic Loss: 0.73, Policy Loss: -2.81\n",
      "Beam: 2, Iter: 100001, Q: 1.1532, Reward pred: -1, Reward: -1, BF Gain pred: 111.48, BF Gain: 111.49, Critic Loss: 0.63, Policy Loss: -0.62\n",
      "Beam: 3, Iter: 100001, Q: 1.9589, Reward pred: -1, Reward: -1, BF Gain pred: 271.42, BF Gain: 269.95, Critic Loss: 0.55, Policy Loss: -2.13\n",
      "Training for 500 iteration for each Beam uses 30.751547813415527 seconds.\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.device(options['gpu_idx']):\n",
    "    u_classifier, sensing_beam = KMeans_only(ch, options['num_NNs'], n_bit=options['num_bits'], n_rand_beam=30)\n",
    "    np.save('sensing_beam.npy', sensing_beam)\n",
    "    sensing_beam = torch.from_numpy(sensing_beam).float().cuda()\n",
    "\n",
    "    filename = 'kmeans_model.sav'\n",
    "    pickle.dump(u_classifier, open(filename, 'wb'))\n",
    "\n",
    "    # Quantization settings\n",
    "    options['num_ph'] = 2 ** options['num_bits']\n",
    "    options['multi_step'] = torch.from_numpy(\n",
    "        np.linspace(int(-(options['num_ph'] - 2) / 2),\n",
    "                    int(options['num_ph'] / 2),\n",
    "                    num=options['num_ph'],\n",
    "                    endpoint=True)).type(dtype=torch.float32).reshape(1, -1).cuda()\n",
    "    options['pi'] = torch.tensor(np.pi).cuda()\n",
    "    options['ph_table'] = (2 * options['pi']) / options['num_ph'] * options['multi_step']\n",
    "    options['ph_table'].cuda()\n",
    "    options['ph_table_rep'] = options['ph_table'].repeat(options['num_ant'], 1)\n",
    "\n",
    "    # initialize DRL models\n",
    "    actor_net_list = []\n",
    "    critic_net_list = []\n",
    "    actor_net_t_list = []\n",
    "    critic_net_t_list = []\n",
    "    ounoise_list = []\n",
    "    env_list = []\n",
    "    train_opt_list = []\n",
    "\n",
    "    for beam_id in range(options['num_NNs']):\n",
    "        actor_net_list.append(Actor(options['num_ant'], options['num_ant']))\n",
    "        actor_net_t_list.append(Actor(options['num_ant'], options['num_ant']))\n",
    "        critic_net_list.append(Critic(2 * options['num_ant'], 1))\n",
    "        critic_net_t_list.append(Critic(2 * options['num_ant'], 1))\n",
    "        ounoise_list.append(OUNoise((1, options['num_ant'])))\n",
    "        env_list.append(envCB_(ch, options['num_ant'], options['num_bits'], beam_id, options))\n",
    "        train_opt_list.append(copy.deepcopy(train_opt))\n",
    "\n",
    "        actor_net_list[beam_id] = actor_net_list[beam_id].cuda()\n",
    "        actor_net_t_list[beam_id] = actor_net_t_list[beam_id].cuda()\n",
    "        critic_net_list[beam_id] = critic_net_list[beam_id].cuda()\n",
    "        critic_net_t_list[beam_id] = critic_net_t_list[beam_id].cuda()\n",
    "        actor_net_list[beam_id].apply(init_weights)\n",
    "        actor_net_t_list[beam_id].load_state_dict(actor_net_list[beam_id].state_dict())\n",
    "        critic_net_list[beam_id].apply(init_weights)\n",
    "        critic_net_t_list[beam_id].load_state_dict(critic_net_list[beam_id].state_dict())\n",
    "\n",
    "    # start_time = time.time()\n",
    "\n",
    "    # outer loop for randomly sampling users, emulating user dynamics\n",
    "    for sample_id in range(options['num_loop']):\n",
    "\n",
    "        # ---------- Sampling ---------- #\n",
    "        n_sample = int(ch.shape[0] * options['ch_sample_ratio'])\n",
    "        ch_sample_id = np.random.permutation(ch.shape[0])[0:n_sample]\n",
    "        ch_sample = torch.from_numpy(ch[ch_sample_id, :]).float().cuda()\n",
    "\n",
    "        # ---------- Clustering ---------- #\n",
    "#         start_time = time.time()\n",
    "\n",
    "        bf_mat_sample = bf_gain_cal(sensing_beam, ch_sample)\n",
    "        # print(\"Clustering -1 uses %s seconds.\" % (time.time() - start_time))\n",
    "        # start_time = time.time()\n",
    "        f_matrix = corr_mining(bf_mat_sample)\n",
    "        f_matrix_np = torch.Tensor.cpu(f_matrix).numpy()\n",
    "        # print(\"Clustering 0 uses %s seconds.\" % (time.time() - start_time))\n",
    "        # start_time = time.time()\n",
    "        labels = u_classifier.predict(np.transpose(f_matrix_np).astype(float))\n",
    "\n",
    "        # print(\"Clustering 1 uses %s seconds.\" % (time.time() - start_time))\n",
    "        # start_time = time.time()\n",
    "\n",
    "        user_group = []  # order: clusters\n",
    "        ch_group = []  # order: clusters\n",
    "        for ii in range(options['num_NNs']):\n",
    "            user_group.append(np.where(labels == ii)[0].tolist())\n",
    "            ch_group.append(ch_sample[user_group[ii], :])\n",
    "\n",
    "#         print(\"Clustering 2 uses %s seconds.\" % (time.time() - start_time))\n",
    "\n",
    "        # ---------- Assignment ---------- #\n",
    "#         start_time = time.time()\n",
    "\n",
    "        # best_state matrix\n",
    "        best_beam_mtx = torch.zeros((options['num_NNs'], 2 * options['num_ant'])).float().cuda()\n",
    "        for pp in range(options['num_NNs']):\n",
    "            best_beam_mtx[pp, :] = env_list[pp].best_bf_vec\n",
    "        gain_mtx = bf_gain_cal(best_beam_mtx, ch_sample)  # (n_beam, n_user)\n",
    "        for ii in range(options['num_NNs']):\n",
    "            if ii == 0:\n",
    "                cost_mtx = torch.mean(gain_mtx[:, user_group[ii]], dim=1).reshape(options['num_NNs'], -1)\n",
    "            else:\n",
    "                sub = torch.mean(gain_mtx[:, user_group[ii]], dim=1).reshape(options['num_NNs'], -1)\n",
    "                cost_mtx = torch.cat((cost_mtx, sub), dim=1)\n",
    "        cost_mtx = -torch.Tensor.cpu(cost_mtx).numpy()\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_mtx)\n",
    "        assignment_record = dict(zip(row_ind.tolist(), col_ind.tolist()))  # key: network, value: cluster\n",
    "#         print(assignment_record)\n",
    "        for ii in range(options['num_NNs']):\n",
    "            env_list[ii].ch = ch_group[assignment_record[ii]]\n",
    "\n",
    "#         print(\"Assignment uses %s seconds.\" % (time.time() - start_time))\n",
    "        if (train_opt_list[beam_id]['overall_iter']-1)%500==0 or train_opt_list[beam_id]['overall_iter']==1:\n",
    "            start_time = time.time()\n",
    "        # ---------- Learning ---------- #\n",
    "        for beam_id in range(options['num_NNs']):\n",
    "            train_opt_list[beam_id] = train(actor_net_list[beam_id],\n",
    "                                            critic_net_list[beam_id],\n",
    "                                            actor_net_t_list[beam_id],\n",
    "                                            critic_net_t_list[beam_id],\n",
    "                                            ounoise_list[beam_id],\n",
    "                                            env_list[beam_id],\n",
    "                                            options,\n",
    "                                            train_opt_list[beam_id],\n",
    "                                            beam_id)\n",
    "        if (train_opt_list[beam_id]['overall_iter']-1)%500==0: \n",
    "            print(\"Training for 500 iteration for each Beam uses %s seconds.\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:47:10.239597Z",
     "iopub.status.busy": "2023-11-06T19:47:10.239328Z",
     "iopub.status.idle": "2023-11-06T19:47:10.249306Z",
     "shell.execute_reply": "2023-11-06T19:47:10.248447Z",
     "shell.execute_reply.started": "2023-11-06T19:47:10.239575Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "\n",
    "num_ant = 32\n",
    "num_beam = 4\n",
    "results = np.empty((num_beam, 2*num_ant))\n",
    "\n",
    "path = './beams/'\n",
    "\n",
    "for beam_id in range(num_beam):\n",
    "    fname = 'beams_' + str(beam_id) + '_max.txt'\n",
    "    with open(path + fname, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        last_line = lines[-1]\n",
    "        results[beam_id, :] = np.fromstring(last_line.replace(\"\\n\", \"\"), sep=',').reshape(1, -1)\n",
    "\n",
    "results = (1 / np.sqrt(num_ant)) * (results[:, ::2] + 1j * results[:, 1::2])\n",
    "\n",
    "scio.savemat('beam_codebook.mat', {'beams': results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-06T19:47:10.251983Z",
     "iopub.status.busy": "2023-11-06T19:47:10.251666Z",
     "iopub.status.idle": "2023-11-06T19:47:10.691829Z",
     "shell.execute_reply": "2023-11-06T19:47:10.690894Z",
     "shell.execute_reply.started": "2023-11-06T19:47:10.251958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(f'Best beamforming gain')\n",
    "plt.xlabel('Beam_framing_gain')   \n",
    "for beam_id in range(options['num_NNs']):\n",
    "    gain_record=np.array(env_list[beam_id].gain_history[1:])\n",
    "    np.save(f'beam_{beam_id}_gain_records',gain_record)\n",
    "    plt.plot(gain_record, label=f'Beam_{beam_id}')\n",
    "    plt.plot([])\n",
    "plt.plot([231.17094]*100000,linestyle='dashed',color='black')\n",
    "plt.legend(loc=\"lower right\")  \n",
    "plt.show()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
