{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-07T18:12:55.616821Z",
     "iopub.status.busy": "2023-11-07T18:12:55.616381Z",
     "iopub.status.idle": "2023-11-07T18:12:57.715541Z",
     "shell.execute_reply": "2023-11-07T18:12:57.714318Z",
     "shell.execute_reply.started": "2023-11-07T18:12:55.616750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Codebook_Learning_RL' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/YuZhang-GitHub/Codebook_Learning_RL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:12:57.717998Z",
     "iopub.status.busy": "2023-11-07T18:12:57.717711Z",
     "iopub.status.idle": "2023-11-07T18:13:13.960801Z",
     "shell.execute_reply": "2023-11-07T18:13:13.959613Z",
     "shell.execute_reply.started": "2023-11-07T18:12:57.717970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pfrl in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pfrl) (2.1.0a0+32f93b1)\n",
      "Requirement already satisfied: gym>=0.9.7 in /usr/local/lib/python3.10/dist-packages (from pfrl) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from pfrl) (1.22.2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pfrl) (9.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from pfrl) (3.12.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.9.7->pfrl) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.9.7->pfrl) (0.0.8)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (4.7.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->pfrl) (2023.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->pfrl) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->pfrl) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.22.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pfrl\n",
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:13:13.962474Z",
     "iopub.status.busy": "2023-11-07T18:13:13.962149Z",
     "iopub.status.idle": "2023-11-07T18:13:13.967460Z",
     "shell.execute_reply": "2023-11-07T18:13:13.966388Z",
     "shell.execute_reply.started": "2023-11-07T18:13:13.962444Z"
    }
   },
   "outputs": [],
   "source": [
    "# from Codebook_Learning_RL import DDPG_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:13:13.970037Z",
     "iopub.status.busy": "2023-11-07T18:13:13.969775Z",
     "iopub.status.idle": "2023-11-07T18:13:13.980920Z",
     "shell.execute_reply": "2023-11-07T18:13:13.979942Z",
     "shell.execute_reply.started": "2023-11-07T18:13:13.970015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Codebook_Learning_RL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/workspace/Codebook_Learning_RL')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:13:13.982235Z",
     "iopub.status.busy": "2023-11-07T18:13:13.981998Z",
     "iopub.status.idle": "2023-11-07T18:13:20.276087Z",
     "shell.execute_reply": "2023-11-07T18:13:20.275229Z",
     "shell.execute_reply.started": "2023-11-07T18:13:13.982214Z"
    }
   },
   "outputs": [],
   "source": [
    "import pfrl\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from DataPrep import dataPrep\n",
    "from env_ddpg import envCB\n",
    "from clustering import KMeans_only\n",
    "from function_lib import bf_gain_cal, corr_mining\n",
    "from DDPG_classes import OUNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1321,\n",
       " 27148,\n",
       " 26104,\n",
       " 30786,\n",
       " 31404,\n",
       " 31885,\n",
       " 31885,\n",
       " 15171,\n",
       " 31573,\n",
       " 31573,\n",
       " 31883,\n",
       " 31883,\n",
       " 31883,\n",
       " 14856,\n",
       " 1,\n",
       " 2035]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "\n",
    "def get_gpu_memory():\n",
    "    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    return memory_free_values\n",
    "\n",
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:13:20.278225Z",
     "iopub.status.busy": "2023-11-07T18:13:20.277693Z",
     "iopub.status.idle": "2023-11-07T18:13:20.283686Z",
     "shell.execute_reply": "2023-11-07T18:13:20.282666Z",
     "shell.execute_reply.started": "2023-11-07T18:13:20.278188Z"
    }
   },
   "outputs": [],
   "source": [
    "class envCB_(envCB):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.gain_history = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:13:20.285453Z",
     "iopub.status.busy": "2023-11-07T18:13:20.285104Z",
     "iopub.status.idle": "2023-11-07T18:13:38.312574Z",
     "shell.execute_reply": "2023-11-07T18:13:38.311420Z",
     "shell.execute_reply.started": "2023-11-07T18:13:20.285421Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "options = {\n",
    "        'gpu_idx': 12,\n",
    "        'num_ant': 32,\n",
    "        'num_bits': 4,\n",
    "        'num_NNs': 4,  # codebook size\n",
    "        'ch_sample_ratio': 0.5,\n",
    "        'num_loop': 1000,  # outer loop\n",
    "        'target_update': 3,\n",
    "        'path': './grid1101-1400.mat',\n",
    "        'clustering_mode': 'random',\n",
    "    }\n",
    "\n",
    "train_opt = {\n",
    "        'state': 0,\n",
    "        'best_state': 0,\n",
    "        'num_iter': 100,  # inner loop\n",
    "        'tau': 1e-2,\n",
    "        'overall_iter': 1,\n",
    "        'replay_memory': [],\n",
    "        'replay_memory_size': 8192,\n",
    "        'minibatch_size': 1024,\n",
    "        'gamma': 0,\n",
    "        'gpu':12\n",
    "    }\n",
    "if not os.path.exists('beams/'):\n",
    "    os.mkdir('beams/')\n",
    "\n",
    "ch = dataPrep(options['path'])\n",
    "ch = np.concatenate((ch[:, :options['num_ant']],\n",
    "                     ch[:, int(ch.shape[1] / 2):int(ch.shape[1] / 2) + options['num_ant']]), axis=1)\n",
    "with torch.cuda.device(options['gpu_idx']):\n",
    "    u_classifier, sensing_beam = KMeans_only(ch, options['num_NNs'], n_bit=options['num_bits'], n_rand_beam=30)\n",
    "    np.save('sensing_beam.npy', sensing_beam)\n",
    "    sensing_beam = torch.from_numpy(sensing_beam).float().cuda()\n",
    "\n",
    "    filename = 'kmeans_model.sav'\n",
    "    pickle.dump(u_classifier, open(filename, 'wb'))\n",
    "\n",
    "    # Quantization settings\n",
    "    options['num_ph'] = 2 ** options['num_bits']\n",
    "    options['multi_step'] = torch.from_numpy(\n",
    "        np.linspace(int(-(options['num_ph'] - 2) / 2),\n",
    "                    int(options['num_ph'] / 2),\n",
    "                    num=options['num_ph'],\n",
    "                    endpoint=True)).type(dtype=torch.float32).reshape(1, -1).cuda()\n",
    "    options['pi'] = torch.tensor(np.pi).cuda()\n",
    "    options['ph_table'] = (2 * options['pi']) / options['num_ph'] * options['multi_step']\n",
    "    options['ph_table'].cuda()\n",
    "    options['ph_table_rep'] = options['ph_table'].repeat(options['num_ant'], 1)\n",
    "    actor_net_list = []\n",
    "    critic_net_list = []\n",
    "    actor_net_t_list = []\n",
    "    critic_net_t_list = []\n",
    "    ounoise_list = []\n",
    "    env_list = []\n",
    "    train_opt_list = []\n",
    "    agent_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:13:38.314670Z",
     "iopub.status.busy": "2023-11-07T18:13:38.314142Z",
     "iopub.status.idle": "2023-11-07T18:13:38.329119Z",
     "shell.execute_reply": "2023-11-07T18:13:38.328135Z",
     "shell.execute_reply.started": "2023-11-07T18:13:38.314631Z"
    }
   },
   "outputs": [],
   "source": [
    "from pfrl.utils.batch_states import batch_states\n",
    "def batch_experiences_(experiences, device, phi, gamma, batch_states=batch_states):\n",
    "    \"\"\"Takes a batch of k experiences each of which contains j\n",
    "\n",
    "    consecutive transitions and vectorizes them, where j is between 1 and n.\n",
    "\n",
    "    Args:\n",
    "        experiences: list of experiences. Each experience is a list\n",
    "            containing between 1 and n dicts containing\n",
    "              - state (object): State\n",
    "              - action (object): Action\n",
    "              - reward (float): Reward\n",
    "              - is_state_terminal (bool): True iff next state is terminal\n",
    "              - next_state (object): Next state\n",
    "        device : GPU or CPU the tensor should be placed on\n",
    "        phi : Preprocessing function\n",
    "        gamma: discount factor\n",
    "        batch_states: function that converts a list to a batch\n",
    "    Returns:\n",
    "        dict of batched transitions\n",
    "    \"\"\"\n",
    "\n",
    "    batch_exp = {\n",
    "        \"state\": batch_states([elem[0][\"state\"] for elem in experiences], device, phi),\n",
    "        \"action\": torch.as_tensor(\n",
    "            np.array([elem[0][\"action\"] for elem in experiences]), device=device\n",
    "        ),\n",
    "        \"reward\": torch.as_tensor(\n",
    "            [\n",
    "                sum((gamma**i) * exp[i][\"reward\"] for i in range(len(exp)))\n",
    "                for exp in experiences\n",
    "            ],\n",
    "            dtype=torch.float32,\n",
    "            device=device,\n",
    "        ),\n",
    "        \"next_state\": batch_states(\n",
    "            [elem[-1][\"next_state\"] for elem in experiences], device, phi\n",
    "        ),\n",
    "        \"is_state_terminal\": torch.as_tensor(\n",
    "            [\n",
    "                any(transition[\"is_state_terminal\"] for transition in exp)\n",
    "                for exp in experiences\n",
    "            ],\n",
    "            dtype=torch.float32,\n",
    "            device=device,\n",
    "        ),\n",
    "        \"discount\": torch.as_tensor(\n",
    "            [(gamma ** len(elem)) for elem in experiences],\n",
    "            dtype=torch.float32,\n",
    "            device=device,\n",
    "        ),\n",
    "    }\n",
    "    if all(elem[-1][\"next_action\"] is not None for elem in experiences):\n",
    "        batch_exp[\"next_action\"] = torch.as_tensor(\n",
    "            [elem[-1][\"next_action\"] for elem in experiences], device=device\n",
    "        )\n",
    "    return batch_exp\n",
    "\n",
    "from pfrl.agents.ddpg import DDPG\n",
    "class DDPG_(DDPG):\n",
    "    def update(self, experiences, errors_out=None):\n",
    "        \"\"\"Update the model from experiences\"\"\"\n",
    "\n",
    "        batch = batch_experiences_(experiences, self.device, self.phi, self.gamma)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        self.compute_critic_loss(batch).backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.compute_actor_loss(batch).backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.n_updates += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:13:38.330988Z",
     "iopub.status.busy": "2023-11-07T18:13:38.330457Z",
     "iopub.status.idle": "2023-11-07T18:13:38.344990Z",
     "shell.execute_reply": "2023-11-07T18:13:38.344059Z",
     "shell.execute_reply.started": "2023-11-07T18:13:38.330961Z"
    }
   },
   "outputs": [],
   "source": [
    "import pfrl\n",
    "from pfrl import experiments, explorers, replay_buffers, utils\n",
    "from pfrl.agents.ddpg import DDPG\n",
    "from pfrl.nn import BoundByTanh, ConcatObsAndAction\n",
    "from pfrl.policies import DeterministicHead\n",
    "from pfrl.explorers import Greedy\n",
    "\n",
    "def create_agent(state_size,action_size,train_options,gpu=12):\n",
    "    \n",
    "    #critic_network\n",
    "    q_func = nn.Sequential(\n",
    "        ConcatObsAndAction(),\n",
    "        nn.Linear(state_size + action_size, 16*state_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16*state_size, 16*action_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16*action_size, 1),\n",
    "    )\n",
    "    #actor_network\n",
    "    policy = nn.Sequential(\n",
    "        nn.Linear(state_size, 16*state_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16*state_size, 16*action_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16*action_size, action_size),\n",
    "        BoundByTanh(low= -torch.tensor(np.pi).float(), high= torch.tensor(np.pi).float()),\n",
    "        DeterministicHead(),\n",
    "    )\n",
    "    \n",
    "    opt_a = torch.optim.Adam(policy.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "    opt_c = torch.optim.Adam(q_func.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "    rbuf = replay_buffers.ReplayBuffer(train_options['replay_memory_size'])\n",
    "    \n",
    "    explorer =  Greedy()\n",
    "    \n",
    "    agent = DDPG_(\n",
    "        policy,\n",
    "        q_func,\n",
    "        opt_a,\n",
    "        opt_c,\n",
    "        rbuf,\n",
    "        gamma=train_options['gamma'],\n",
    "        explorer = explorer,\n",
    "        replay_start_size=train_options['minibatch_size'],\n",
    "        target_update_method=\"soft\",\n",
    "        target_update_interval=options['target_update'],\n",
    "        update_interval=1,\n",
    "        soft_update_tau= train_opt['tau'],\n",
    "        n_times_update=1,\n",
    "        gpu=gpu,\n",
    "        minibatch_size=train_options['minibatch_size'],\n",
    "    )\n",
    "    \n",
    "    return agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:13:38.348114Z",
     "iopub.status.busy": "2023-11-07T18:13:38.347862Z",
     "iopub.status.idle": "2023-11-07T18:13:38.362335Z",
     "shell.execute_reply": "2023-11-07T18:13:38.361570Z",
     "shell.execute_reply.started": "2023-11-07T18:13:38.348093Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_agent_sac(state_size,action_size,train_options,gpu=12):\n",
    "    def squashed_diagonal_gaussian_head(x):\n",
    "        assert x.shape[-1] == action_size * 2\n",
    "        mean, log_scale = torch.chunk(x, 2, dim=2)\n",
    "        log_scale = torch.clamp(log_scale, -20.0, 2.0)\n",
    "        var = torch.exp(log_scale * 2)\n",
    "        base_distribution = distributions.Independent(\n",
    "            distributions.Normal(loc=mean, scale=torch.sqrt(var)), 1\n",
    "        )\n",
    "        # cache_size=1 is required for numerical stability\n",
    "        return distributions.transformed_distribution.TransformedDistribution(\n",
    "            base_distribution, [distributions.transforms.TanhTransform(cache_size=1),\n",
    "                                distributions.transforms.AffineTransform(0, torch.pi, event_dim=0, cache_size=1)]\n",
    "        )\n",
    "\n",
    "    #critic_networks\n",
    "    q_func_1 = nn.Sequential(\n",
    "        ConcatObsAndAction(),\n",
    "        nn.Linear(state_size + action_size, 16*state_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16*state_size, 16*action_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16*action_size, 1),\n",
    "    )\n",
    "    q_func_2 = nn.Sequential(\n",
    "        ConcatObsAndAction(),\n",
    "        nn.Linear(state_size + action_size, 16*state_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16*state_size, 16*action_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16*action_size, 1),\n",
    "    )\n",
    "    \n",
    "    #actor_network\n",
    "    policy = nn.Sequential(\n",
    "        nn.Linear(state_size, 16*state_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16*state_size, 16*action_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16*action_size, action_size* 2),\n",
    "        pfrl.nn.lmbda.Lambda(squashed_diagonal_gaussian_head),\n",
    "    )\n",
    "\n",
    "\n",
    "    opt_a = torch.optim.Adam(policy.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "    opt_c_1 = torch.optim.Adam(q_func_1.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "    opt_c_2 = torch.optim.Adam(q_func_2.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "    \n",
    "    rbuf = replay_buffers.ReplayBuffer(train_options['replay_memory_size'])\n",
    "    \n",
    "    agent = pfrl.agents.SoftActorCritic(\n",
    "        policy,\n",
    "        q_func_1,\n",
    "        q_func_2,\n",
    "        opt_a,\n",
    "        opt_c_1,\n",
    "        opt_c_2,\n",
    "        rbuf,\n",
    "        gamma=train_options['gamma'],\n",
    "        update_interval=1,\n",
    "        replay_start_size=train_options['minibatch_size'],\n",
    "        gpu=gpu,\n",
    "        soft_update_tau= train_opt['tau'],\n",
    "        minibatch_size = train_options['minibatch_size'],\n",
    "        entropy_target=-action_size,\n",
    "        temperature_optimizer_lr=1e-3,\n",
    "    )\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:13:38.363891Z",
     "iopub.status.busy": "2023-11-07T18:13:38.363588Z",
     "iopub.status.idle": "2023-11-07T18:13:38.380440Z",
     "shell.execute_reply": "2023-11-07T18:13:38.379643Z",
     "shell.execute_reply.started": "2023-11-07T18:13:38.363865Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "          env,\n",
    "          options,\n",
    "          train_options,\n",
    "          agent,\n",
    "          ounoise,\n",
    "          beam_id):\n",
    "    \n",
    "#     action_pred_noisy = ounoise.get_action(action_pred,\n",
    "#                                                t=train_options['overall_iter'])\n",
    "    \n",
    "    CB_Env = env   \n",
    "    if train_options['overall_iter'] == 1:\n",
    "        state = torch.zeros((1, options['num_ant'])).float().cuda()\n",
    "        print('Initial State Activated.')\n",
    "    else:\n",
    "        state = train_options['state']\n",
    "    \n",
    "    \n",
    "    #training_loop\n",
    "    iteration = 0\n",
    "    num_of_iter = train_options['num_iter']  \n",
    "    while iteration < num_of_iter:\n",
    "        \n",
    "        device_index = train_options['gpu']\n",
    "        device = f'cuda:{device_index}'\n",
    "        \n",
    "        action = agent.act(state)\n",
    "#         print(type(agent.batch_last_action),agent.batch_last_action)\n",
    "        reward_pred, bf_gain_pred, action_quant_pred, state_1_pred = CB_Env.get_reward(torch.Tensor(action).to(device))\n",
    "        reward_pred = torch.from_numpy(reward_pred).float().to(device)\n",
    "        \n",
    "        action_pred_noisy = torch.Tensor(action).to(device)\n",
    "        mat_dist = torch.abs(action_pred_noisy.reshape(options['num_ant'], 1) - options['ph_table_rep'])\n",
    "        action_quant = options['ph_table_rep'][range(options['num_ant']), torch.argmin(mat_dist, dim=1)].reshape(1, -1)\n",
    "   \n",
    "        state_1, reward, bf_gain, terminal = CB_Env.step(action_quant)\n",
    "        reward = torch.from_numpy(reward).float().to(device)\n",
    "        action = action_quant.reshape((1, -1)).float().to(device)\n",
    "        \n",
    "        \n",
    "#         print(type(action_quant_pred.cpu().numpy()),action_quant_pred.cpu().numpy().shape)\n",
    "#         print(type(action.cpu().numpy()),action.cpu().numpy().shape)\n",
    "        agent.replay_buffer.append(\n",
    "                    state=state,\n",
    "                    action=action_quant_pred.cpu().numpy(),\n",
    "                    reward=reward_pred,\n",
    "                    next_state=state_1_pred,\n",
    "                    next_action=None,\n",
    "                    is_state_terminal=terminal,\n",
    "                    env_id=0,\n",
    "                )\n",
    "#         print(agent.batch_last_action,agent.batch_last_action[0].shape)\n",
    "        agent.batch_last_action = [action.cpu().numpy()] #because the critic is expecting quantizazed actions\n",
    "        agent.observe(state_1, reward, terminal, False)\n",
    "\n",
    "        \n",
    "        iteration += 1\n",
    "        train_options['overall_iter'] += 1\n",
    "        state = state_1\n",
    "        \n",
    "        new_gain = torch.Tensor.cpu(CB_Env.achievement).detach().numpy().reshape((1, 1))\n",
    "        max_previous_gain = max(CB_Env.gain_history)\n",
    "        if new_gain > max_previous_gain:\n",
    "            CB_Env.gain_history.append(float(new_gain))                   \n",
    "        else:\n",
    "            CB_Env.gain_history.append(float(max_previous_gain))\n",
    "            \n",
    "    train_options['state'] = state  # used for the next loop\n",
    "    train_options['best_state'] = CB_Env.best_bf_vec  # used for clustering and assignment\n",
    "    if (train_options['overall_iter']-1)%500==0:\n",
    "#         print(\n",
    "#             \"Beam: %d, Iter: %d, Reward: %d, BF Gain: %.2f\" % \\\n",
    "#             (beam_id, \n",
    "#              train_options['overall_iter'],\n",
    "#              int(torch.Tensor.cpu(reward).numpy().squeeze()),\n",
    "#              torch.Tensor.cpu(bf_gain.detach()).numpy().squeeze(),))\n",
    "        print(\n",
    "            \"Beam: %d, Iter: %d, Reward pred: %d, Reward: %d, BF Gain pred: %.2f, BF Gain: %.2f\" % \\\n",
    "            (beam_id, train_options['overall_iter'],\n",
    "             int(torch.Tensor.cpu(reward_pred).numpy().squeeze()),\n",
    "             int(torch.Tensor.cpu(reward).numpy().squeeze()),\n",
    "             torch.Tensor.cpu(bf_gain_pred.detach()).numpy().squeeze(),\n",
    "             torch.Tensor.cpu(bf_gain.detach()).numpy().squeeze(),))      \n",
    "    \n",
    "    return train_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:13:38.382023Z",
     "iopub.status.busy": "2023-11-07T18:13:38.381733Z",
     "iopub.status.idle": "2023-11-07T18:13:38.511939Z",
     "shell.execute_reply": "2023-11-07T18:13:38.510646Z",
     "shell.execute_reply.started": "2023-11-07T18:13:38.381999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EGC bf gain:  231.17094\n",
      "EGC bf gain:  231.17094\n",
      "EGC bf gain:  231.17094\n",
      "EGC bf gain:  231.17094\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.device(options['gpu_idx']):\n",
    "    for beam_id in range(options['num_NNs']):\n",
    "        ounoise_list.append(OUNoise((1, options['num_ant'])))\n",
    "        env_list.append(envCB_(ch, options['num_ant'], options['num_bits'], beam_id, options))\n",
    "        train_opt_list.append(copy.deepcopy(train_opt))\n",
    "        agent = create_agent_sac(options['num_ant'],options['num_ant'],train_opt_list[beam_id],gpu=train_opt['gpu'])\n",
    "        agent_list.append(agent)\n",
    "        ounoise_list.append(OUNoise((1, options['num_ant'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T18:13:38.513735Z",
     "iopub.status.busy": "2023-11-07T18:13:38.513389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State Activated.\n",
      "Initial State Activated.\n",
      "Initial State Activated.\n",
      "Initial State Activated.\n",
      "Beam: 0, Iter: 501, Reward pred: 1, Reward: 1, BF Gain pred: 14.92, BF Gain: 14.92\n",
      "Beam: 1, Iter: 501, Reward pred: -1, Reward: -1, BF Gain pred: 2.87, BF Gain: 2.87\n",
      "Beam: 2, Iter: 501, Reward pred: 1, Reward: 1, BF Gain pred: 12.85, BF Gain: 12.85\n",
      "Beam: 3, Iter: 501, Reward pred: 1, Reward: 1, BF Gain pred: 24.21, BF Gain: 24.21\n",
      "Training for 500 iteration for each Beam uses 11.09652829170227 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pfrl/replay_buffer.py:180: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  \"action\": torch.as_tensor(\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.device(options['gpu_idx']):\n",
    "    for sample_id in range(options['num_loop']):\n",
    "\n",
    "        # ---------- Sampling ---------- #\n",
    "        n_sample = int(ch.shape[0] * options['ch_sample_ratio'])\n",
    "        ch_sample_id = np.random.permutation(ch.shape[0])[0:n_sample]\n",
    "        ch_sample = torch.from_numpy(ch[ch_sample_id, :]).float().cuda()\n",
    "\n",
    "        # ---------- Clustering ---------- #\n",
    "    #     start_time = time.time()\n",
    "\n",
    "        bf_mat_sample = bf_gain_cal(sensing_beam, ch_sample)\n",
    "        # print(\"Clustering -1 uses %s seconds.\" % (time.time() - start_time))\n",
    "        # start_time = time.time()\n",
    "        f_matrix = corr_mining(bf_mat_sample)\n",
    "        f_matrix_np = torch.Tensor.cpu(f_matrix).numpy()\n",
    "        # print(\"Clustering 0 uses %s seconds.\" % (time.time() - start_time))\n",
    "        # start_time = time.time()\n",
    "        labels = u_classifier.predict(np.transpose(f_matrix_np).astype(float))\n",
    "\n",
    "        # print(\"Clustering 1 uses %s seconds.\" % (time.time() - start_time))\n",
    "        # start_time = time.time()\n",
    "\n",
    "        user_group = []  # order: clusters\n",
    "        ch_group = []  # order: clusters\n",
    "        for ii in range(options['num_NNs']):\n",
    "            user_group.append(np.where(labels == ii)[0].tolist())\n",
    "            ch_group.append(ch_sample[user_group[ii], :])\n",
    "\n",
    "    #     print(\"Clustering 2 uses %s seconds.\" % (time.time() - start_time))\n",
    "\n",
    "        # ---------- Assignment ---------- #\n",
    "    #     start_time = time.time()\n",
    "\n",
    "        # best_state matrix\n",
    "        best_beam_mtx = torch.zeros((options['num_NNs'], 2 * options['num_ant'])).float().cuda()\n",
    "        for pp in range(options['num_NNs']):\n",
    "            best_beam_mtx[pp, :] = env_list[pp].best_bf_vec\n",
    "        gain_mtx = bf_gain_cal(best_beam_mtx, ch_sample)  # (n_beam, n_user)\n",
    "        for ii in range(options['num_NNs']):\n",
    "            if ii == 0:\n",
    "                cost_mtx = torch.mean(gain_mtx[:, user_group[ii]], dim=1).reshape(options['num_NNs'], -1)\n",
    "            else:\n",
    "                sub = torch.mean(gain_mtx[:, user_group[ii]], dim=1).reshape(options['num_NNs'], -1)\n",
    "                cost_mtx = torch.cat((cost_mtx, sub), dim=1)\n",
    "        cost_mtx = -torch.Tensor.cpu(cost_mtx).numpy()\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_mtx)\n",
    "        assignment_record = dict(zip(row_ind.tolist(), col_ind.tolist()))  # key: network, value: cluster\n",
    "        for ii in range(options['num_NNs']):\n",
    "            env_list[ii].ch = ch_group[assignment_record[ii]]\n",
    "\n",
    "    #     print(\"Assignment uses %s seconds.\" % (time.time() - start_time))\n",
    "        if (train_opt_list[beam_id]['overall_iter']-1)%500==0 or train_opt_list[beam_id]['overall_iter']==1:\n",
    "            start_time = time.time()\n",
    "        for beam_id in range(options['num_NNs']):\n",
    "            train_opt_list[beam_id] = train(env_list[beam_id],options, train_opt_list[beam_id],agent_list[beam_id],ounoise_list[beam_id], beam_id)\n",
    "        if (train_opt_list[beam_id]['overall_iter']-1)%500==0: \n",
    "            print(\"Training for 500 iteration for each Beam uses %s seconds.\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "\n",
    "num_ant = 32\n",
    "num_beam = 4\n",
    "results = np.empty((num_beam, 2*num_ant))\n",
    "\n",
    "path = './beams/'\n",
    "\n",
    "for beam_id in range(num_beam):\n",
    "    fname = 'beams_' + str(beam_id) + '_max.txt'\n",
    "    with open(path + fname, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        last_line = lines[-1]\n",
    "        results[beam_id, :] = np.fromstring(last_line.replace(\"\\n\", \"\"), sep=',').reshape(1, -1)\n",
    "\n",
    "results = (1 / np.sqrt(num_ant)) * (results[:, ::2] + 1j * results[:, 1::2])\n",
    "\n",
    "scio.savemat('beam_codebook.mat', {'beams': results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(f'Best beamforming gain')\n",
    "plt.xlabel('Beam_framing_gain')   \n",
    "for beam_id in range(options['num_NNs']):\n",
    "    gain_record=np.array(env_list[beam_id].gain_history[1:])\n",
    "    np.save(f'beam_{beam_id}_gain_records',gain_record)\n",
    "    plt.plot(gain_record, label=f'Beam_{beam_id}')\n",
    "    plt.plot([])\n",
    "plt.plot([231.17094]*40000,linestyle='dashed',color='black')\n",
    "plt.legend(loc=\"lower right\")  \n",
    "plt.show()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
